
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Py: Machine Learning Triangles &#8212; Actuaries&#39; Analytical Cookbook</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="R: Baudry ML Reserving Pt 1" href="MLRWP_R_Baudry_Notebook_1_SimulateData_v1.html" />
    <link rel="prev" title="R: Machine Learning Triangles" href="MLRWP_R_mlr3.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/actuaries-logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Actuaries' Analytical Cookbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Python
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="about_py.html">
   About Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="getting_started.html">
   Setting up Your Python Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="python_by_example.html">
   An Introductory Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="learn_more.html">
   Learn More
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Useful_Python_packages.html">
   Useful Python packages for Data Science
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to R
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="about_R.html">
   About R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="getting_started_R.html">
   Setting Up R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="introductory_R.html">
   Introduction to R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="intermediate_R.html">
   Next Steps With R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="top_ten_r_packages.html">
   Useful Packages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_R_DataTable.html">
   R: data.table for actuaries
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Workflow Management
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="version_control.html">
   Version control
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Regression, Classification and Technical Price
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M05_CS1.html">
   Py: Customer Churn Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multitasking_risk_pricing.html">
   Py/R: Multitasking Risk Pricing Using Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="py_shap_values.html">
   Py: Explainable Models with SHAP
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Life Insurance
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="LifeRecipeBook.html">
   R: Life Modelling Recipes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="life_stats.html">
   R: Life Industry Stats in Tableau and R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bayesian-applications.html">
   R: Bayesian Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="trees.html">
   R: Decision Tree Applications
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  General Insurance
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="SQL%20Query%20for%20Triangles.html">
   SQL: Queries to Create Triangles
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_R_GLMs.html">
   R: Reserving with GLMs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_R_Lasso.html">
   R: Reserving with LASSO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_R_mlr3.html">
   R: Machine Learning Triangles
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Py: Machine Learning Triangles
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_R_Baudry_Notebook_1_SimulateData_v1.html">
   R: Baudry ML Reserving Pt 1
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_R_Baudry_Notebook_2_CreateReservingDatabase_v1.html">
   R: Baudry ML Reserving Pt 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_R_Baudry_Notebook_3_ApplyMachineLearningReserving_v1.html">
   R: Baudry ML Reserving Pt 3
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unsupervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M06_CS1.html">
   Py: Socio-Economic Index Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M06_CS2.html">
   Py: Clustering Credit Card Fraud
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M06_Ex4.html">
   Py: K-means clustering of COVID dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M06_Ex5.html">
   Py: Hierarchical clustering on COVID dataset
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Natural Language Processing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="R_case_study_word_cloud.html">
   R: Word Cloud Case Study
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="textClassificationEntry.html">
   Py: Text Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M05_Ex10.html">
   Py: Decision Tree Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M05_Ex18.html">
   Py: Neural Net Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M07_CS1.html">
   Py: Classifying review sentiment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M07_CS2.html">
   Py: Customer Sentiment Analysis
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Business Optimization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_2021_S2_Tutorial10_exercise_scipy.html">
   Py: Linear Programming
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Image Recognition
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M05_CS2.html">
   Py: Image Recognition
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Ethics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="automated_decision.html">
   Automated Decision-Making Systems
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="zbibliography.html">
   Bibliography
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Contributing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="contributing.html">
   Contributing
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/ActuariesInstitute/cookbook/main?urlpath=tree/cookbook/docs/MLRWP_Py_triangles_example.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/ActuariesInstitute/cookbook/blob/main/cookbook/docs/MLRWP_Py_triangles_example.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ActuariesInstitute/cookbook"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ActuariesInstitute/cookbook/edit/main/cookbook/docs/MLRWP_Py_triangles_example.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/docs/MLRWP_Py_triangles_example.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Py: Machine Learning Triangles
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#who-is-this-article-aimed-at">
     Who is this article aimed at?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pre-requisites-to-this-article">
     Pre-requisites to this article
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-data-set">
     The data set
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#machine-learning-and-aggregate-triangles">
     Machine learning and aggregate triangles
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#don-t-rank-the-ml-models-used-based-on-this-example">
     Don’t rank the ML models used based on this example
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-scikit-learn-package">
     The scikit-learn package
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ml-methods-used">
     ML methods used
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#outline-of-the-workflow">
   Outline of the workflow
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-preparation">
   Data preparation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-pandas-package">
     The pandas package
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-the-data">
     Load the data
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-and-test">
     Train and Test
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tuning-process-for-hyper-parameters">
   Tuning process for hyper-parameters
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation">
     Cross-validation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation-in-scikit-learn">
     Cross-validation in scikit-learn
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-measure">
     Performance measure
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#searching-hyper-parameter-space">
     Searching hyper-parameter space
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-some-ml-models">
   Fitting some ML models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-tree">
     Decision tree
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fitting-a-single-model">
       Fitting a single model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tuning-the-decision-tree">
       Tuning the decision tree
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-forest-fitting">
     Random forest fitting
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gbm">
     GBM
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-network">
     Neural Network
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chainladder-the-baseline-model">
     Chainladder - the baseline model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#getting-the-chainladder-reserve-estimates">
       Getting the Chainladder reserve estimates
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lasso-regularised-regression">
     LASSO (regularised regression)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#basis-functions">
       Basis functions
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#setup-for-lasso">
       Setup for LASSO
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#diy-glmnet-with-pytorch">
       DIY GLMnet with Pytorch
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#previous-best-fit">
       Previous best fit
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Cross Validation
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-analysis">
   Model analysis
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#consolidate-results">
     Consolidate results
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rmse">
     RMSE
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualising-the-fit">
     Visualising the fit
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fitted-values">
       Fitted values
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#actual-vs-fitted-heat-maps">
       Actual vs Fitted heat maps
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#quarterly-tracking">
       Quarterly tracking
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reserves">
   Reserves
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#commentary">
   Commentary
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#models">
     Models
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#additional-features">
     Additional Features
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tuning-method-and-cross-validation">
     Tuning Method and Cross Validation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#so-what-s-going-on">
       So what’s going on?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#finer-fine-tuning">
       Finer fine-tuning
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#time-based-hold-out-testing">
       Time-based hold-out testing
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-next">
     What next?
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Py: Machine Learning Triangles</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Py: Machine Learning Triangles
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#who-is-this-article-aimed-at">
     Who is this article aimed at?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pre-requisites-to-this-article">
     Pre-requisites to this article
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-data-set">
     The data set
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#machine-learning-and-aggregate-triangles">
     Machine learning and aggregate triangles
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#don-t-rank-the-ml-models-used-based-on-this-example">
     Don’t rank the ML models used based on this example
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-scikit-learn-package">
     The scikit-learn package
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ml-methods-used">
     ML methods used
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#outline-of-the-workflow">
   Outline of the workflow
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-preparation">
   Data preparation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-pandas-package">
     The pandas package
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-the-data">
     Load the data
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-and-test">
     Train and Test
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tuning-process-for-hyper-parameters">
   Tuning process for hyper-parameters
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation">
     Cross-validation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation-in-scikit-learn">
     Cross-validation in scikit-learn
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-measure">
     Performance measure
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#searching-hyper-parameter-space">
     Searching hyper-parameter space
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-some-ml-models">
   Fitting some ML models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-tree">
     Decision tree
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fitting-a-single-model">
       Fitting a single model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tuning-the-decision-tree">
       Tuning the decision tree
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-forest-fitting">
     Random forest fitting
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gbm">
     GBM
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-network">
     Neural Network
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chainladder-the-baseline-model">
     Chainladder - the baseline model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#getting-the-chainladder-reserve-estimates">
       Getting the Chainladder reserve estimates
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lasso-regularised-regression">
     LASSO (regularised regression)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#basis-functions">
       Basis functions
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#setup-for-lasso">
       Setup for LASSO
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#diy-glmnet-with-pytorch">
       DIY GLMnet with Pytorch
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#previous-best-fit">
       Previous best fit
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Cross Validation
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-analysis">
   Model analysis
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#consolidate-results">
     Consolidate results
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rmse">
     RMSE
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualising-the-fit">
     Visualising the fit
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fitted-values">
       Fitted values
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#actual-vs-fitted-heat-maps">
       Actual vs Fitted heat maps
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#quarterly-tracking">
       Quarterly tracking
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reserves">
   Reserves
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#commentary">
   Commentary
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#models">
     Models
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#additional-features">
     Additional Features
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tuning-method-and-cross-validation">
     Tuning Method and Cross Validation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#so-what-s-going-on">
       So what’s going on?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#finer-fine-tuning">
       Finer fine-tuning
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#time-based-hold-out-testing">
       Time-based hold-out testing
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-next">
     What next?
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="py-machine-learning-triangles">
<h1>Py: Machine Learning Triangles<a class="headerlink" href="#py-machine-learning-triangles" title="Permalink to this headline">#</a></h1>
<p><em>This article was originally created by Jacky Poon and Grainne McGuire, and published in the <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/">General Insurance Machine Learning for Reserving Working Party (“MLR-WP”) blog</a>. The MLR-WP is an international research group on machine learning techniques to reserving, with over 50 actuaries from around the globe. The goal of the group is to bring machine learning techniques into widespread adoption ‘on the ground’ by identifying what the barriers are, communicating any benefits, and helping develop the research techniques in pragmatic ways. Whilst some articles have been brought into this cookbook, consider exploring the <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/">blog</a> further for additional content including detailed walkthroughs of more advanced models.</em></p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h1>
<p>In this article we look at applying different machine learning (ML) models to a data set.
Our goal is to illustrate a work flow for:</p>
<ul class="simple">
<li><p>setting up a data set for ML</p></li>
<li><p>applying different ML models to this data</p></li>
<li><p>tuning the ML hyper-parameters</p></li>
<li><p>comparing and contrasting performance for the past fitted values and the future predictions.</p></li>
</ul>
<p>A secondary goal is to demonstrate the utility of a multi-model machine learning framework which enables the user to easily “plug and play” different machine learning models within the same framework.</p>
<p>We use the <strong>scikit-learn</strong> in Python here. Followers of the blog may see significant similarities with the <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-mlr3example/">earlier <strong>mlr3</strong> article for R users</a>, except this article uses Python.</p>
<p>You can download this notebook <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/f_notebooks/ML_triangles_example.ipynb">here</a> and run it on your local machine, or on a cloud service like Google Colab. If you are looking at such a downloaded copy, find the Machine Learning for Reserving blog <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/">here</a>.</p>
<section id="who-is-this-article-aimed-at">
<h2>Who is this article aimed at?<a class="headerlink" href="#who-is-this-article-aimed-at" title="Permalink to this headline">#</a></h2>
<p>This article is aimed to Python users who know a little about some of the standard machine learning models, but who may not have done much hands-on analysis work. It will also be useful for those who have done some experimentation, but maybe not on a reserving data set.</p>
</section>
<section id="pre-requisites-to-this-article">
<h2>Pre-requisites to this article<a class="headerlink" href="#pre-requisites-to-this-article" title="Permalink to this headline">#</a></h2>
<p>We’ve tried to make this article accessible to people new to ML, and to make this article stand-alone. Having some knowledge about basic machine learning techniques like decision trees, random forests and gradient boosting will help. Furthermore, we also fit the Chain Ladder model as a GLM (sometimes referred to as a Stochastic Chain Ladder) and fit a particular type of LASSO model. We’ve included limited details on these models here. Although our previous blog posts are in R, they have more details on the background of these:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/glms/">Reserving with GLMs</a></p></li>
<li><p><a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-lasso/">Self-assembling claim reserving models using the LASSO</a></p></li>
</ul>
<p>We also fit some simple neural network models in this Python version.</p>
</section>
<section id="the-data-set">
<h2>The data set<a class="headerlink" href="#the-data-set" title="Permalink to this headline">#</a></h2>
<p>Our example deals with using ML to set reserves for a single aggregate 40x40 triangle. We’ve selected the data set because:</p>
<ul class="simple">
<li><p>It’s small, so the code will run relatively quickly on your machine - there’s nothing worse than running through a worked example and having to wait hours for results!</p></li>
<li><p>A lot of reserving is still done using traditional accident (or underwriting) + development aggregated triangles so it is relevant to the real world.</p></li>
</ul>
<p>We use a simulated data set. There are several reasons for this:</p>
<ul class="simple">
<li><p>We know the future results so can examine how different reserve predictions perform.</p></li>
<li><p>We can control how the future experience emerges. In particular, we can ensure there are no future systemic changes (e.g. from legislation or court precedent) that would impact future payments. Changes like this can make it difficult when examining performance of reserve prediction methods on real data - it can be hard to separate poor performance of the model from a good model where the future experience departs markedly from that used to build the model.</p></li>
<li><p>We can share the data set with you.</p></li>
</ul>
<p>The data set used is simulated data set 3 from the paper <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3241906">Self-assembling insurance claim models using regularized regression and machine learning</a>. This is a 40x40 triangle of incremental quarterly payments over 10 years. Variables on the data set are accident, development and calendar quarters only. A copy of the data set is available <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/csv/lasso_simdata3.csv">here</a>.</p>
</section>
<section id="machine-learning-and-aggregate-triangles">
<h2>Machine learning and aggregate triangles<a class="headerlink" href="#machine-learning-and-aggregate-triangles" title="Permalink to this headline">#</a></h2>
<p>Typically, ML and big data go hand-in-hand. By big data we mean lots of observations and lots of features (covariates / independent variables / regressors). Aggregate triangles are small in both senses - small numbers of observations and limited features (often just the 3 time features of accident/underwriting period, development period and calendar period).</p>
<p>This is not to say that it is invalid to use machine learning for aggregate triangles - many people have demonstrated good results from machine learning for such data - there are many papers on this topic. But it’s also true that an experienced actuary using a Chainladder model (or other traditional) method, overlaid with judgement, will often get a similar answer to the best ML method, even for a complex triangle. However ML may be a more efficient way of getting to an answer. In the worked example below, we use a data set which deviates significantly from Chainladder assumptions. Therefore the unadjusted Chainladder projection is quite poor and would need a significant investment of time to yield a better model. In contrast, the better performing ML models we looked at have a better first estimate, so may not require as much intervention to produce a final estimate, thereby leading to a time-saving.</p>
</section>
<section id="don-t-rank-the-ml-models-used-based-on-this-example">
<h2>Don’t rank the ML models used based on this example<a class="headerlink" href="#don-t-rank-the-ml-models-used-based-on-this-example" title="Permalink to this headline">#</a></h2>
<p>The purpose of this article is to demonstrate a workflow for fitting ML models in R. While we’ve done some work to improve model fitting, there are a lot more things we would consider if we were looking for the best models (e.g. feature engineering, train/test splits, performance measures). So, although we do look at the relative performance of the different models at the end, you should not make any conclusions about the relative performance of the different ML methods based on this work.</p>
<p>You may find that by adjusting some hyper-parameters, or by revising the training/test data set split, you’re able to find better models.
We’ll be discussing this point a bit further at the end of the article.</p>
<p><em>A priori</em>, given the form of the data (simulated using a regression style structure) and that the LASSO model we used is based on previous work which did aim to establish a framework to fit good models to triangular data using the LASSO, we expected that the LASSO model would perform the best and (spoiler alert!) it did.</p>
</section>
<section id="the-scikit-learn-package">
<h2>The scikit-learn package<a class="headerlink" href="#the-scikit-learn-package" title="Permalink to this headline">#</a></h2>
<p>One strength of using Python for machine learning is the consistency in input specifications for data. The Python community have generally coalesced onto <strong>pandas</strong> for data frames, <strong>numpy</strong> for matrices and <strong>scikit-learn</strong> for machine learning.</p>
<p>The benefit of these is apparent - set up the data in the aggregator and then switch models in and out at will.</p>
<p>As a rough rule of thumb, when working with <strong>scikit-learn</strong> we:</p>
<ul class="simple">
<li><p>First set up an instance of the class. Note that the class can contain both methods (like functions) and data.</p></li>
<li><p>We then run methods for the class object. These may update the data in the object.</p></li>
<li><p>We can view and use the data in the class object.</p></li>
</ul>
<p>If you want to learn more there is plenty of documentation out there:</p>
<ul class="simple">
<li><p>The <a class="reference external" href="https://scikit-learn.org/">scikit-learn website</a></p></li>
<li><p>Particularly, this <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/index.html">gallery of examples</a></p></li>
<li><p><a class="reference external" href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf">Cheatsheets</a>, with <a class="reference external" href="https://www.datacamp.com/community/blog/scikit-learn-cheat-sheet">explanations</a></p></li>
</ul>
<p>When using Python code, your code editor may also display help and documentation (via “docstrings”). You can also get inline help by using the <code class="docutils literal notranslate"><span class="pre">help()</span></code> function on an object. This will bring you to help pages.</p>
<p>E.g. <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">sklearn;</span> <span class="pre">help(sklearn)</span></code> with get you help on sklearn in general.</p>
</section>
<section id="ml-methods-used">
<h2>ML methods used<a class="headerlink" href="#ml-methods-used" title="Permalink to this headline">#</a></h2>
<p>We’ve looked at the following methods:</p>
<ul class="simple">
<li><p>Decision trees</p></li>
<li><p>Random forests</p></li>
<li><p>Gradient boosted machines (GBM) / gradient boosted decision trees</p></li>
<li><p>Neural Networks (a basic one that comes with <strong>scikit-learn</strong>)</p></li>
<li><p>Chainladder - or rather, a GLM which is set up to replicate the chain ladder estimates (refer to this <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/glms/">post</a> for more details)</p></li>
<li><p>LASSO</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="outline-of-the-workflow">
<h1>Outline of the workflow<a class="headerlink" href="#outline-of-the-workflow" title="Permalink to this headline">#</a></h1>
<p>The workflow we have set up consists of the following:</p>
<ul class="simple">
<li><p>Prepare the data for use in the models, including train and test partitions (past data) and hold-out or validation partitions (future data).</p></li>
<li><p>Fit each of the models selected. For each of these we:</p>
<ul>
<li><p>Select hyper-parameters (these control the model-fitting process) for tuning</p></li>
<li><p>Tune the hyper-parameters using the train and test partitions and select the set of values that yield the best results</p></li>
<li><p>Calculate predicted values on the holdout (future) data.</p></li>
</ul>
</li>
<li><p>Run diagnostics on the predicted values for each model and look at the reserve estimates.</p></li>
</ul>
<p>We’ll now work through each of the stages below.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">#</a></h1>
<p>The first step is to import all the packages required for this work (and install them first if needed).</p>
<p>This session uses <code class="docutils literal notranslate"><span class="pre">pandas</span></code>, <code class="docutils literal notranslate"><span class="pre">numpy</span></code>, <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, <code class="docutils literal notranslate"><span class="pre">seaborn</span></code>, <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> and <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>.</p>
<p>The versions are shown below. If you are having problems running our code, check that your packages are the same as ours and <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-U</span></code> upgrade / downgrade them if needed.</p>
<p>So let us get started with the example with some setup code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>!pip install -U scikit-learn
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: scikit-learn in /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages (0.24.2)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages (from scikit-learn) (2.1.0)
Requirement already satisfied: joblib&gt;=0.11 in /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages (from scikit-learn) (1.0.1)
Requirement already satisfied: scipy&gt;=0.19.1 in /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages (from scikit-learn) (1.6.3)
Requirement already satisfied: numpy&gt;=1.13.3 in /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages (from scikit-learn) (1.20.3)
Requirement already satisfied: numpy&gt;=1.13.3 in /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages (from scikit-learn) (1.20.3)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>!pip show pandas numpy scikit-learn seaborn matplotlib
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Name: pandas
Version: 1.2.4
Summary: Powerful data structures for data analysis, time series, and statistics
Home-page: https://pandas.pydata.org
Author: None
Author-email: None
License: BSD
Location: /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages
Requires: pytz, numpy, python-dateutil
Required-by: seaborn
---
Name: numpy
Version: 1.20.3
Summary: NumPy is the fundamental package for array computing with Python.
Home-page: https://www.numpy.org
Author: Travis E. Oliphant et al.
Author-email: None
License: BSD
Location: /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages
Requires: 
Required-by: torchvision, tensorflow, tensorboard, seaborn, scipy, scikit-learn, pandas, opt-einsum, mkl-random, mkl-fft, matplotlib, Keras, Keras-Preprocessing, h5py
---
Name: scikit-learn
Version: 0.24.2
Summary: A set of python modules for machine learning and data mining
Home-page: http://scikit-learn.org
Author: None
Author-email: None
License: new BSD
Location: /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages
Requires: threadpoolctl, scipy, numpy, joblib
Required-by: 
---
Name: seaborn
Version: 0.11.1
Summary: seaborn: statistical data visualization
Home-page: https://seaborn.pydata.org
Author: Michael Waskom
Author-email: mwaskom@nyu.edu
License: BSD (3-clause)
Location: /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages
Requires: scipy, pandas, matplotlib, numpy
Required-by: 
---
Name: matplotlib
Version: 3.4.2
Summary: Python plotting package
Home-page: https://matplotlib.org
Author: John D. Hunter, Michael Droettboom
Author-email: matplotlib-users@python.org
License: PSF
Location: /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages
Requires: python-dateutil, kiwisolver, numpy, pillow, cycler, pyparsing
Required-by: seaborn
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import pandas as pd
import numpy as np

from sklearn.linear_model import PoissonRegressor

from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.ensemble import HistGradientBoostingRegressor

from sklearn.neural_network import MLPRegressor
from sklearn.compose import TransformedTargetRegressor

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.pipeline import Pipeline

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import mean_squared_error

from sklearn.utils import all_estimators

import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm

import time

import itertools

# Set the seed for the random number generation so
# results are reproducible.
import random
random.seed(0)
np.random.seed(0)

# colours for plots - not all are used
# We will also use viridis for some plots
plot_colors = {
  &quot;dblue&quot;  : &quot;#113458&quot;,
  &quot;mblue&quot;  : &quot;#4096b8&quot;,
  &quot;gold&quot;   : &quot;#d9ab16&quot;,
  &quot;lgrey&quot;  : &quot;#dcddd9&quot;,
  &quot;dgrey&quot;  : &quot;#3f4548&quot;,
  &quot;black&quot;  : &quot;#3F4548&quot;,
  &quot;red&quot;    : &quot;#d01e45&quot;,
  &quot;purple&quot; : &quot;#8f4693&quot;,
  &quot;orange&quot; : &quot;#ee741d&quot;,
  &quot;fuscia&quot; : &quot;#e9458c&quot;,
  &quot;violet&quot; : &quot;#8076cf&quot;
}
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="data-preparation">
<h1>Data preparation<a class="headerlink" href="#data-preparation" title="Permalink to this headline">#</a></h1>
<p>We’re using a simulated data triangle with all values (past and future).</p>
<p><img alt="past and future" src="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-07-mlr3example/index_files/figure-html/unnamed-chunk-3-1.png" /></p>
<p>The data set we use is the simulated data set 3 from this <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3241906">paper</a>.
It is available in CSV form <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/csv/lasso_simdata3.csv">here</a>.</p>
<p>This is a 40x40 triangle of payments:</p>
<ul class="simple">
<li><p>That vary by accident quarter</p></li>
<li><p>That vary by development quarter</p></li>
<li><p>Have varying rates of superimposed inflation in payment size by calendar quarter (including no inflation)</p></li>
<li><p>Have a step-up in payment size for accident quarters &gt; 16 and development quarters &gt; 20.</p></li>
</ul>
<p>The first two effects are captured by a Chainladder model but the last two effects depart from Chainladder assumptions.</p>
<section id="the-pandas-package">
<h2>The pandas package<a class="headerlink" href="#the-pandas-package" title="Permalink to this headline">#</a></h2>
<p>We’ll be using the <strong>pandas</strong> package for manipulating the data. There’s an introduction to <strong>pandas</strong> on its <a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html">website</a> if you are unfamiliar with it.</p>
</section>
<section id="load-the-data">
<h2>Load the data<a class="headerlink" href="#load-the-data" title="Permalink to this headline">#</a></h2>
<p>First, load in the data and have a look at it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 1-4 will bring up different datasets. Covered in &quot;next steps&quot; note at the end of the article
dataset_number = 3

dat = pd.read_csv(
    f&quot;https://institute-and-faculty-of-actuaries.github.io/mlr-blog/csv/lasso_simdata{dataset_number}.csv&quot;,
    dtype={
        &quot;pmts&quot;: np.float32,
        &quot;acc&quot;: np.float32,
        &quot;dev&quot;: np.float32,
        &quot;cal&quot;: np.float32,
        &quot;mu&quot;: np.float32,
        &quot;train_ind&quot;: bool
    })

dat.head()

# create the num_periods variable - number of acc/dev periods
num_periods = int(dat[&quot;acc&quot;].max())
</pre></div>
</div>
</div>
</div>
<p>As you can see, the data is not in triangular form as per the diagram above, but is instead in long form where:</p>
<ul class="simple">
<li><p>each row of the data set consists of one observation</p></li>
<li><p>each observation has the accident(<code class="docutils literal notranslate"><span class="pre">acc</span></code>), development(<code class="docutils literal notranslate"><span class="pre">dev</span></code>) and calendar(<code class="docutils literal notranslate"><span class="pre">cal</span></code>) period associated with it</p></li>
<li><p>the <code class="docutils literal notranslate"><span class="pre">mu</span></code> value is the mean value of the distribution from which <code class="docutils literal notranslate"><span class="pre">pmts</span></code> was simulated - this won’t form part of the analysis below so can be ignored</p></li>
<li><p>the final variable, <code class="docutils literal notranslate"><span class="pre">train_ind</span></code> is <code class="docutils literal notranslate"><span class="pre">TRUE</span></code> for past values and <code class="docutils literal notranslate"><span class="pre">FALSE</span></code> for future values.</p></li>
</ul>
<p>This long format of data is standard for a lot of modelling and data analysis.</p>
<p>We can also show a visualisation of the data. This contains both past and future data (with a diagonal line marking the boundary). The plot uses shading to indicate the size of the payments (specifically, log(payments)).
The step-up in claim size (acc &gt; 16 and dev &gt; 20) is quite obvious in this graphic. What is also apparent is this this increase only affects a small part of the past triangle (10 cells out of 820 to be exact), but impacts much of the future lower triangle.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def plot_triangle(
    data, 
    mask_bottom=True, 
    title=&quot;Claims Development&quot;, 
    cmap=sns.color_palette(&quot;viridis_r&quot;, as_cmap=True), # Use Viridis colour palette just like the R code
    center=None,
    fig=None,
    ax=None):

    &quot;&quot;&quot; Plots a claims triangle heatmap
    data:
        Pandas DataFrame, indexed by date
    mask_bottom:
        Hide bottom half of triangle. Assumes origin and development periods
        are the same. e.g. both months
    cmap:
        Seaborn colour palette
    center:
        Centre for heatmap
    fig, ax: optional fig and ax
    &quot;&quot;&quot;
    sns.set(style=&quot;white&quot;)

    # Generate a mask for the triangle
    if mask_bottom:
        mask = np.ones_like(data, dtype=bool)

        mask[np.tril_indices_from(mask)] = False

        mask = np.flipud(mask)
    else:
        mask = None

    # Set up the matplotlib figure
    if fig is None or ax is None:
        fig, ax = plt.subplots(figsize=(6, 5))

    # Draw the heatmap with the mask and correct aspect ratio vmax=.3,
    sns.heatmap(
        data,
        mask=mask,
        cmap=cmap,
        square=True,
        linewidths=0.5,
        fmt=&quot;.0f&quot;,
        cbar_kws={&quot;shrink&quot;: 0.5},
        norm=LogNorm()  # Log scale
    ).set_title(title)

    return fig, ax
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Partial triangle - the model only sees this for training
plot_triangle(
    dat.pivot(index=&quot;acc&quot;, columns=&quot;dev&quot;, values=&quot;pmts&quot;),
    mask_bottom=True
)

plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_23_0.png" src="../_images/MLRWP_Py_triangles_example_23_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Whole triangle - with test dataset
plot_triangle(
    dat.pivot(index=&quot;acc&quot;, columns=&quot;dev&quot;, values=&quot;pmts&quot;),
    mask_bottom=False
)

plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_24_0.png" src="../_images/MLRWP_Py_triangles_example_24_0.png" />
</div>
</div>
<p>The step-up in payments can be modelled as an interaction between accident and development quarters.
Because it affects only a small number of past values, it may be difficult for some of the modelling approaches to capture this.
It’s likely, therefore, that the main differentiator between the performance of different methods for this data set will be how much well they capture this interaction.</p>
<p>One thing to note is that the Chainladder will struggle with this data set. Both the calendar period terms and the interaction are not effects that the Chainladder can model - it assumes that only accident and development period main effects (i.e. no interactions) are present. So an actuary using the Chainladder for this data set would need to overlay judgement to obtain a good result.</p>
<p>We need to modify this data a little before proceeding further and add factor versions (essentially a categorical version) of acc and dev - these are needed later for fitting the Chainladder.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Factors with zero index
dat[&quot;accf&quot;] = (dat.acc - 1).astype(int)
dat[&quot;devf&quot;] = (dat.dev - 1).astype(int)
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-and-test">
<h2>Train and Test<a class="headerlink" href="#train-and-test" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X_train = (dat.loc[dat.train_ind, [&quot;acc&quot;, &quot;dev&quot;, &quot;cal&quot;, &quot;accf&quot;, &quot;devf&quot;]])
y_train = (dat.loc[dat.train_ind, &quot;pmts&quot;])

X_test = (dat.loc[dat.train_ind == False, [&quot;acc&quot;, &quot;dev&quot;, &quot;cal&quot;, &quot;accf&quot;, &quot;devf&quot;]])
y_test = (dat.loc[dat.train_ind == False, &quot;pmts&quot;])

X = (dat.loc[:, [&quot;acc&quot;, &quot;dev&quot;, &quot;cal&quot;, &quot;accf&quot;, &quot;devf&quot;]])
y = (dat.loc[:, &quot;pmts&quot;])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="tuning-process-for-hyper-parameters">
<h1>Tuning process for hyper-parameters<a class="headerlink" href="#tuning-process-for-hyper-parameters" title="Permalink to this headline">#</a></h1>
<p>Machine learning models have a number of hyper-parameters that control the model fit.
Tweaking the hyper-parameters that control model fitting (e.g. for decision trees, the depth of the tree, or for random forests, the number of trees to average over) can have a significant impact on the quality of the fit.
The standard way of doing this is to use a train and test data set where:</p>
<ul class="simple">
<li><p>The model is trained (built) on the train data set</p></li>
<li><p>The performance of the model is evaluated on the test data set</p></li>
<li><p>This is repeated for a number of different combinations of hyper-parameters, and the best performing one is selected.</p></li>
</ul>
<p>Evaluating the models on a separate data set not used in the model fitting helps to control over-fitting.
Usually we would then select the hyper-parameters that lead to the best results on the test data set and use these to predict our future values.</p>
<p>There are various ways we can select the test and train data sets.
For this work we use cross-validation. We’ll give a brief overview of it below.
Note that we will be discussing validation options in a separate article.</p>
<section id="cross-validation">
<h2>Cross-validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline">#</a></h2>
<p>The simplest implementation of a train and test partition is a random one where each point in the data set is allocated to train and test at random.
Typically, the train part might be around 70-80% of the data and the test part the remainder.</p>
<p><strong>Random test data set</strong></p>
<p><img alt="random test dataset" src="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-07-mlr3example/index_files/figure-html/unnamed-chunk-13-1.png" /></p>
<p>Cross-validation repeats this type of split a number of times.</p>
<p>In more detail, the steps are:</p>
<ul class="simple">
<li><p>Randomly partition the data into <em>k</em> equal-sized folds (between 5-10 folds are common choices). These folds are then fixed for the remainder of the algorithm.</p></li>
<li><p>Fit the model using data from <em>k</em>-1 of the folds, using the remaining fold as the test data. Do this <em>k</em> times, so each fold is used as test data once.</p></li>
<li><p>Calculate the performance metrics for each model on the corresponding test fold.</p></li>
<li><p>Average the performance metrics across all the folds.</p></li>
</ul>
<p>A simplified representation of the process is given below.</p>
<p><strong>5-fold cross validation</strong></p>
<p><img alt="n-fold CV" src="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-07-mlr3example/index_files/figure-html/unnamed-chunk-14-1.png" /></p>
<p>Cross-validation provides an estimate of out-of-sample performance even though all the data is used for training and testing.
It is often considered more robust due to its use of the full dataset for testing, but can be computationally expensive for larger datasets. Here we only have a small amount of data, so there is an advantage to using the full dataset, whilst the computational cost is manageable.</p>
</section>
<section id="cross-validation-in-scikit-learn">
<h2>Cross-validation in scikit-learn<a class="headerlink" href="#cross-validation-in-scikit-learn" title="Permalink to this headline">#</a></h2>
<p>We will be setting up cross-validation resamplers in scikit-learn to apply to our models and find optimal hyper-parameters. As always, the documentation for <a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html">cross validation</a>, <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html">grid search CV</a> and <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html">random search CV</a> are useful starting reference point.</p>
<p>Here we will use 6-fold cross validation. As applied to our triangles, it will look something like this, where the yellow points are the training data and the blue the test data in each fold:</p>
<p><img alt="6-fold-on-triangle" src="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-07-mlr3example/index_files/figure-html/unnamed-chunk-17-1.png" /></p>
</section>
<section id="performance-measure">
<h2>Performance measure<a class="headerlink" href="#performance-measure" title="Permalink to this headline">#</a></h2>
<p>During the cross-validation process, we need to have a performance measure to optimise.</p>
<p>Here we will use RMSE (root mean square error).</p>
</section>
<section id="searching-hyper-parameter-space">
<h2>Searching hyper-parameter space<a class="headerlink" href="#searching-hyper-parameter-space" title="Permalink to this headline">#</a></h2>
<p>Searching hyper-parameter space needs to be customised to each model, so we can’t set up a common framework here. However, to control computations, it is useful to set a limit on the number of searches that can be performed in a tuning exercise.</p>
<p>Given how we set up the tuning process below we need 25 evaluations for the decision tree and random forest models. We need many more evaluations for the GBM model since we tune more parameters. However, this can take a long time to run. In practice, using a more powerful computer, or running things in parallel can help.</p>
<p>So we’ve set the number of evaluations to a low number here (25).</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="fitting-some-ml-models">
<h1>Fitting some ML models<a class="headerlink" href="#fitting-some-ml-models" title="Permalink to this headline">#</a></h1>
<p>Now it’s time to fit and tune the following ML models using <strong>scikit-learn</strong>:</p>
<ul class="simple">
<li><p>Decision tree</p></li>
<li><p>Random forest</p></li>
<li><p>Gradient boosting machines (GBM)</p></li>
<li><p>Neural Network</p></li>
</ul>
<section id="decision-tree">
<h2>Decision tree<a class="headerlink" href="#decision-tree" title="Permalink to this headline">#</a></h2>
<p>A decision tree is unlikely to be a very good model for this data, and tuning often doesn’t help much.
Nonetheless, it’s a simple model, so it’s useful to fit it to see what happens.</p>
<p>To illustrate the use of scikit-learn, we’ll first show how to fit a model using the default hyper-parameters.
We’ll then move onto the code needed to run hyper-parameter tuning using cross-validation.</p>
<section id="fitting-a-single-model">
<h3>Fitting a single model<a class="headerlink" href="#fitting-a-single-model" title="Permalink to this headline">#</a></h3>
<p>First let’s fit a simple decision tree.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>dtree = DecisionTreeRegressor(max_depth=3)
dtree.fit(X_train, y_train)

plot_tree(dtree)

plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_38_0.png" src="../_images/MLRWP_Py_triangles_example_38_0.png" />
</div>
</div>
<p>Unfortunately this plot is too small to read. If you are unfamiliar with decision tree diagrams, then these diagrams give splitting rules for the data. At the terminal nodes, predicted values for each segment are given. If you are able to read this plot (e.g. you are running the Jupyter notebook), then</p>
<p>Here’s a list of the parameters - <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html">the documentation</a> will have more information.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>dtree.get_params()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;ccp_alpha&#39;: 0.0,
 &#39;criterion&#39;: &#39;mse&#39;,
 &#39;max_depth&#39;: 3,
 &#39;max_features&#39;: None,
 &#39;max_leaf_nodes&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_impurity_split&#39;: None,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;random_state&#39;: None,
 &#39;splitter&#39;: &#39;best&#39;}
</pre></div>
</div>
</div>
</div>
<p>Let’s try tuning the <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> and ‘ccp_alpha’ parameters.</p>
</section>
<section id="tuning-the-decision-tree">
<h3>Tuning the decision tree<a class="headerlink" href="#tuning-the-decision-tree" title="Permalink to this headline">#</a></h3>
<p>To set up the tuning we need to specify:</p>
<ul class="simple">
<li><p>The ranges of values to search over</p></li>
<li><p>A resampling strategy (already have this - crossvalidation)</p></li>
<li><p>An evaluation measure (this is RMSE)</p></li>
<li><p>A termination criterion so searches don’t go on for ever (our <code class="docutils literal notranslate"><span class="pre">evals_trm</span></code>)</p></li>
<li><p>The search strategy (e.g. grid search, random search, etc - see, e.g., <a class="reference external" href="https://towardsdatascience.com/hyperparameter-tuning-a-practical-guide-and-template-b3bf0504f095">this post</a> )</p></li>
</ul>
<p>We first set ranges of values to consider for these:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">criterion</span></code>: whether to fit on the basis of <code class="docutils literal notranslate"><span class="pre">mse</span></code> which aligns with our evaluation metric or <code class="docutils literal notranslate"><span class="pre">poisson</span></code> to match the log-link GLMs later in the piece</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_depth</span></code>: how big the trees can be,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code>: complexity parameter for pruning.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>parameters_tree = {
    &quot;criterion&quot;: [&quot;mse&quot;, &quot;poisson&quot;],
    &quot;max_depth&quot;: [2, 3, 5, 7, None], # None is unlimited depth.
    &quot;ccp_alpha&quot;: [0.0, 0.2, 0.4, 0.6, 0.8]
}
</pre></div>
</div>
</div>
</div>
<p>5 of each of <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> and <code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> for a 2x5x5 grid. Given we only have 50 points, we can easily evaluate every option so we’ll use a grid search strategy.</p>
<p>In practice other search strategies may be preferable - e.g. a random search often gets similar results to a grid search, in a much smaller amount of time.
Another possible choice is Bayesian optimisation search.</p>
<p>So now we have everything we need to tune our hyper-parameters so we can set this up with <strong>GridSearchCV</strong> now.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>grid_tree = GridSearchCV(DecisionTreeRegressor(), parameters_tree)
grid_tree.fit(X_train, y_train)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GridSearchCV(estimator=DecisionTreeRegressor(),
             param_grid={&#39;ccp_alpha&#39;: [0.0, 0.2, 0.4, 0.6, 0.8],
                         &#39;criterion&#39;: [&#39;mse&#39;, &#39;poisson&#39;],
                         &#39;max_depth&#39;: [2, 3, 5, 7, None]})
</pre></div>
</div>
</div>
</div>
<p>The parameters in the best fit may be accessed here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>grid_tree.best_estimator_.get_params()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;ccp_alpha&#39;: 0.8,
 &#39;criterion&#39;: &#39;mse&#39;,
 &#39;max_depth&#39;: None,
 &#39;max_features&#39;: None,
 &#39;max_leaf_nodes&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_impurity_split&#39;: None,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;random_state&#39;: None,
 &#39;splitter&#39;: &#39;best&#39;}
</pre></div>
</div>
</div>
</div>
<p>These also fit a final model to the data using these optimised parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># plot the model
plot_tree(grid_tree.best_estimator_)

plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_50_0.png" src="../_images/MLRWP_Py_triangles_example_50_0.png" />
</div>
</div>
<p>This model is much more complicated than the original and looks overfitted - we’ll see if this is the case when we evaluate the model performance later in the article.</p>
</section>
</section>
<section id="random-forest-fitting">
<h2>Random forest fitting<a class="headerlink" href="#random-forest-fitting" title="Permalink to this headline">#</a></h2>
<p>The random forest model for regression is <code class="docutils literal notranslate"><span class="pre">sklearn.ensemble.RandomForestRegressor</span></code>.</p>
<p>Let’s have a look at the hyper-parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>rforest = RandomForestRegressor()
rforest.fit(X_train, y_train)

rforest.get_params()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;bootstrap&#39;: True,
 &#39;ccp_alpha&#39;: 0.0,
 &#39;criterion&#39;: &#39;mse&#39;,
 &#39;max_depth&#39;: None,
 &#39;max_features&#39;: &#39;auto&#39;,
 &#39;max_leaf_nodes&#39;: None,
 &#39;max_samples&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_impurity_split&#39;: None,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;n_estimators&#39;: 100,
 &#39;n_jobs&#39;: None,
 &#39;oob_score&#39;: False,
 &#39;random_state&#39;: None,
 &#39;verbose&#39;: 0,
 &#39;warm_start&#39;: False}
</pre></div>
</div>
</div>
</div>
<p>Here, we’ll try tuning <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> and ‘ccp_alpha’ parameters again.</p>
<p>We’ll follow the same steps as for decision trees - set up a parameter space for searching, combine this with RMSE and cross-validation, tune using grid search, and get the best fit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>parameters_forest = {
    &quot;max_depth&quot;: [2, 3, 5, 7, None], # None is unlimited depth.
    &quot;ccp_alpha&quot;: [0.0, 0.2, 0.4, 0.6, 0.8],
    &quot;random_state&quot;: [0]
}

grid_forest = GridSearchCV(
    RandomForestRegressor(), 
    parameters_forest,  
    n_jobs=-1)

grid_forest.fit(X_train, y_train)
grid_forest.best_estimator_.get_params()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;bootstrap&#39;: True,
 &#39;ccp_alpha&#39;: 0.0,
 &#39;criterion&#39;: &#39;mse&#39;,
 &#39;max_depth&#39;: None,
 &#39;max_features&#39;: &#39;auto&#39;,
 &#39;max_leaf_nodes&#39;: None,
 &#39;max_samples&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_impurity_split&#39;: None,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;n_estimators&#39;: 100,
 &#39;n_jobs&#39;: None,
 &#39;oob_score&#39;: False,
 &#39;random_state&#39;: 0,
 &#39;verbose&#39;: 0,
 &#39;warm_start&#39;: False}
</pre></div>
</div>
</div>
</div>
</section>
<section id="gbm">
<h2>GBM<a class="headerlink" href="#gbm" title="Permalink to this headline">#</a></h2>
<p>There are two GBM implementations in <strong>scikit-learn</strong>. The first is <code class="docutils literal notranslate"><span class="pre">sklearn.ensemble.GradientBoostingRegressor</span></code> which implements a “traditional” GBM.</p>
<p>The other is <code class="docutils literal notranslate"><span class="pre">sklearn.ensemble.HistGradientBoostingRegressor</span></code> which implements a number of performance tricks to speed up model fitting, similar to <code class="docutils literal notranslate"><span class="pre">LightGBM</span></code> or the fast histogram method in <code class="docutils literal notranslate"><span class="pre">XGBoost</span></code>. This is considered an experimental feature with the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> version we are using.</p>
<p>Here are the GBM parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>gbm = GradientBoostingRegressor()
gbm.fit(X_train, y_train)

gbm.get_params()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;alpha&#39;: 0.9,
 &#39;ccp_alpha&#39;: 0.0,
 &#39;criterion&#39;: &#39;friedman_mse&#39;,
 &#39;init&#39;: None,
 &#39;learning_rate&#39;: 0.1,
 &#39;loss&#39;: &#39;ls&#39;,
 &#39;max_depth&#39;: 3,
 &#39;max_features&#39;: None,
 &#39;max_leaf_nodes&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_impurity_split&#39;: None,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;n_estimators&#39;: 100,
 &#39;n_iter_no_change&#39;: None,
 &#39;random_state&#39;: None,
 &#39;subsample&#39;: 1.0,
 &#39;tol&#39;: 0.0001,
 &#39;validation_fraction&#39;: 0.1,
 &#39;verbose&#39;: 0,
 &#39;warm_start&#39;: False}
</pre></div>
</div>
</div>
</div>
<p>With gradient boosting, it is often helpful to optimise over hyper-parameters, so we will vary some parameters and select the best performing set.</p>
<p>For speed reasons we will just consider <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>, <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>, <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> , and <code class="docutils literal notranslate"><span class="pre">subsample</span></code> but in practice, we could consider doing more.</p>
<p>The steps are the generally same as before for decision trees and random forests. However as we are searching over a greater number of parameters, we’ve switched to a random search to try to achieve better results.</p>
<p>Depending on your computer, this step takes a while to run since we are running the slower GradientBoostingRegressor - even though we have a low <code class="docutils literal notranslate"><span class="pre">n_iter</span></code> - so it might be a good point to grab a cup of coffee or tea!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>parameters_gbm = {
    &quot;n_estimators&quot;: [100, 200, 300, 400, 500],
    &quot;max_depth&quot;: [1, 2, 3, 5, 6],
    &quot;learning_rate&quot;: [0.01, 0.02, 0.05, 0.1, 0.3],
    &quot;subsample&quot;: [0.5, 0.7, 1.0]
}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tic = time.perf_counter()

grid_gbm = RandomizedSearchCV(
    GradientBoostingRegressor(), 
    parameters_gbm, 
    n_iter=25, 
    n_jobs=-1, # Run in parallel
    random_state=0
)

grid_gbm.fit(X_train, y_train)

toc = time.perf_counter()
print(f&quot;Ran in {toc - tic:0.4f} seconds&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ran in 7.4581 seconds
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Get the best fit:
grid_gbm.best_estimator_.get_params()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;alpha&#39;: 0.9,
 &#39;ccp_alpha&#39;: 0.0,
 &#39;criterion&#39;: &#39;friedman_mse&#39;,
 &#39;init&#39;: None,
 &#39;learning_rate&#39;: 0.01,
 &#39;loss&#39;: &#39;ls&#39;,
 &#39;max_depth&#39;: 5,
 &#39;max_features&#39;: None,
 &#39;max_leaf_nodes&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_impurity_split&#39;: None,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;n_estimators&#39;: 400,
 &#39;n_iter_no_change&#39;: None,
 &#39;random_state&#39;: None,
 &#39;subsample&#39;: 0.5,
 &#39;tol&#39;: 0.0001,
 &#39;validation_fraction&#39;: 0.1,
 &#39;verbose&#39;: 0,
 &#39;warm_start&#39;: False}
</pre></div>
</div>
</div>
</div>
<p>However, remember that these results are from 25 evaluations only, and with 4 hyper-parameters, we’d really like to use more evaluations. The model above isn’t a particularly good one.</p>
<p>Next, the HistGradientBooster which is most similar to LightGBM, which was in turn based on XGBoost. It’s quite possible to install those packages too to include more full featured models but to make this tutorial code easy to install and use, we won’t introduce the extra dependencies and just use the scikit-learn implementation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>parameters_hgbm = {
    &quot;loss&quot;: [&quot;least_squares&quot;, &quot;poisson&quot;],
    &quot;max_iter&quot;: [100, 200, 300, 400, 500],
    &quot;max_depth&quot;: [1, 2, 3, 5, 6],
    &quot;learning_rate&quot;: [0.01, 0.02, 0.05, 0.1, 0.3]
}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tic = time.perf_counter()
# This regressor is generally quite fast so we can run a few more.
grid_hgbm = RandomizedSearchCV(
    HistGradientBoostingRegressor(), 
    parameters_hgbm, 
    n_iter=100, 
    n_jobs=-1, # Run in parallel
    random_state=0
)

grid_hgbm.fit(X_train, y_train)

toc = time.perf_counter()
print(f&quot;Ran in {toc - tic:0.4f} seconds&quot;)
# It was not faster in this case with this dataset.
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ran in 38.4765 seconds
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Get the best fit:
grid_hgbm.best_estimator_.get_params()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;categorical_features&#39;: None,
 &#39;early_stopping&#39;: &#39;auto&#39;,
 &#39;l2_regularization&#39;: 0.0,
 &#39;learning_rate&#39;: 0.1,
 &#39;loss&#39;: &#39;poisson&#39;,
 &#39;max_bins&#39;: 255,
 &#39;max_depth&#39;: 6,
 &#39;max_iter&#39;: 400,
 &#39;max_leaf_nodes&#39;: 31,
 &#39;min_samples_leaf&#39;: 20,
 &#39;monotonic_cst&#39;: None,
 &#39;n_iter_no_change&#39;: 10,
 &#39;random_state&#39;: None,
 &#39;scoring&#39;: &#39;loss&#39;,
 &#39;tol&#39;: 1e-07,
 &#39;validation_fraction&#39;: 0.1,
 &#39;verbose&#39;: 0,
 &#39;warm_start&#39;: False}
</pre></div>
</div>
</div>
</div>
<p>In the R code, we mentioned a previously tuned XGBoost model with hyperparameters tuned over a larger search. Can we replicate the model in Python here?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>gbm_prev = HistGradientBoostingRegressor(
    loss=&#39;poisson&#39;,  # Poisson is close to Tweedie with variance power 1.01
    max_iter=233,
    learning_rate=0.3,
    max_depth=3
)

gbm_prev.fit(X_train, y_train)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>HistGradientBoostingRegressor(learning_rate=0.3, loss=&#39;poisson&#39;, max_depth=3,
                              max_iter=233)
</pre></div>
</div>
</div>
</div>
</section>
<section id="neural-network">
<h2>Neural Network<a class="headerlink" href="#neural-network" title="Permalink to this headline">#</a></h2>
<p>Neural networks - also known as deep learning models - are models based on interconnected neurons which take in inputs, apply a function and produce outputs used by other neurons, ultimately to predict response variables.</p>
<p>In the <strong>R</strong> article with <strong>mlr3</strong> we skipped neural networks as these required keras which can be a little tricky to install. However, <strong>scikit-learn</strong> includes a simple <a class="reference external" href="https://scikit-learn.org/stable/modules/neural_networks_supervised.html">Multi-layer Perceptron</a> model so we will try that here.</p>
<p>This is a simple feedforward model where each layer is connected in order.</p>
<p><img alt="Feedforward model diagram" src="https://upload.wikimedia.org/wikipedia/en/5/54/Feed_forward_neural_net.gif" /></p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Feedforward_neural_network#/media/File:Feed_forward_neural_net.gif">Source: Wikipedia</a></p>
<p>Unlike the diagram all neurons are just simply connected to all neurons in the following layer. But there are many variations on how those neurons can be connected which we may explore in future articles.</p>
<p>With the features, we will transform accident, development and calendar periods with one-hot encoding to treat them as continuous variables, and let the model figure the rest out. With the target, given the average value is approx $300m, we’ll scale it so the model can hopefully find convergence faster - and we will try a few different learning rates.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>parameters_nn = {
    &quot;hidden_layer_sizes&quot;: [ 
        # Keep these to 50 neurons to keep things small
        # (We do only have 3 factors)
        (50,),           # single hidden layer
        (25, 25),         # two layers
        (16, 16, 16),     # three layers
        (12, 12, 12, 12), # four layers
        (30, 15, 5),     # bottleneck model - reduce data to 10 final effects
    ],
    &quot;alpha&quot;: [0.00001, 0.0001, 0.001, 0.01, 0.1],
    # Activation: Relu is standard but logistic / sigmoid has been used by some
    # researchers in reserving applications
    &quot;activation&quot;: [&quot;logistic&quot;, &quot;relu&quot;], 
    &quot;random_state&quot;: [0],
    &quot;max_iter&quot;: [200000]  # More iterations worked well for lasso model in R
}

col_transformer_nn = ColumnTransformer(
    [
        (&#39;zero_to_one&#39;, MinMaxScaler(), [&quot;acc&quot;, &quot;dev&quot;, &quot;cal&quot;])
    ],
    remainder=&#39;drop&#39;
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tic = time.perf_counter()
neural_network = Pipeline(
    steps=[
        (&#39;transform&#39;, col_transformer_nn), 
        (&#39;nn&#39;, TransformedTargetRegressor(
                regressor=RandomizedSearchCV(
                  MLPRegressor(),
                  parameters_nn, 
                  n_jobs=-1, # Run in parallel
                  n_iter=25, # Models train slowly, so try only a few models
                  random_state=0),
                # An output activation function of exp would have been nice
                # But not available in scikit, so we will fit on log-transformed 
                # values and exponentiate.
                func=lambda x: np.log(x / y_train.mean()), 
                inverse_func=lambda x: np.exp(x) * y_train.mean(),
                check_inverse=False))
    ]
)

neural_network.fit(X_train, y_train)
toc = time.perf_counter()
print(f&quot;Ran in {toc - tic:0.4f} seconds&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ran in 86.3401 seconds
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>neural_network[&quot;nn&quot;].regressor_.best_estimator_.get_params()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;activation&#39;: &#39;relu&#39;,
 &#39;alpha&#39;: 1e-05,
 &#39;batch_size&#39;: &#39;auto&#39;,
 &#39;beta_1&#39;: 0.9,
 &#39;beta_2&#39;: 0.999,
 &#39;early_stopping&#39;: False,
 &#39;epsilon&#39;: 1e-08,
 &#39;hidden_layer_sizes&#39;: (12, 12, 12, 12),
 &#39;learning_rate&#39;: &#39;constant&#39;,
 &#39;learning_rate_init&#39;: 0.001,
 &#39;max_fun&#39;: 15000,
 &#39;max_iter&#39;: 200000,
 &#39;momentum&#39;: 0.9,
 &#39;n_iter_no_change&#39;: 10,
 &#39;nesterovs_momentum&#39;: True,
 &#39;power_t&#39;: 0.5,
 &#39;random_state&#39;: 0,
 &#39;shuffle&#39;: True,
 &#39;solver&#39;: &#39;adam&#39;,
 &#39;tol&#39;: 0.0001,
 &#39;validation_fraction&#39;: 0.1,
 &#39;verbose&#39;: False,
 &#39;warm_start&#39;: False}
</pre></div>
</div>
</div>
</div>
<p>Before we do any analysis, let’s fit a couple more models to compare these results against:</p>
<ul class="simple">
<li><p>a Chainladder model</p></li>
<li><p>a LASSO model</p></li>
</ul>
</section>
<section id="chainladder-the-baseline-model">
<h2>Chainladder - the baseline model<a class="headerlink" href="#chainladder-the-baseline-model" title="Permalink to this headline">#</a></h2>
<p>Since this is a traditional triangle, it seems natural to compare any results to the Chainladder result. So here, we will get the predicted values and reserve estimate for the Chainladder model.</p>
<p>It’s important to note that we will just use a volume-all Chainladder (i.e. include all periods in the estimation of the development factors) and will not attempt to impose any judgement over the results. In practice, of course, models like the Chainladder are often subject to additional analysis, reasonableness tests and manual assumption selections, so it’s likely the actual result would be different, perhaps significantly, from that returned here.</p>
<p>At the same time, no model, whether it be the Chainladder, or a more sophisticated ML model should be accepted without further testing, so on that basis, comparing Chainladder and ML results without modification is a reasonable thing to do. Better methods should require less human intervention to return a reasonable result.</p>
<section id="getting-the-chainladder-reserve-estimates">
<h3>Getting the Chainladder reserve estimates<a class="headerlink" href="#getting-the-chainladder-reserve-estimates" title="Permalink to this headline">#</a></h3>
<p>The Chainladder reserve can also be calculated using a GLM with:</p>
<ul class="simple">
<li><p>Accident and development factors</p></li>
<li><p>The Poisson or over-dispersed Poisson distribution</p></li>
<li><p>The log link.</p></li>
</ul>
<p>We’ve used this method here as it’s easy to set up in Python but practical work using the Chainladder may be better done with a package like the <a class="reference external" href="https://chainladder-python.readthedocs.io/en/latest/">Python Chainladder package</a> by CASact.</p>
<p>The easiest way to do this is to use <a class="reference external" href="https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines">Pipelines</a> which allows us to have a neat package of a model that comes with its own data transformations - specifically to use <code class="docutils literal notranslate"><span class="pre">accf</span></code> and <code class="docutils literal notranslate"><span class="pre">devf</span></code> properly.</p>
<p>Here’s the code using <code class="docutils literal notranslate"><span class="pre">PoissonRegressor</span></code> and the <code class="docutils literal notranslate"><span class="pre">predict()</span></code> method to add predicted values into <code class="docutils literal notranslate"><span class="pre">model_forecasts</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>col_transformer = ColumnTransformer(
    [(&#39;encoder&#39;, OneHotEncoder(), [&quot;accf&quot;, &quot;devf&quot;])], 
    remainder=&#39;drop&#39;
)

# PoissonRegressor gives a log-link GLM. But need to set alpha=0 so it is not a LASSO.
chain_ladder = Pipeline(
    steps=[
        (&#39;transform&#39;, col_transformer), 
        (&#39;glm&#39;, PoissonRegressor(alpha=0, max_iter=5000))
    ]
)

chain_ladder.fit(X_train, y_train)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Pipeline(steps=[(&#39;transform&#39;,
                 ColumnTransformer(transformers=[(&#39;encoder&#39;, OneHotEncoder(),
                                                  [&#39;accf&#39;, &#39;devf&#39;])])),
                (&#39;glm&#39;, PoissonRegressor(alpha=0, max_iter=5000))])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="lasso-regularised-regression">
<h2>LASSO (regularised regression)<a class="headerlink" href="#lasso-regularised-regression" title="Permalink to this headline">#</a></h2>
<p>Finally, we will use the LASSO to fit a model. We are going to fit the same model as we did in our previous <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-lasso/">post</a>. We’ll include a bare-bones description of the model here. If you haven’t seen it before, then you may want to just take the model as given and skip ahead to the next section. Once you’ve read this entire article you may then wish to read the post on the LASSO model to understand the specifics of this model.</p>
<p>The main things to note about this model are:</p>
<ul class="simple">
<li><p>Feature engineering is needed to produce continuous functions of accident / development / calendar quarters for the LASSO to fit.</p></li>
<li><p>The blog post and related paper detail how to do this - essentially we create a large group of basis functions from the primary <code class="docutils literal notranslate"><span class="pre">acc</span></code>, <code class="docutils literal notranslate"><span class="pre">dev</span></code> and <code class="docutils literal notranslate"><span class="pre">cal</span></code> variables that are flexible enough to capture a wide variety of shapes.</p></li>
<li><p>We need to create a model that contains these basis functions as features.</p></li>
<li><p>We then need to fit and predict using the values for a particular penalty setting (the regularisation parameter) - following the paper, we use the penalty value that leads to the lowest cross-validation error.</p></li>
</ul>
<section id="basis-functions">
<h3>Basis functions<a class="headerlink" href="#basis-functions" title="Permalink to this headline">#</a></h3>
<p>The first step is to create all the basis functions that we need. The functions below create the ramp and step functions needed (over 4000 of these!). The end result is a data.table of the original payments and all the basis functions. As noted above, all the details are in the blog post and paper.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def LinearSpline(var, start, stop):
    &quot;&quot;&quot;
    Linear spline function - used in data generation and in spline generation below
    &quot;&quot;&quot;
    return np.minimum(stop - start, np.maximum(0, var - start))


def GetScaling(vec):
    &quot;&quot;&quot;
    Function to calculate scaling factors for the basis functions
    scaling is discussed in the paper
    &quot;&quot;&quot;
    fn = len(vec)
    fm = np.mean(vec)
    fc = vec - fm  

    return  ((np.sum(fc**2))/fn)**0.5


def GetRamps(vec, vecname, nperiods, scaling):
    &quot;&quot;&quot;
    Function to create the ramps for a particular primary vector
    vec = fundamental regressor
    vecname = name of regressor
    np = number of periods
    scaling = scaling factor to use
    &quot;&quot;&quot;
    df = pd.DataFrame.from_dict(
        {
            f&quot;L_{i}_999_{vecname}&quot;: LinearSpline(vec, i, 999) / scaling
            for i in range(1, nperiods)
        }
    )
    return df


def GetInts(vec1, vec2, vecname1, vecname2, nperiods, scaling1, scaling2, train_ind):
    &quot;&quot;&quot;
    Create the step (heaviside) function interactions.
    f&quot;I_{vecname1}_ge_{i}xI_{vecname2}_ge_{j}&quot; formats the name of the column
    LinearSpline(vec1, 1, i+1) / scaling1 * LinearSpline(vec2, j, j + 1) / scaling2
    is the interaction term
    and we loop over all combinations of 1:nperiods and 2:nperiods
    &quot;&quot;&quot;
    
    vecs = {}
    for i, j in itertools.product(*[range(2, nperiods), range(2, nperiods)]):
        interaction = (
            LinearSpline(vec1, i - 1, i) / scaling1 * 
            LinearSpline(vec2, j - 1, j) / scaling2
        )

        # Only include if non-constant over training data
        if not np.all(interaction[train_ind] == interaction[0]):
            vecs[f&quot;I_{vecname1}_ge_{i}xI_{vecname2}_ge_{j}&quot;] = interaction

    df = pd.DataFrame.from_dict(vecs)
    return df
</pre></div>
</div>
</div>
</div>
<p>Now the functions are defined, we’ll create a data.table of basis functions here which we’ll use later when fitting the LASSO.
The <code class="docutils literal notranslate"><span class="pre">dat_plus</span></code> table will hold them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># get the scaling values
rho_factor_list = {
    v: GetScaling(dat.loc[dat.train_ind, v].values) 
    for v in [&quot;acc&quot;, &quot;dev&quot;, &quot;cal&quot;]
}

rho_factor_list
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;acc&#39;: 9.539392014169456, &#39;dev&#39;: 9.539392014169456, &#39;cal&#39;: 9.539392014169456}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># main effects - matrix of values
main_effects_acc = GetRamps(vec = dat[&quot;acc&quot;], vecname = &quot;acc&quot;, nperiods = num_periods, scaling = rho_factor_list[&quot;acc&quot;])
main_effects_dev = GetRamps(vec = dat[&quot;dev&quot;], vecname = &quot;dev&quot;, nperiods = num_periods, scaling = rho_factor_list[&quot;dev&quot;])
main_effects_cal = GetRamps(vec = dat[&quot;cal&quot;], vecname = &quot;cal&quot;, nperiods = num_periods, scaling = rho_factor_list[&quot;cal&quot;])

main_effects = pd.concat([main_effects_acc, main_effects_dev, main_effects_cal], axis=&quot;columns&quot;)
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">main_effects</span></code> looks like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>main_effects
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>L_1_999_acc</th>
      <th>L_2_999_acc</th>
      <th>L_3_999_acc</th>
      <th>L_4_999_acc</th>
      <th>L_5_999_acc</th>
      <th>L_6_999_acc</th>
      <th>L_7_999_acc</th>
      <th>L_8_999_acc</th>
      <th>L_9_999_acc</th>
      <th>L_10_999_acc</th>
      <th>...</th>
      <th>L_30_999_cal</th>
      <th>L_31_999_cal</th>
      <th>L_32_999_cal</th>
      <th>L_33_999_cal</th>
      <th>L_34_999_cal</th>
      <th>L_35_999_cal</th>
      <th>L_36_999_cal</th>
      <th>L_37_999_cal</th>
      <th>L_38_999_cal</th>
      <th>L_39_999_cal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1595</th>
      <td>4.088311</td>
      <td>3.983482</td>
      <td>3.878654</td>
      <td>3.773825</td>
      <td>3.668997</td>
      <td>3.564168</td>
      <td>3.45934</td>
      <td>3.354511</td>
      <td>3.249683</td>
      <td>3.144854</td>
      <td>...</td>
      <td>4.717281</td>
      <td>4.612453</td>
      <td>4.507625</td>
      <td>4.402796</td>
      <td>4.297967</td>
      <td>4.193139</td>
      <td>4.088311</td>
      <td>3.983482</td>
      <td>3.878654</td>
      <td>3.773825</td>
    </tr>
    <tr>
      <th>1596</th>
      <td>4.088311</td>
      <td>3.983482</td>
      <td>3.878654</td>
      <td>3.773825</td>
      <td>3.668997</td>
      <td>3.564168</td>
      <td>3.45934</td>
      <td>3.354511</td>
      <td>3.249683</td>
      <td>3.144854</td>
      <td>...</td>
      <td>4.822110</td>
      <td>4.717281</td>
      <td>4.612453</td>
      <td>4.507625</td>
      <td>4.402796</td>
      <td>4.297967</td>
      <td>4.193139</td>
      <td>4.088311</td>
      <td>3.983482</td>
      <td>3.878654</td>
    </tr>
    <tr>
      <th>1597</th>
      <td>4.088311</td>
      <td>3.983482</td>
      <td>3.878654</td>
      <td>3.773825</td>
      <td>3.668997</td>
      <td>3.564168</td>
      <td>3.45934</td>
      <td>3.354511</td>
      <td>3.249683</td>
      <td>3.144854</td>
      <td>...</td>
      <td>4.926939</td>
      <td>4.822110</td>
      <td>4.717281</td>
      <td>4.612453</td>
      <td>4.507625</td>
      <td>4.402796</td>
      <td>4.297967</td>
      <td>4.193139</td>
      <td>4.088311</td>
      <td>3.983482</td>
    </tr>
    <tr>
      <th>1598</th>
      <td>4.088311</td>
      <td>3.983482</td>
      <td>3.878654</td>
      <td>3.773825</td>
      <td>3.668997</td>
      <td>3.564168</td>
      <td>3.45934</td>
      <td>3.354511</td>
      <td>3.249683</td>
      <td>3.144854</td>
      <td>...</td>
      <td>5.031767</td>
      <td>4.926939</td>
      <td>4.822110</td>
      <td>4.717281</td>
      <td>4.612453</td>
      <td>4.507625</td>
      <td>4.402796</td>
      <td>4.297967</td>
      <td>4.193139</td>
      <td>4.088311</td>
    </tr>
    <tr>
      <th>1599</th>
      <td>4.088311</td>
      <td>3.983482</td>
      <td>3.878654</td>
      <td>3.773825</td>
      <td>3.668997</td>
      <td>3.564168</td>
      <td>3.45934</td>
      <td>3.354511</td>
      <td>3.249683</td>
      <td>3.144854</td>
      <td>...</td>
      <td>5.136595</td>
      <td>5.031767</td>
      <td>4.926939</td>
      <td>4.822110</td>
      <td>4.717281</td>
      <td>4.612453</td>
      <td>4.507625</td>
      <td>4.402796</td>
      <td>4.297967</td>
      <td>4.193139</td>
    </tr>
  </tbody>
</table>
<p>1600 rows × 117 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Export to file for debugging
# main_effects.to_csv(&quot;main_effects_py.csv&quot;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># interaction effects
int_effects = pd.concat([
    GetInts(vec1=dat[&quot;acc&quot;], vecname1=&quot;acc&quot;, scaling1=rho_factor_list[&quot;acc&quot;],  
            vec2=dat[&quot;dev&quot;], vecname2=&quot;dev&quot;, scaling2=rho_factor_list[&quot;dev&quot;], 
            nperiods=num_periods, train_ind=dat[&quot;train_ind&quot;]),

    GetInts(vec1=dat[&quot;dev&quot;], vecname1=&quot;dev&quot;, scaling1=rho_factor_list[&quot;dev&quot;],  
            vec2=dat[&quot;cal&quot;], vecname2=&quot;cal&quot;, scaling2=rho_factor_list[&quot;cal&quot;], 
            nperiods=num_periods, train_ind=dat[&quot;train_ind&quot;]),
    
    GetInts(vec1=dat[&quot;acc&quot;], vecname1=&quot;acc&quot;, scaling1=rho_factor_list[&quot;acc&quot;],  
            vec2=dat[&quot;cal&quot;], vecname2=&quot;cal&quot;, scaling2=rho_factor_list[&quot;cal&quot;], 
            nperiods=num_periods, train_ind=dat[&quot;train_ind&quot;])
    ], 
    axis=&quot;columns&quot;)
</pre></div>
</div>
</div>
</div>
<p>The table looks like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>int_effects
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>I_acc_ge_2xI_dev_ge_2</th>
      <th>I_acc_ge_2xI_dev_ge_3</th>
      <th>I_acc_ge_2xI_dev_ge_4</th>
      <th>I_acc_ge_2xI_dev_ge_5</th>
      <th>I_acc_ge_2xI_dev_ge_6</th>
      <th>I_acc_ge_2xI_dev_ge_7</th>
      <th>I_acc_ge_2xI_dev_ge_8</th>
      <th>I_acc_ge_2xI_dev_ge_9</th>
      <th>I_acc_ge_2xI_dev_ge_10</th>
      <th>I_acc_ge_2xI_dev_ge_11</th>
      <th>...</th>
      <th>I_acc_ge_39xI_cal_ge_30</th>
      <th>I_acc_ge_39xI_cal_ge_31</th>
      <th>I_acc_ge_39xI_cal_ge_32</th>
      <th>I_acc_ge_39xI_cal_ge_33</th>
      <th>I_acc_ge_39xI_cal_ge_34</th>
      <th>I_acc_ge_39xI_cal_ge_35</th>
      <th>I_acc_ge_39xI_cal_ge_36</th>
      <th>I_acc_ge_39xI_cal_ge_37</th>
      <th>I_acc_ge_39xI_cal_ge_38</th>
      <th>I_acc_ge_39xI_cal_ge_39</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1595</th>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>...</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
    </tr>
    <tr>
      <th>1596</th>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>...</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
    </tr>
    <tr>
      <th>1597</th>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>...</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
    </tr>
    <tr>
      <th>1598</th>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>...</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
    </tr>
    <tr>
      <th>1599</th>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>...</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
    </tr>
  </tbody>
</table>
<p>1600 rows × 3629 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Export to file for debugging
# int_effects.to_csv(&quot;int_effects_py.csv&quot;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>varset = pd.concat([main_effects, int_effects], axis=&quot;columns&quot;)
varset_train = varset.loc[dat.train_ind]

# drop any constant columns over the training data set
# do this by identifying the constant columns and dropping them
keep_cols = (varset != varset.iloc[0]).any()

varset = varset.loc[:, keep_cols] 
varset_train = varset_train.loc[:, keep_cols]

# now add these variables into an extended data object
# remove anything not used in modelling
dat_plus =  pd.concat(
    [
        dat[[&quot;pmts&quot;, &quot;train_ind&quot;]], 
        varset
    ], 
    axis=&quot;columns&quot;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="setup-for-lasso">
<h3>Setup for LASSO<a class="headerlink" href="#setup-for-lasso" title="Permalink to this headline">#</a></h3>
<p>First we need to set up the data for the model. This differs from the tree-based models task as follows in that the input variables are all the basis functions we created and not the raw accident / development / calendar quarter terms - the <code class="docutils literal notranslate"><span class="pre">dat_plus</span></code> data.table.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X_train_plus = varset[dat.train_ind]
X_test_plus = varset.loc[dat.train_ind == False]
X_plus = varset
</pre></div>
</div>
</div>
</div>
</section>
<section id="diy-glmnet-with-pytorch">
<h3>DIY GLMnet with Pytorch<a class="headerlink" href="#diy-glmnet-with-pytorch" title="Permalink to this headline">#</a></h3>
<p>The results will not be identical to those in the paper since the Python implementation is different and uses different hyper-parameters.</p>
<p>We want to use GLM with LASSO regularization and CV, but at the time of writing, the implementation in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> has major limitations.</p>
<p>The <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PoissonRegressor.html">PoissonRegressor</a> does not have “warm restarts” which allows re-using the coefficients as it tests through different regularisation factors, so CV is slow. More importantly, its <code class="docutils literal notranslate"><span class="pre">alpha</span></code> regularization parameter is a l2 ridge regression penalty, not the l1 LASSO penalty.</p>
<p>As far as alternatives go, the following are unlikely to meet our needs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> seemed incredibly slow in our early experiments, taking over a day to train a single model on this data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pyglmnet</span></code> also seemed incredibly slow.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">glmnet_py</span></code> requires Linux. It should in theory run via <a class="reference external" href="https://colab.research.google.com/">Google Colab</a> or <a class="reference external" href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">Windows Subsystem for Linux</a> as well, but for this particular tutorial we wanted to share a multi-platform solution.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">python-glmnet</span></code> only implements linear and logistic regression.</p></li>
</ul>
<p>Our options seem to be using the cluster computing framework <a class="reference external" href="https://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/intro.html">h2o.ai</a> or creating our own LASSO using a neural network framework like <strong>pytorch</strong>.</p>
<p>Both seem a little overkill but, but since we’re planning to cover more on neural networks in future articles, we can use <strong>pytorch</strong> to introduce some concepts here.</p>
<p>Firstly we will just import pytorch. If you are running this on your pc and have a GPU you would like to utilise, make sure you <a class="reference external" href="https://pytorch.org/get-started/locally/">install the right version for best performance</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import torch
import torch.nn as nn
from torch.distributions import Normal, OneHotCategorical
import torch.nn.functional as F

torch.manual_seed(0)

from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
</pre></div>
</div>
</div>
</div>
<p>First we will create a <strong>Pytorch Module</strong> for the GLM. This is pretty straightforward - a neural network with no hidden layers is a LM, and by applying an exponential to the output, we have a log-link GLM.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class LogLinkGLM(nn.Module):
    # Define the parameters in __init__
    def __init__(
        self, 
        n_input=3): # number of inputs

        
        super(LogLinkGLM, self).__init__()
        
        
        self.linear = torch.nn.Linear(n_input, 1)  # These will be the coefficients
        nn.init.zeros_(self.linear.weight)         # Initialise these to zero


    # forward defines how you get y from X.
    def forward(self, x):
        return torch.exp(self.linear(x))  # log(Y) = XB -&gt; Y = exp(XB)
</pre></div>
</div>
</div>
</div>
<p>Next, to fit it within our <strong>scikit-learn</strong> framework, we will make a <strong>scikit-learn</strong> Regressor. The key components are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>: Define any hyperparameters here. At a minimum, this would include the lasso regularization penalty.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fit</span></code>: Define the training process for the model. For <code class="docutils literal notranslate"><span class="pre">pytorch</span></code> we have a training loop where we read data, calculated the loss, backpropogate it and update the weights.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">predict</span></code>: Define how to get predictions - i.e. apply the <code class="docutils literal notranslate"><span class="pre">forward</span></code> method of the Module.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">score</span></code>: Define performance. Here we will use RMSE for consistency with the notebook.</p></li>
</ul>
<p>There is also some added boilerplate about getting data into the right format.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class LogLinkGLMNetRegressor(BaseEstimator, RegressorMixin):

    def __init__(
        self, 
        l1_penalty=0.0,          # lambda is a reserved word
        weight_decay=0.0,
        max_iter=200000,
        lr=0.01,
        min_improvement=0.01,
        check_improvement_every_iter=5000,
        patience=3,
        init_weight=None,
        init_bias=None,
        verbose=1,
        target_device=torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)):
        &quot;&quot;&quot; Log Link GLM LASSO Regressor

        This trains a GLM with Poisson loss, Log Link and l1 LASSO penalties
        using Pytorch. It has early stopping 

        Note:
            Do not include the `self` parameter in the ``Args`` section.

        Args:
            l1_penalty (float): l1 penalty factor (we use l1_penalty because lambda is
                a reserved word in Python for anonymous functions)

            weight_decay (float): weight decay - analogous to l2 penalty factor
            
            max_iter (int): Maximum number of epochs before training stops

            lr (float): Learning rate

            min_improvement (float), patience (int), check_improvement_every_iter (int): 
                RMSE must improve by this percent otherwise patience will decrement. 
                Once patience reaches zero, training stops.

            verbose (int): 0 means don&#39;t print. 1 means do print.

            init_weight, init_bias (torch tensor): initial weight (coefficient) 
                and bias (intercept) for a warm start. 
        &quot;&quot;&quot;
        self.l1_penalty = l1_penalty
        self.weight_decay = weight_decay
        self.max_iter = max_iter
        self.target_device = target_device
        self.min_improvement = min_improvement
        self.lr = lr
        self.patience = patience
        self.init_weight = init_weight
        self.init_bias = init_bias
        self.check_improvement_every_iter = check_improvement_every_iter
        self.verbose = verbose

    def fit(self, X, y):

        # Check that X and y have correct shape
        X, y = check_X_y(X, y)

        # Skorch is picky about format
        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):
            X = X.values
        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):
            y = y.values

        # Convert to Pytorch Tensor
        X_tensor = torch.from_numpy(X.astype(np.float32)).to(self.target_device)
        y_tensor = torch.from_numpy(y.astype(np.float32)).reshape(-1, 1).to(self.target_device)

        n_input = X.shape[-1]
        self.module_ = LogLinkGLM(n_input=n_input).to(self.target_device)

        # Set initial weight
        if self.init_weight is not None:
            self.module_.linear.weight.data = self.init_weight

        # Set initial intercept 
        if self.init_bias is not None:
            self.module_.linear.bias.data = self.init_bias
        else:
            # To something close so gradients do not explode
            avg_log_y = np.log(np.mean(y))
            self.module_.linear.bias.data = torch.tensor([avg_log_y])

        optimizer = torch.optim.AdamW(
            params=self.module_.parameters(),
            lr=self.lr,
            weight_decay=self.weight_decay
        )
        loss_fn = nn.PoissonNLLLoss(log_input=False).to(self.target_device)

        last_rmse = -1
        patience = self.patience

        # Training loop
        for epoch in range(self.max_iter):        # Repeat max_iter times
            y_pred = self.module_(X_tensor)  # Apply current model
            loss = loss_fn(y_pred, y_tensor) # What is the loss on it?
            loss += self.l1_penalty * self.module_.linear.weight.abs().sum()  # lasso        
            optimizer.zero_grad()            # Reset optimizer
            loss.backward()                  # Apply back propagation
            optimizer.step()                 # Update model parameters

            # Every 5,000 steps (or setting) get RMSE 
            if epoch % self.check_improvement_every_iter == 0:   
                rmse = torch.sqrt(torch.mean(torch.square(y_pred - y_tensor)))

                if self.verbose &gt; 0:
                    print(&quot;Train RMSE: &quot;, rmse.data.tolist())    # Print loss

                # Stop if not 
                if rmse.data.tolist() &gt; last_rmse * (1 - self.min_improvement):
                    patience += -1
                
                if patience == 0:
                    if self.verbose &gt; 0:
                        print(&quot;Insufficient improvement found, stopping training&quot;)
                    break

                last_rmse = rmse.data.tolist()

        # Return the regressor
        return self

    def predict(self, X):

        # Check is fit had been called
        check_is_fitted(self)

        # Input validation
        X = check_array(X)

        # Skorch is picky about format
        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):
            X = X.values     

        # Convert to Pytorch Tensor
        X_tensor = torch.from_numpy(X.astype(np.float32)).to(self.target_device)

        # Apply current model and return
        return self.module_(X_tensor).cpu().detach().numpy().ravel()


    def score(self, X, y):
        # Negative RMSE score (higher needs to be better)
        y_pred = self.predict(X)
        return -np.sqrt(np.mean((y_pred - y)**2))
</pre></div>
</div>
</div>
</div>
</section>
<section id="previous-best-fit">
<h3>Previous best fit<a class="headerlink" href="#previous-best-fit" title="Permalink to this headline">#</a></h3>
<p>So, can we replicate the previously tuned model from R in Python?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tic = time.perf_counter()

lasso_prev = LogLinkGLMNetRegressor(l1_penalty=7674)
lasso_prev.fit(X_train_plus, y_train)

toc = time.perf_counter()
print(f&quot;Ran in {toc - tic:0.4f} seconds&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train RMSE:  428498688.0
Train RMSE:  53182680.0
Train RMSE:  52894192.0
Train RMSE:  48454752.0
Train RMSE:  47999504.0
Insufficient improvement found, stopping training
Ran in 64.2999 seconds
</pre></div>
</div>
</div>
</div>
</section>
<section id="id1">
<h3>Cross Validation<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h3>
<p>With our new Regressor we can now do CV with something that resembles warm starts by putting an initial fit (here we will recycle the previous best GLM from R) as the starting weights for the search.</p>
<p>The key parameter to tune in lasso is the regularisation penalty. With <strong>glmnet</strong> in R and in most academic literature, this is typically called <code class="docutils literal notranslate"><span class="pre">lambda</span></code>. However, with scikit-learn in Python, with <code class="docutils literal notranslate"><span class="pre">lambda</span></code> being a reserved keyword for anonymous functions, we will call the same parameter <code class="docutils literal notranslate"><span class="pre">l1_penalty</span></code>.</p>
<p>With our DIY “semi warm start” the training is faster but still takes a while. To keep a reasonable running time for running the example we will keep it to ten runs near the optimal lambda from the R run, but if we were building the model from scratch we would be testing many more values of lambda.</p>
<p>If running time is a critical factor and it is taking too long, three considerations:</p>
<ol class="simple">
<li><p><strong>pytorch</strong> runs faster with a GPU,</p></li>
<li><p>You can get much smarter with the warm start logic.</p></li>
<li><p>You can also consider the GLM implementation in the <a class="reference external" href="http://h2o.ai">h2o.ai</a> package mentioned earlier which is quite fast.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>lambdas = [np.exp(0.1 * x) for x in range(86, 96)]
lambdas
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[5431.659591362978,
 6002.912217261029,
 6634.24400627789,
 7331.973539155995,
 8103.083927575384,
 8955.292703482508,
 9897.129058743927,
 10938.019208165191,
 12088.380730216988,
 13359.726829661873]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>parameters_lasso = {
    &quot;l1_penalty&quot;: lambdas, 
    &quot;init_weight&quot;: [lasso_prev.module_.linear.weight.data],
    &quot;init_bias&quot;: [lasso_prev.module_.linear.bias.data],
    &quot;lr&quot;: [0.001], # Fine tuning?
    &quot;check_improvement_every_iter&quot;: [100], 
    &quot;verbose&quot;: [0]
}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tic = time.perf_counter()

# N.b. this will not run in parallel.
lasso = GridSearchCV(
    LogLinkGLMNetRegressor(), 
    parameters_lasso, 
    verbose=1)

lasso.fit(X_train_plus, y_train)
toc = time.perf_counter()
print(f&quot;Ran in {toc - tic:0.4f} seconds&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting 5 folds for each of 10 candidates, totalling 50 fits
Ran in 34.7665 seconds
</pre></div>
</div>
</div>
</div>
<p>Let’s plot the test error vs different regularisation parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Check the path includes a minimum value of error
pd.DataFrame({
  &quot;log(lambda)&quot;: [np.log(p[&quot;l1_penalty&quot;]) for p in lasso.cv_results_[&quot;params&quot;]],
  &quot;mean_test_score&quot;: lasso.cv_results_[&quot;mean_test_score&quot;]
}).plot(x=&quot;log(lambda)&quot;, y=&quot;mean_test_score&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:xlabel=&#39;log(lambda)&#39;&gt;
</pre></div>
</div>
<img alt="../_images/MLRWP_Py_triangles_example_109_1.png" src="../_images/MLRWP_Py_triangles_example_109_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(&quot;Best regularisation parameter [log(lambda)] was:&quot;) 
print(np.log(lasso.best_estimator_.get_params()[&quot;l1_penalty&quot;]))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best regularisation parameter [log(lambda)] was:
9.1
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Get the best fit:
lasso.best_estimator_.get_params()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;check_improvement_every_iter&#39;: 100,
 &#39;init_bias&#39;: tensor([15.3488]),
 &#39;init_weight&#39;: tensor([[-1.1317e+00, -8.1524e-01, -5.3453e-01,  ...,  1.3915e-04,
           1.3895e-04,  1.3895e-04]]),
 &#39;l1_penalty&#39;: 8955.292703482508,
 &#39;lr&#39;: 0.001,
 &#39;max_iter&#39;: 200000,
 &#39;min_improvement&#39;: 0.01,
 &#39;patience&#39;: 3,
 &#39;target_device&#39;: device(type=&#39;cpu&#39;),
 &#39;verbose&#39;: 0,
 &#39;weight_decay&#39;: 0.0}
</pre></div>
</div>
</div>
</div>
<p>We arrive back at the original value of <code class="docutils literal notranslate"><span class="pre">lambda</span></code>.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="model-analysis">
<h1>Model analysis<a class="headerlink" href="#model-analysis" title="Permalink to this headline">#</a></h1>
<p>This is the interesting bit - let’s look at all the models to see how they’ve performed (but remember not to draw too many conclusions about model performance from this example as discussed earlier!).</p>
<section id="consolidate-results">
<h2>Consolidate results<a class="headerlink" href="#consolidate-results" title="Permalink to this headline">#</a></h2>
<p>Now we will consolidate results for these models. We will gather together:</p>
<ul class="simple">
<li><p>RMSE for past and future data</p></li>
<li><p>predictions for each model.</p></li>
</ul>
<p>Note that <strong>scikit-learn</strong> has various tuning tools which we haven’t used here yet - partially because we want to focus on the concepts rather than the code and partially because we want to compare the results to a couple of other models not fitted in the same framework. If you’re interested in learning more then the section on <a class="reference external" href="https://scikit-learn.org/stable/modules/grid_search.html">tuning</a> in the documentation is a good start.</p>
<p>First, we’ll set up a data frame to hold the model projections for each model and populate these projections for each model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Dataset of all forecasts
model_forecasts = dat.copy()
model_forecasts[&quot;Decision Tree&quot;] = grid_tree.best_estimator_.predict(X)
model_forecasts[&quot;Random Forest&quot;] = grid_forest.best_estimator_.predict(X)
model_forecasts[&quot;Classic GBM&quot;] = grid_gbm.best_estimator_.predict(X)
model_forecasts[&quot;Histogram GBM&quot;] = grid_hgbm.best_estimator_.predict(X)
model_forecasts[&quot;Hist GBM (prev)&quot;] = gbm_prev.predict(X)
model_forecasts[&quot;Neural Network&quot;] = neural_network.predict(X)
model_forecasts[&quot;Chain Ladder&quot;] = chain_ladder.predict(X)
model_forecasts[&quot;LASSO&quot;] = lasso.predict(X_plus)
model_forecasts[&quot;LASSO (prev)&quot;] = lasso_prev.predict(X_plus)
</pre></div>
</div>
</div>
</div>
<p>Here’s how our collated forecast DataFrame looks so far:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model_forecasts
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pmts</th>
      <th>acc</th>
      <th>dev</th>
      <th>cal</th>
      <th>mu</th>
      <th>train_ind</th>
      <th>accf</th>
      <th>devf</th>
      <th>Decision Tree</th>
      <th>Random Forest</th>
      <th>Classic GBM</th>
      <th>Histogram GBM</th>
      <th>Hist GBM (prev)</th>
      <th>Neural Network</th>
      <th>Chain Ladder</th>
      <th>LASSO</th>
      <th>LASSO (prev)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.426712e+05</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>7.165313e+04</td>
      <td>True</td>
      <td>0</td>
      <td>0</td>
      <td>2.426712e+05</td>
      <td>1.887456e+05</td>
      <td>6.014116e+06</td>
      <td>3.500558e+04</td>
      <td>4.569216e+04</td>
      <td>2.034437e+04</td>
      <td>3.585367e+04</td>
      <td>4633215.0</td>
      <td>4641892.5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.640013e+05</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.042776e+06</td>
      <td>True</td>
      <td>0</td>
      <td>1</td>
      <td>1.640013e+05</td>
      <td>2.087371e+05</td>
      <td>6.072307e+06</td>
      <td>5.235618e+05</td>
      <td>7.127655e+05</td>
      <td>2.364422e+05</td>
      <td>1.234291e+06</td>
      <td>5844869.0</td>
      <td>5853403.5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.224478e+06</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>4.362600e+06</td>
      <td>True</td>
      <td>0</td>
      <td>2</td>
      <td>3.224478e+06</td>
      <td>3.578825e+06</td>
      <td>6.259892e+06</td>
      <td>2.676693e+06</td>
      <td>2.217522e+06</td>
      <td>2.121035e+06</td>
      <td>3.324359e+06</td>
      <td>8970644.0</td>
      <td>8977765.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.682531e+06</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>1.095567e+07</td>
      <td>True</td>
      <td>0</td>
      <td>3</td>
      <td>3.682531e+06</td>
      <td>5.627768e+06</td>
      <td>9.945203e+06</td>
      <td>5.691426e+06</td>
      <td>5.253524e+06</td>
      <td>4.810498e+06</td>
      <td>8.746143e+06</td>
      <td>15132947.0</td>
      <td>15134997.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.014937e+07</td>
      <td>1.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>2.080055e+07</td>
      <td>True</td>
      <td>0</td>
      <td>4</td>
      <td>1.014937e+07</td>
      <td>1.605141e+07</td>
      <td>2.417595e+07</td>
      <td>1.582105e+07</td>
      <td>1.564311e+07</td>
      <td>1.021459e+07</td>
      <td>1.715827e+07</td>
      <td>24922094.0</td>
      <td>24914536.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1595</th>
      <td>6.265737e+07</td>
      <td>40.0</td>
      <td>36.0</td>
      <td>75.0</td>
      <td>8.285330e+07</td>
      <td>False</td>
      <td>39</td>
      <td>35</td>
      <td>3.276252e+09</td>
      <td>3.088414e+09</td>
      <td>2.819504e+09</td>
      <td>3.895124e+08</td>
      <td>3.132752e+08</td>
      <td>1.119816e+08</td>
      <td>2.794976e+08</td>
      <td>155724928.0</td>
      <td>140357952.0</td>
    </tr>
    <tr>
      <th>1596</th>
      <td>6.346768e+07</td>
      <td>40.0</td>
      <td>37.0</td>
      <td>76.0</td>
      <td>6.268272e+07</td>
      <td>False</td>
      <td>39</td>
      <td>36</td>
      <td>3.276252e+09</td>
      <td>3.088414e+09</td>
      <td>2.819504e+09</td>
      <td>3.895124e+08</td>
      <td>3.132752e+08</td>
      <td>1.005221e+08</td>
      <td>1.896219e+09</td>
      <td>200310144.0</td>
      <td>182843872.0</td>
    </tr>
    <tr>
      <th>1597</th>
      <td>2.604198e+07</td>
      <td>40.0</td>
      <td>38.0</td>
      <td>77.0</td>
      <td>4.722784e+07</td>
      <td>False</td>
      <td>39</td>
      <td>37</td>
      <td>3.276252e+09</td>
      <td>3.088414e+09</td>
      <td>2.819504e+09</td>
      <td>3.895124e+08</td>
      <td>3.132752e+08</td>
      <td>9.023538e+07</td>
      <td>4.350021e+08</td>
      <td>257674688.0</td>
      <td>238155184.0</td>
    </tr>
    <tr>
      <th>1598</th>
      <td>3.394727e+07</td>
      <td>40.0</td>
      <td>39.0</td>
      <td>78.0</td>
      <td>3.544488e+07</td>
      <td>False</td>
      <td>39</td>
      <td>38</td>
      <td>3.276252e+09</td>
      <td>3.088414e+09</td>
      <td>2.819504e+09</td>
      <td>3.895124e+08</td>
      <td>3.132752e+08</td>
      <td>8.100099e+07</td>
      <td>5.732754e+09</td>
      <td>331492544.0</td>
      <td>310216256.0</td>
    </tr>
    <tr>
      <th>1599</th>
      <td>3.725869e+07</td>
      <td>40.0</td>
      <td>40.0</td>
      <td>79.0</td>
      <td>2.650330e+07</td>
      <td>False</td>
      <td>39</td>
      <td>39</td>
      <td>3.276252e+09</td>
      <td>3.088414e+09</td>
      <td>2.819504e+09</td>
      <td>3.895124e+08</td>
      <td>3.132752e+08</td>
      <td>7.271181e+07</td>
      <td>6.280085e+08</td>
      <td>426433856.0</td>
      <td>404176352.0</td>
    </tr>
  </tbody>
</table>
<p>1600 rows × 17 columns</p>
</div></div></div>
</div>
</section>
<section id="rmse">
<h2>RMSE<a class="headerlink" href="#rmse" title="Permalink to this headline">#</a></h2>
<p>We are going to do these calculations by using the <strong>scikit-learn</strong> benchmark tools. Since the LASSO model is the only one using the <strong>plus</strong> datasets, we will use that as the starting example</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Empty list to store results.
scikit_results = []
y_predicted_full_results = {}
y_predicted_test_results = {&quot;Actuals&quot;: y_test}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># LASSO uses the &quot;plus&quot; datasets
# Train:
y_predicted_train = lasso.predict(X_train_plus)
train_rmse = mean_squared_error(y_train, y_predicted_train, squared=False)

# Test:
y_predicted_test = lasso.predict(X_test_plus)
test_rmse = mean_squared_error(y_test, y_predicted_test, squared=False)

# Append to the list of results.
y_predicted_full_results[&quot;LASSO&quot;] = lasso.predict(X_plus)
y_predicted_test_results[&quot;LASSO&quot;] = y_predicted_test

scikit_results += [{
    &quot;Name&quot;: &quot;LASSO&quot;, 
    &quot;Train RMSE&quot;: train_rmse,
    &quot;Test RMSE&quot;: test_rmse
}]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># LASSO (prev) also uses the &quot;plus&quot; datasets
# Train:
y_predicted_train = lasso_prev.predict(X_train_plus)
train_rmse = mean_squared_error(y_train, y_predicted_train, squared=False)

# Test:
y_predicted_test = lasso_prev.predict(X_test_plus)
test_rmse = mean_squared_error(y_test, y_predicted_test, squared=False)

# Append to the list of results.
y_predicted_full_results[&quot;LASSO (prev)&quot;] = lasso_prev.predict(X_plus)
y_predicted_test_results[&quot;LASSO (prev)&quot;] = y_predicted_test

scikit_results += [{
    &quot;Name&quot;: &quot;LASSO (prev)&quot;, 
    &quot;Train RMSE&quot;: train_rmse,
    &quot;Test RMSE&quot;: test_rmse
}]
</pre></div>
</div>
</div>
</div>
<p>and then for the rest we put it in a loop running on the <strong>“non-plussed”</strong> datasets:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Using zip in a for loop means pipe and name will run through 
# the tuples of the two lists in that order.
for pipe, name in zip(
        [grid_tree.best_estimator_, grid_forest.best_estimator_, grid_gbm.best_estimator_, 
         grid_hgbm.best_estimator_, gbm_prev, neural_network, chain_ladder],
        [&quot;Decision Tree&quot;, &quot;Random Forest&quot;, &quot;Classic GBM&quot;, &quot;Histogram GBM&quot;, &quot;Hist GBM (prev)&quot;, &quot;Neural Network&quot;, &quot;Chain Ladder&quot;]
    ):

    y_predicted_train = pipe.predict(X_train)
    train_rmse = mean_squared_error(y_train, y_predicted_train, squared=False)

    y_predicted_test = pipe.predict(X_test)
    test_rmse = mean_squared_error(y_test, y_predicted_test, squared=False)

    y_predicted_full_results[name] = pipe.predict(X)
    y_predicted_test_results[name] = y_predicted_test

    scikit_results += [{
    &quot;Name&quot;: name, 
    &quot;Train RMSE&quot;: train_rmse,
    &quot;Test RMSE&quot;: test_rmse
    }]
</pre></div>
</div>
</div>
</div>
<p>Let’s have a look at these results for past and future data sets with results ranked by test RMSE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>df_results = pd.DataFrame(scikit_results).sort_values(&quot;Test RMSE&quot;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>pd.set_option(&#39;display.float_format&#39;, &#39;{0:,.0f}&#39;.format)
display(df_results)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Name</th>
      <th>Train RMSE</th>
      <th>Test RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>LASSO</td>
      <td>48,283,020</td>
      <td>252,009,808</td>
    </tr>
    <tr>
      <th>1</th>
      <td>LASSO (prev)</td>
      <td>49,082,112</td>
      <td>269,974,336</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Hist GBM (prev)</td>
      <td>40,110,165</td>
      <td>313,377,932</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Histogram GBM</td>
      <td>34,464,022</td>
      <td>455,524,543</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Neural Network</td>
      <td>274,930,720</td>
      <td>930,978,112</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Classic GBM</td>
      <td>42,979,008</td>
      <td>1,603,949,161</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Chain Ladder</td>
      <td>138,528,934</td>
      <td>1,722,351,650</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Random Forest</td>
      <td>30,698,739</td>
      <td>1,789,873,775</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Decision Tree</td>
      <td>0</td>
      <td>1,916,509,816</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The key indicator is performance on a hold-out data set, in this case the future data. Otherwise, looking at Train RMSE, the decision tree would be perfect, having overfit the data to a loss of zero.</p>
<p>For the future data, we see a very different story. As expected, the tuned decision tree does appear to be over-fitted, performing the worst.</p>
<p>The LASSO model performs the best. The CV tuned model is pretty similar to the previous model from the R run.</p>
<p>The histogram GBM model follows. The “previously tuned” version has essentially identical performance to the XGBoost model from the R version of this article.</p>
<p>The simple neural network follows that.</p>
</section>
<section id="visualising-the-fit">
<h2>Visualising the fit<a class="headerlink" href="#visualising-the-fit" title="Permalink to this headline">#</a></h2>
<p>Although useful, the RMSE is just a single number so it’s helpful to visualise the fit.
In particular, because these are models for a reserving data set, we can take advantage of that structure when analysing how well each model is performing.</p>
<section id="fitted-values">
<h3>Fitted values<a class="headerlink" href="#fitted-values" title="Permalink to this headline">#</a></h3>
<p>First, lets show visualise the fitted payments.
The graphs below show the log(fitted values), or log(payments) in the case of the actual values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>for name, y_pred in y_predicted_test_results.items():
    # Skip prev models - these are similar enough to last version
    if name not in [&quot;LASSO (prev)&quot;, &quot;Hist GBM (prev)&quot;]:  
        # Plot
        dat_ = dat.copy() 
        dat_.loc[dat_.train_ind == False, &quot;pmts&quot;] = y_pred
        
        plot_triangle(
            dat_.pivot(index=&quot;acc&quot;, columns=&quot;dev&quot;, values=&quot;pmts&quot;),
            mask_bottom=False,
            title=name
        )

plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_130_0.png" src="../_images/MLRWP_Py_triangles_example_130_0.png" />
<img alt="../_images/MLRWP_Py_triangles_example_130_1.png" src="../_images/MLRWP_Py_triangles_example_130_1.png" />
<img alt="../_images/MLRWP_Py_triangles_example_130_2.png" src="../_images/MLRWP_Py_triangles_example_130_2.png" />
<img alt="../_images/MLRWP_Py_triangles_example_130_3.png" src="../_images/MLRWP_Py_triangles_example_130_3.png" />
<img alt="../_images/MLRWP_Py_triangles_example_130_4.png" src="../_images/MLRWP_Py_triangles_example_130_4.png" />
<img alt="../_images/MLRWP_Py_triangles_example_130_5.png" src="../_images/MLRWP_Py_triangles_example_130_5.png" />
<img alt="../_images/MLRWP_Py_triangles_example_130_6.png" src="../_images/MLRWP_Py_triangles_example_130_6.png" />
<img alt="../_images/MLRWP_Py_triangles_example_130_7.png" src="../_images/MLRWP_Py_triangles_example_130_7.png" />
</div>
</div>
<p>Some things are apparent from this:</p>
<ul class="simple">
<li><p>The lack of interactions or diagonal effects in the Chain ladder</p></li>
<li><p>All the machine learning models do detect and project the interaction</p></li>
<li><p>The blocky nature of the decision tree and the overfit to the past data</p></li>
<li><p>The random forest and GBMs fit look smoother since they are a function of a number of trees.</p></li>
<li><p>The LASSO and neural network fits are the smoothest, which is not surprising since it consists of continuous functions.</p></li>
<li><p>Visually, the LASSO seems to capture the interaction the best.</p></li>
</ul>
</section>
<section id="actual-vs-fitted-heat-maps">
<h3>Actual vs Fitted heat maps<a class="headerlink" href="#actual-vs-fitted-heat-maps" title="Permalink to this headline">#</a></h3>
<p>We can also look at the model fits via heat maps of the actual/fitted values.</p>
<p>For triangular reserving data, these types of plots are very helpful when examining plot fit.</p>
<p>First, here’s a function to draw the heatmaps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>for name, y_pred in y_predicted_full_results.items():
    # Skip prev models - these are similar enough to last version
    if name not in [&quot;LASSO (prev)&quot;, &quot;Hist GBM (prev)&quot;]: 

        # Plot
        dat_ = dat.copy() 
        dat_[&quot;pmts_ratio&quot;] = dat_.pmts / y_pred
        
        cmap = sns.diverging_palette(255,0,sep=16, as_cmap=True)  # to be similar to R colouring
        
        plot_triangle(
            dat_.pivot(index=&quot;acc&quot;, columns=&quot;dev&quot;, values=&quot;pmts_ratio&quot;),
            mask_bottom=False,
            title=name,
            cmap=cmap  # vlag also not a bad palette for this
        )

plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_133_0.png" src="../_images/MLRWP_Py_triangles_example_133_0.png" />
<img alt="../_images/MLRWP_Py_triangles_example_133_1.png" src="../_images/MLRWP_Py_triangles_example_133_1.png" />
<img alt="../_images/MLRWP_Py_triangles_example_133_2.png" src="../_images/MLRWP_Py_triangles_example_133_2.png" />
<img alt="../_images/MLRWP_Py_triangles_example_133_3.png" src="../_images/MLRWP_Py_triangles_example_133_3.png" />
<img alt="../_images/MLRWP_Py_triangles_example_133_4.png" src="../_images/MLRWP_Py_triangles_example_133_4.png" />
<img alt="../_images/MLRWP_Py_triangles_example_133_5.png" src="../_images/MLRWP_Py_triangles_example_133_5.png" />
<img alt="../_images/MLRWP_Py_triangles_example_133_6.png" src="../_images/MLRWP_Py_triangles_example_133_6.png" />
</div>
</div>
<p>All models have shortcomings, but the LASSO and previously tuned XGBoost models outperform the others.</p>
</section>
<section id="quarterly-tracking">
<h3>Quarterly tracking<a class="headerlink" href="#quarterly-tracking" title="Permalink to this headline">#</a></h3>
<p>Finally, we will look at the predictions for specific parts of the data set.
Quarterly tracking graphs:</p>
<ul class="simple">
<li><p>Plot the actual and fitted payments, including future values, by one of accident / development / calendar period</p></li>
<li><p>Hold one of the other triangle directions fixed, meaning that the third direction is then determined.</p></li>
<li><p>Additionally plot the underlying mean from which the data were simulated.</p></li>
</ul>
<p>We’ll use a couple of functions because we want to do this repeatedly.</p>
<p>The first function (<code class="docutils literal notranslate"><span class="pre">GraphModelVals</span></code>) is from the blog on the LASSO model, and draws a single tracking graph.
The second function (<code class="docutils literal notranslate"><span class="pre">QTracking</span></code>) generates the graphs for all models and uses the <strong>patchwork</strong> package to wrap them into a single plot.</p>
<p>Let’s have a look at the tracking for accident quarter when development quarter = 5.
We’ll wrap this in another function since we want to call this for a few different combinations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model_names = [x[&quot;Name&quot;] for x in scikit_results if name not in [&quot;LASSO (prev)&quot;, &quot;Hist GBM (prev)&quot;]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def QTrack(model_forecasts, model_names, period_to_filter, period, total_num_periods, plot_mu=True):
    &quot;&quot;&quot;
    Plots model vs payments and mu
    
    Args:
    model_forecasts: Pandas DataFrame. Should have dev, acc, mu, pmts 
    model_names: List of model names to plot Should be columns in model_forecasts
    period_to_filter: &quot;dev&quot; or &quot;acc&quot;. The period to filter by
    period: integer. filter value for period_to_filter. I.e. period_to_filter=&quot;acc&quot;, period=5 filters for acc=5.
    total_num_periods: total number of periods. First total_num_periods - period periods are greyed out.
    &quot;&quot;&quot;
    fig, axs = plt.subplots(len(model_names), 1, sharex=True, figsize=(4, 1.8 * len(model_names)))
    
    other_period = [x for x in [&quot;dev&quot;, &quot;acc&quot;] if x != period_to_filter][0]
    
    for model_name, ax in zip(model_names, axs):
        # Shade historical quarters
        ax.axvspan(0, total_num_periods - period, facecolor=plot_colors[&quot;lgrey&quot;])
        
        # Plot mu
        if plot_mu:
            ax.plot(
            model_forecasts.loc[model_forecasts[period_to_filter] == period, other_period].values,
            np.log(model_forecasts.loc[model_forecasts[period_to_filter] == period, &quot;mu&quot;].values),
            color=plot_colors[&quot;dgrey&quot;],
            linestyle=&#39;:&#39;)
        
        # Plot Payments
        ax.plot(
        model_forecasts.loc[model_forecasts[period_to_filter] == period, other_period].values, 
        np.log(model_forecasts.loc[model_forecasts[period_to_filter] == period, &quot;pmts&quot;].values), 
        color=plot_colors[&quot;dblue&quot;])
        
        # Plot model
        ax.plot(
        model_forecasts.loc[model_forecasts[period_to_filter] == period, other_period].values, 
        np.log(model_forecasts.loc[model_forecasts[period_to_filter] == period, model_name].values), 
        color=plot_colors[&quot;red&quot;],
        linewidth=6)
            
        ax.set_ylabel(&#39;Log(Payments)&#39;)
        ax.set_title(f&#39;Modelled Values for {model_name}&#39;)
    
    if period_to_filter == &quot;acc&quot;:
        ax.set_xlabel(&#39;Development Quarter&#39;)
    else:
        ax.set_xlabel(&#39;Accident Quarter&#39;)  
    
    plt.show()
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>QTrack(model_forecasts, model_names, period_to_filter=&quot;dev&quot;, period=5, total_num_periods=num_periods)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_138_0.png" src="../_images/MLRWP_Py_triangles_example_138_0.png" />
</div>
</div>
<p>Because this is mostly old data and because it is not affected by the interaction, the tracking should be generally good.</p>
<p>That being said, the predictions are a bit all over the place.</p>
<p>The decision tree, random forest and classic GBM seem underfit, as does the neural network. The histogram GBM is decent, slightly overfit. The Chain Ladder final quarter is a bit wild as can happen with a Chain Ladder. The LASSO model is a bit rough - especially if compared to the fit from the R implementation - but isn’t too bad.</p>
<p>Here are a few more tracking plots to help further illustrate the model fits.</p>
<p><strong>Tracking for accident quarter when development quarter = 24</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>QTrack(model_forecasts, model_names, period_to_filter=&quot;dev&quot;, period=24, total_num_periods=num_periods)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_140_0.png" src="../_images/MLRWP_Py_triangles_example_140_0.png" />
</div>
</div>
<p>Development quarter 24 is impacted by the interactions for accident quarters&gt;17.</p>
<p>LASSO and the histogram GBM reflects this the best. The other tree models extrapolate a flat curve, whilst the neural network continues an upward trend. The Chain Ladder overpredicts the last value.</p>
<p><strong>Tracking for accident quarter when development quarter = 35</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>QTrack(model_forecasts, model_names, period_to_filter=&quot;dev&quot;, period=35, total_num_periods=num_periods)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_143_0.png" src="../_images/MLRWP_Py_triangles_example_143_0.png" />
</div>
</div>
<p>This is quite hard for the models since most of the values are in the future. The neural network fits a smooth curve. The histogram GBM and the LASSO work well here.</p>
<p>We can look at similar plots by development quarter for older and newer accident quarters</p>
<p><strong>Tracking for development quarter when accident quarter = 5</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>QTrack(model_forecasts, model_names, period_to_filter=&quot;acc&quot;, period=5, total_num_periods=num_periods)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_147_0.png" src="../_images/MLRWP_Py_triangles_example_147_0.png" />
</div>
</div>
<p>The decision tree, random forest and classical GBM all project a flat line, but the others generally capture the downward trend.</p>
<p><strong>Tracking for development quarter when accident quarter = 20</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>QTrack(model_forecasts, model_names, period_to_filter=&quot;acc&quot;, period=20, total_num_periods=num_periods)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_150_0.png" src="../_images/MLRWP_Py_triangles_example_150_0.png" />
</div>
</div>
<p>Again, the LASSO, the histogram GBM, and neural network are all able to capture the trend.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="reserves">
<h1>Reserves<a class="headerlink" href="#reserves" title="Permalink to this headline">#</a></h1>
<p>Finally, we’ll look at the reserve estimates from the different models:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>df_reserve_estimates = (
    model_forecasts
    .loc[model_forecasts.train_ind==False]
    .groupby(&quot;acc&quot;)
    .agg(&quot;sum&quot;)
    .drop(columns=[&quot;pmts&quot;, &quot;accf&quot;, &quot;devf&quot;, &quot;dev&quot;, &quot;cal&quot;, &quot;train_ind&quot;])
)

df_reserve_estimates.plot(ylim=(0, 1.0e11))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:xlabel=&#39;acc&#39;&gt;
</pre></div>
</div>
<img alt="../_images/MLRWP_Py_triangles_example_153_1.png" src="../_images/MLRWP_Py_triangles_example_153_1.png" />
</div>
</div>
<p>Finally, here are the overall reserves, summed over all accident quarters</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>df_reserve_summary= pd.DataFrame.from_dict({
    name: y_pred.sum() / 10**9
    for name, y_pred in y_predicted_test_results.items()},
    orient=&quot;index&quot;,
    columns=[&quot;Reserves ($B)&quot;]
    )

df_reserve_summary[&quot;Ratio to actual(%)&quot;] = (
    df_reserve_summary[&quot;Reserves ($B)&quot;] / 
    y_predicted_test_results[&quot;Actuals&quot;].sum() * 10**11
)

df_reserve_summary
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Reserves ($B)</th>
      <th>Ratio to actual(%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Actuals</th>
      <td>608</td>
      <td>100</td>
    </tr>
    <tr>
      <th>LASSO</th>
      <td>596</td>
      <td>98</td>
    </tr>
    <tr>
      <th>LASSO (prev)</th>
      <td>579</td>
      <td>95</td>
    </tr>
    <tr>
      <th>Decision Tree</th>
      <td>1,678</td>
      <td>276</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>1,610</td>
      <td>265</td>
    </tr>
    <tr>
      <th>Classic GBM</th>
      <td>1,497</td>
      <td>246</td>
    </tr>
    <tr>
      <th>Histogram GBM</th>
      <td>683</td>
      <td>112</td>
    </tr>
    <tr>
      <th>Hist GBM (prev)</th>
      <td>704</td>
      <td>116</td>
    </tr>
    <tr>
      <th>Neural Network</th>
      <td>300</td>
      <td>49</td>
    </tr>
    <tr>
      <th>Chain Ladder</th>
      <td>563</td>
      <td>93</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The overall reserve for the Chain Ladder model hides the fact that this result is actually significant under-estimation in most accident quarters balanced by significant over-estimation in the last.</p>
<p>The neural network seems to be underfit overall, whilst the flat line projections by the decision tree, random forest and classic GBM are too conservative.</p>
<p>Taking this into account, the best performers on this basis are the LASSO followed by the histogram GBM.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="commentary">
<h1>Commentary<a class="headerlink" href="#commentary" title="Permalink to this headline">#</a></h1>
<p>What’s going on with the results? Can we improve matters?</p>
<p>Success in machine learning often comes down to optimising the following:</p>
<ul class="simple">
<li><p>What you model</p></li>
<li><p>How you model (i.e. what features you use)</p></li>
<li><p>Your performance measure</p></li>
<li><p>Your train/test split.</p></li>
</ul>
<section id="models">
<h2>Models<a class="headerlink" href="#models" title="Permalink to this headline">#</a></h2>
<p><strong>scikit-learn</strong> is incredibly convenient in that it gives access to so many different models. The Python syntax is also quite clean and easy to use compared to R.</p>
<p>But we ran into some limitations:</p>
<ul class="simple">
<li><p>The <strong>sci-kit</strong> GLM model didn’t have all the features we needed.</p></li>
<li><p>The classic GBM and Random Forest just performed poorly.</p></li>
<li><p>The histogram GBM held up well well. There are many more customisable functionality if we were to use XGBoost, LightGBM or CatBoost, but the performance was similar to the two XGBoost models from the R version.</p></li>
<li><p>Similarly the neural network is quite basic with the notable absence of the selection of activation functions. The target transform is a workaround but a full-featured package like <strong>tensorflow</strong> or <strong>pytorch</strong> may do better.</p></li>
</ul>
</section>
<section id="additional-features">
<h2>Additional Features<a class="headerlink" href="#additional-features" title="Permalink to this headline">#</a></h2>
<p>More complex models such as GBMs and Neural Networks tend to perform better with bigger datasets. Our datasets in the example only include three factors - only two if you consider calendar quarter is just accident plus development. In real-world examples, you should be able to augment with additional rating factors or claims features to get a more predictive result.</p>
</section>
<section id="tuning-method-and-cross-validation">
<h2>Tuning Method and Cross Validation<a class="headerlink" href="#tuning-method-and-cross-validation" title="Permalink to this headline">#</a></h2>
<p>As we noted earlier, there are a few components of randomness from one run to another, with different seeds:</p>
<ol class="simple">
<li><p>The cross validation folds will be different</p></li>
<li><p>The random search parameters will be different.</p></li>
</ol>
<p>For a small data set like this one, this has the potential to alter the results - afterall, getting a good result on the future data depends on whether the tuning gets lucky in terms of estimating the magnitude of the interaction.</p>
<p>We could reduce the variability by using a grid-search over a fixed set of parameters but - full disclosure - that led to even worse results.</p>
<section id="so-what-s-going-on">
<h3>So what’s going on?<a class="headerlink" href="#so-what-s-going-on" title="Permalink to this headline">#</a></h3>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>It might be helpful to have another look at folds used in a cross-validation search.
</pre></div>
</div>
<p>These are shown below for six-fold CV - the yellow dots represent training data points in each fold, the blue dots are the test data.
The grey rectangle marks the interaction. These are actually the folds used in the R article; the folds used here will be different in specifics, but the same general ideas hold.</p>
<p><img alt="interaction compared to available triangle" src="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-07-mlr3example/index_files/figure-html/unnamed-chunk-65-1.png" /></p>
<p>The first thing that is apparent is that the interaction only applies to a very small number of points (10) in the past data. So in that sense, it’s quite remarkable that the models perform as well as they actually do - they all detect the interaction. Where they start to slip up is that they do not necessarily follow the development quarter shape in the tail.</p>
</section>
<section id="finer-fine-tuning">
<h3>Finer fine-tuning<a class="headerlink" href="#finer-fine-tuning" title="Permalink to this headline">#</a></h3>
<p>It should be noted that to cut down the run time of this example, we only tuned over 25 sets of parameters for each model, kept models coarse via less trees, less rounds, less neurons, higher learn rates, and used five-fold cross-validation. Performance can possibly be improved by tuning over more sets of parameters, bigger models, and higher fold count CV.</p>
</section>
<section id="time-based-hold-out-testing">
<h3>Time-based hold-out testing<a class="headerlink" href="#time-based-hold-out-testing" title="Permalink to this headline">#</a></h3>
<p>Reserving is a forecasting problem - we have a time series of payments and we want to forecast into the future.</p>
<p>This suggests that cross-validation may not be an ideal method for tuning reserving models - we would be better with a train/test split where the test data is in the future relative to the past data.</p>
<p>This problem has been discussed in the context of time series modelling, where the use of rolling window cross-validation techniques has been advocated. We’ll be discussing this in a future article.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="conclusion">
<h1>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">#</a></h1>
<p>We set out to demonstrate how to fit a number of reserving models to a data set.
Our results have been mixed - but we expected that in advance.
This work lays a baseline for our upcoming articles where we will look at ways of improving the results through:</p>
<ul class="simple">
<li><p>expanding the range of models</p></li>
<li><p>considering more appropriate validation techniques.</p></li>
</ul>
<section id="what-next">
<h2>What next?<a class="headerlink" href="#what-next" title="Permalink to this headline">#</a></h2>
<p>You can try running this code and changing some things - e.g. the hyper-parameters tuned, search strategy, number of iterations etc.</p>
<p>For those looking to use <strong>scikit-learn</strong>, it is worthwhile to read about some other limitations of the models <a class="reference external" href="https://ryxcommar.com/2019/08/30/scikit-learns-defaults-are-wrong/">here</a>.</p>
<p>You can also try other data sets as described in the <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3241906">LASSO paper</a>  by changing <code class="docutils literal notranslate"><span class="pre">dataset_number</span></code> near the start of the code. More details are in the paper, but in brief:</p>
<ul class="simple">
<li><p>Data set 1 = a Chain Ladder model</p></li>
<li><p>Data set 2 = a Chain Ladder model + calendar period effects</p></li>
<li><p>Data set 3 = the data set used here</p></li>
<li><p>Data set 4 = data set 2 but where the strength of the calendar period effect varies by development period.</p></li>
</ul>
<p>There will be more Python content coming out on the blog, so keep an eye out for further <strong>developments</strong>!</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "ActuariesInstitute/cookbook",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="MLRWP_R_mlr3.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">R: Machine Learning Triangles</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="MLRWP_R_Baudry_Notebook_1_SimulateData_v1.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">R: Baudry ML Reserving Pt 1</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By YDAWG, DAPC, YAC and other contributors.<br/>
  
    <div class="extra_footer">
       The Actuaries' Analytical Cookbook is a series of data and analytics recipes to help actuaries quickly get started with a new project.   This site is intended to be a resource to actuaries in both data science and traditional fields.  Opinions expressed in this publication are the opinions of contributors and do not necessarily represent those of either the Institute of Actuaries of Australia (the ‘Institute’), its members, directors, officers, employees, agents, or that of the employers of the contributors. <br>© Institute of Actuaries of Australia and Contributors 2021. All rights reserved.
    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>