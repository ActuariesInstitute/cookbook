
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Py: Machine Learning Triangles &#8212; Actuaries&#39; Analytical Cookbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Py: Socio-Economic Index Construction" href="DAA_M06_CS1.html" />
    <link rel="prev" title="R: Machine Learning Triangles" href="MLRWP_R_mlr3.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/actuaries-logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Actuaries' Analytical Cookbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Python
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="about_py.html">
   About Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="getting_started.html">
   Setting up Your Python Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="python_by_example.html">
   An Introductory Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="learn_more.html">
   Learn More
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Useful_Python_packages.html">
   Useful Python packages for Data Science
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to R
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="about_R.html">
   About R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="getting_started_R.html">
   Setting Up R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="introductory_R.html">
   Introduction to R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="intermediate_R.html">
   Next Steps With R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="top_ten_r_packages.html">
   Useful Packages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_R_DataTable.html">
   R: data.table for actuaries
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Workflow Management
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="version_control.html">
   Version control
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Regression, Classification and Technical Price
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M05_CS1.html">
   Py: Customer Churn Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multitasking_risk_pricing.html">
   Py/R: Multitasking Risk Pricing Using Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="py_shap_values.html">
   Py: Explainable Models with SHAP
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Life Insurance
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="LifeRecipeBook.html">
   R: Life Modelling Recipes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="life_stats.html">
   R: Life Industry Stats in Tableau and R
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  General Insurance
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_R_GLMs.html">
   R: Reserving with GLMs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_R_Lasso.html">
   R: Reserving with LASSO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_R_mlr3.html">
   R: Machine Learning Triangles
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Py: Machine Learning Triangles
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unsupervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M06_CS1.html">
   Py: Socio-Economic Index Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M06_CS2.html">
   Py: Clustering Credit Card Fraud
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M06_Ex4.html">
   Py: K-means clustering of COVID dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M06_Ex5.html">
   Py: Hierarchical clustering on COVID dataset
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Natural Language Processing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="R_case_study_word_cloud.html">
   R: Word Cloud Case Study
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="textClassificationEntry.html">
   Py: Text Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M05_Ex10.html">
   Py: Decision Tree Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M05_Ex18.html">
   Py: Neural Net Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M07_CS1.html">
   Py: Classifying review sentiment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M07_CS2.html">
   Py: Customer Sentiment Analysis
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Business Optimization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_2021_S2_Tutorial10_exercise_scipy.html">
   Py: Linear Programming
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Image Recognition
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M05_CS2.html">
   Py: Image Recognition
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Ethics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="automated_decision.html">
   Automated Decision-Making Systems
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Contributing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="contributing.html">
   Contributing
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/MLRWP_Py_triangles_example.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/ActuariesInstitute/cookbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        <a class="edit-button" href="https://github.com/ActuariesInstitute/cookbook/edit/main/cookbook/docs/MLRWP_Py_triangles_example.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/ActuariesInstitute/cookbook/main?urlpath=tree/cookbook/docs/MLRWP_Py_triangles_example.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/ActuariesInstitute/cookbook/blob/main/cookbook/docs/MLRWP_Py_triangles_example.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Py: Machine Learning Triangles
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#who-is-this-article-aimed-at">
     Who is this article aimed at?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pre-requisites-to-this-article">
     Pre-requisites to this article
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-data-set">
     The data set
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#machine-learning-and-aggregate-triangles">
     Machine learning and aggregate triangles
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#don-t-rank-the-ml-models-used-based-on-this-example">
     Don’t rank the ML models used based on this example
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-scikit-learn-package">
     The scikit-learn package
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ml-methods-used">
     ML methods used
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#outline-of-the-workflow">
   Outline of the workflow
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-preparation">
   Data preparation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-pandas-package">
     The pandas package
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-the-data">
     Load the data
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-and-test">
     Train and Test
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tuning-process-for-hyper-parameters">
   Tuning process for hyper-parameters
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation">
     Cross-validation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation-in-scikit-learn">
     Cross-validation in scikit-learn
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-measure">
     Performance measure
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#searching-hyper-parameter-space">
     Searching hyper-parameter space
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-some-ml-models">
   Fitting some ML models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-tree">
     Decision tree
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fitting-a-single-model">
       Fitting a single model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tuning-the-decision-tree">
       Tuning the decision tree
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-forest-fitting">
     Random forest fitting
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gbm">
     GBM
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-network">
     Neural Network
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chainladder-the-baseline-model">
     Chainladder - the baseline model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#getting-the-chainladder-reserve-estimates">
       Getting the Chainladder reserve estimates
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lasso-regularised-regression">
     LASSO (regularised regression)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#basis-functions">
       Basis functions
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#setup-for-lasso">
       Setup for LASSO
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#diy-glmnet-with-pytorch">
       DIY GLMnet with Pytorch
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#previous-best-fit">
       Previous best fit
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Cross Validation
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-analysis">
   Model analysis
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#consolidate-results">
     Consolidate results
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rmse">
     RMSE
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualising-the-fit">
     Visualising the fit
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fitted-values">
       Fitted values
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#actual-vs-fitted-heat-maps">
       Actual vs Fitted heat maps
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#quarterly-tracking">
       Quarterly tracking
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reserves">
   Reserves
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#commentary">
   Commentary
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#models">
     Models
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#additional-features">
     Additional Features
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tuning-method-and-cross-validation">
     Tuning Method and Cross Validation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#so-what-s-going-on">
       So what’s going on?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#finer-fine-tuning">
       Finer fine-tuning
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#time-based-hold-out-testing">
       Time-based hold-out testing
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-next">
     What next?
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Py: Machine Learning Triangles</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Py: Machine Learning Triangles
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#who-is-this-article-aimed-at">
     Who is this article aimed at?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pre-requisites-to-this-article">
     Pre-requisites to this article
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-data-set">
     The data set
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#machine-learning-and-aggregate-triangles">
     Machine learning and aggregate triangles
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#don-t-rank-the-ml-models-used-based-on-this-example">
     Don’t rank the ML models used based on this example
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-scikit-learn-package">
     The scikit-learn package
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ml-methods-used">
     ML methods used
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#outline-of-the-workflow">
   Outline of the workflow
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-preparation">
   Data preparation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-pandas-package">
     The pandas package
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-the-data">
     Load the data
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-and-test">
     Train and Test
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tuning-process-for-hyper-parameters">
   Tuning process for hyper-parameters
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation">
     Cross-validation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation-in-scikit-learn">
     Cross-validation in scikit-learn
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-measure">
     Performance measure
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#searching-hyper-parameter-space">
     Searching hyper-parameter space
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-some-ml-models">
   Fitting some ML models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-tree">
     Decision tree
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fitting-a-single-model">
       Fitting a single model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tuning-the-decision-tree">
       Tuning the decision tree
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-forest-fitting">
     Random forest fitting
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gbm">
     GBM
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-network">
     Neural Network
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chainladder-the-baseline-model">
     Chainladder - the baseline model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#getting-the-chainladder-reserve-estimates">
       Getting the Chainladder reserve estimates
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lasso-regularised-regression">
     LASSO (regularised regression)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#basis-functions">
       Basis functions
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#setup-for-lasso">
       Setup for LASSO
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#diy-glmnet-with-pytorch">
       DIY GLMnet with Pytorch
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#previous-best-fit">
       Previous best fit
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Cross Validation
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-analysis">
   Model analysis
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#consolidate-results">
     Consolidate results
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rmse">
     RMSE
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualising-the-fit">
     Visualising the fit
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fitted-values">
       Fitted values
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#actual-vs-fitted-heat-maps">
       Actual vs Fitted heat maps
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#quarterly-tracking">
       Quarterly tracking
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reserves">
   Reserves
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#commentary">
   Commentary
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#models">
     Models
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#additional-features">
     Additional Features
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tuning-method-and-cross-validation">
     Tuning Method and Cross Validation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#so-what-s-going-on">
       So what’s going on?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#finer-fine-tuning">
       Finer fine-tuning
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#time-based-hold-out-testing">
       Time-based hold-out testing
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-next">
     What next?
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="py-machine-learning-triangles">
<h1>Py: Machine Learning Triangles<a class="headerlink" href="#py-machine-learning-triangles" title="Permalink to this headline">¶</a></h1>
<p><em>This article was originally created by Jacky Poon and Grainne McGuire, and published in the <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/">General Insurance Machine Learning for Reserving Working Party (“MLR-WP”) blog</a>. The MLR-WP is an international research group on machine learning techniques to reserving, with over 50 actuaries from around the globe. The goal of the group is to bring machine learning techniques into widespread adoption ‘on the ground’ by identifying what the barriers are, communicating any benefits, and helping develop the research techniques in pragmatic ways. Whilst some articles have been brought into this cookbook, consider exploring the <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/">blog</a> further for additional content including detailed walkthroughs of more advanced models.</em></p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p>In this article we look at applying different machine learning (ML) models to a data set.
Our goal is to illustrate a work flow for:</p>
<ul class="simple">
<li><p>setting up a data set for ML</p></li>
<li><p>applying different ML models to this data</p></li>
<li><p>tuning the ML hyper-parameters</p></li>
<li><p>comparing and contrasting performance for the past fitted values and the future predictions.</p></li>
</ul>
<p>A secondary goal is to demonstrate the utility of a multi-model machine learning framework which enables the user to easily “plug and play” different machine learning models within the same framework.</p>
<p>We use the <strong>scikit-learn</strong> in Python here. Followers of the blog may see significant similarities with the <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-mlr3example/">earlier <strong>mlr3</strong> article for R users</a>, except this article uses Python.</p>
<p>You can download this notebook <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/f_notebooks/ML_triangles_example.ipynb">here</a> and run it on your local machine, or on a cloud service like Google Colab. If you are looking at such a downloaded copy, find the Machine Learning for Reserving blog <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/">here</a>.</p>
<div class="section" id="who-is-this-article-aimed-at">
<h2>Who is this article aimed at?<a class="headerlink" href="#who-is-this-article-aimed-at" title="Permalink to this headline">¶</a></h2>
<p>This article is aimed to Python users who know a little about some of the standard machine learning models, but who may not have done much hands-on analysis work. It will also be useful for those who have done some experimentation, but maybe not on a reserving data set.</p>
</div>
<div class="section" id="pre-requisites-to-this-article">
<h2>Pre-requisites to this article<a class="headerlink" href="#pre-requisites-to-this-article" title="Permalink to this headline">¶</a></h2>
<p>We’ve tried to make this article accessible to people new to ML, and to make this article stand-alone. Having some knowledge about basic machine learning techniques like decision trees, random forests and gradient boosting will help. Furthermore, we also fit the Chain Ladder model as a GLM (sometimes referred to as a Stochastic Chain Ladder) and fit a particular type of LASSO model. We’ve included limited details on these models here. Although our previous blog posts are in R, they have more details on the background of these:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/glms/">Reserving with GLMs</a></p></li>
<li><p><a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-lasso/">Self-assembling claim reserving models using the LASSO</a></p></li>
</ul>
<p>We also fit some simple neural network models in this Python version.</p>
</div>
<div class="section" id="the-data-set">
<h2>The data set<a class="headerlink" href="#the-data-set" title="Permalink to this headline">¶</a></h2>
<p>Our example deals with using ML to set reserves for a single aggregate 40x40 triangle. We’ve selected the data set because:</p>
<ul class="simple">
<li><p>It’s small, so the code will run relatively quickly on your machine - there’s nothing worse than running through a worked example and having to wait hours for results!</p></li>
<li><p>A lot of reserving is still done using traditional accident (or underwriting) + development aggregated triangles so it is relevant to the real world.</p></li>
</ul>
<p>We use a simulated data set. There are several reasons for this:</p>
<ul class="simple">
<li><p>We know the future results so can examine how different reserve predictions perform.</p></li>
<li><p>We can control how the future experience emerges. In particular, we can ensure there are no future systemic changes (e.g. from legislation or court precedent) that would impact future payments. Changes like this can make it difficult when examining performance of reserve prediction methods on real data - it can be hard to separate poor performance of the model from a good model where the future experience departs markedly from that used to build the model.</p></li>
<li><p>We can share the data set with you.</p></li>
</ul>
<p>The data set used is simulated data set 3 from the paper <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3241906">Self-assembling insurance claim models using regularized regression and machine learning</a>. This is a 40x40 triangle of incremental quarterly payments over 10 years. Variables on the data set are accident, development and calendar quarters only. A copy of the data set is available <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/csv/lasso_simdata3.csv">here</a>.</p>
</div>
<div class="section" id="machine-learning-and-aggregate-triangles">
<h2>Machine learning and aggregate triangles<a class="headerlink" href="#machine-learning-and-aggregate-triangles" title="Permalink to this headline">¶</a></h2>
<p>Typically, ML and big data go hand-in-hand. By big data we mean lots of observations and lots of features (covariates / independent variables / regressors). Aggregate triangles are small in both senses - small numbers of observations and limited features (often just the 3 time features of accident/underwriting period, development period and calendar period).</p>
<p>This is not to say that it is invalid to use machine learning for aggregate triangles - many people have demonstrated good results from machine learning for such data - there are many papers on this topic. But it’s also true that an experienced actuary using a Chainladder model (or other traditional) method, overlaid with judgement, will often get a similar answer to the best ML method, even for a complex triangle. However ML may be a more efficient way of getting to an answer. In the worked example below, we use a data set which deviates significantly from Chainladder assumptions. Therefore the unadjusted Chainladder projection is quite poor and would need a significant investment of time to yield a better model. In contrast, the better performing ML models we looked at have a better first estimate, so may not require as much intervention to produce a final estimate, thereby leading to a time-saving.</p>
</div>
<div class="section" id="don-t-rank-the-ml-models-used-based-on-this-example">
<h2>Don’t rank the ML models used based on this example<a class="headerlink" href="#don-t-rank-the-ml-models-used-based-on-this-example" title="Permalink to this headline">¶</a></h2>
<p>The purpose of this article is to demonstrate a workflow for fitting ML models in R. While we’ve done some work to improve model fitting, there are a lot more things we would consider if we were looking for the best models (e.g. feature engineering, train/test splits, performance measures). So, although we do look at the relative performance of the different models at the end, you should not make any conclusions about the relative performance of the different ML methods based on this work.</p>
<p>You may find that by adjusting some hyper-parameters, or by revising the training/test data set split, you’re able to find better models.
We’ll be discussing this point a bit further at the end of the article.</p>
<p><em>A priori</em>, given the form of the data (simulated using a regression style structure) and that the LASSO model we used is based on previous work which did aim to establish a framework to fit good models to triangular data using the LASSO, we expected that the LASSO model would perform the best and (spoiler alert!) it did.</p>
</div>
<div class="section" id="the-scikit-learn-package">
<h2>The scikit-learn package<a class="headerlink" href="#the-scikit-learn-package" title="Permalink to this headline">¶</a></h2>
<p>One strength of using Python for machine learning is the consistency in input specifications for data. The Python community have generally coalesced onto <strong>pandas</strong> for data frames, <strong>numpy</strong> for matrices and <strong>scikit-learn</strong> for machine learning.</p>
<p>The benefit of these is apparent - set up the data in the aggregator and then switch models in and out at will.</p>
<p>As a rough rule of thumb, when working with <strong>scikit-learn</strong> we:</p>
<ul class="simple">
<li><p>First set up an instance of the class. Note that the class can contain both methods (like functions) and data.</p></li>
<li><p>We then run methods for the class object. These may update the data in the object.</p></li>
<li><p>We can view and use the data in the class object.</p></li>
</ul>
<p>If you want to learn more there is plenty of documentation out there:</p>
<ul class="simple">
<li><p>The <a class="reference external" href="https://scikit-learn.org/">scikit-learn website</a></p></li>
<li><p>Particularly, this <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/index.html">gallery of examples</a></p></li>
<li><p><a class="reference external" href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf">Cheatsheets</a>, with <a class="reference external" href="https://www.datacamp.com/community/blog/scikit-learn-cheat-sheet">explanations</a></p></li>
</ul>
<p>When using Python code, your code editor may also display help and documentation (via “docstrings”). You can also get inline help by using the <code class="docutils literal notranslate"><span class="pre">help()</span></code> function on an object. This will bring you to help pages.</p>
<p>E.g. <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">sklearn;</span> <span class="pre">help(sklearn)</span></code> with get you help on sklearn in general.</p>
</div>
<div class="section" id="ml-methods-used">
<h2>ML methods used<a class="headerlink" href="#ml-methods-used" title="Permalink to this headline">¶</a></h2>
<p>We’ve looked at the following methods:</p>
<ul class="simple">
<li><p>Decision trees</p></li>
<li><p>Random forests</p></li>
<li><p>Gradient boosted machines (GBM) / gradient boosted decision trees</p></li>
<li><p>Neural Networks (a basic one that comes with <strong>scikit-learn</strong>)</p></li>
<li><p>Chainladder - or rather, a GLM which is set up to replicate the chain ladder estimates (refer to this <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/glms/">post</a> for more details)</p></li>
<li><p>LASSO</p></li>
</ul>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="outline-of-the-workflow">
<h1>Outline of the workflow<a class="headerlink" href="#outline-of-the-workflow" title="Permalink to this headline">¶</a></h1>
<p>The workflow we have set up consists of the following:</p>
<ul class="simple">
<li><p>Prepare the data for use in the models, including train and test partitions (past data) and hold-out or validation partitions (future data).</p></li>
<li><p>Fit each of the models selected. For each of these we:</p>
<ul>
<li><p>Select hyper-parameters (these control the model-fitting process) for tuning</p></li>
<li><p>Tune the hyper-parameters using the train and test partitions and select the set of values that yield the best results</p></li>
<li><p>Calculate predicted values on the holdout (future) data.</p></li>
</ul>
</li>
<li><p>Run diagnostics on the predicted values for each model and look at the reserve estimates.</p></li>
</ul>
<p>We’ll now work through each of the stages below.</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<p>The first step is to import all the packages required for this work (and install them first if needed).</p>
<p>This session uses <code class="docutils literal notranslate"><span class="pre">pandas</span></code>, <code class="docutils literal notranslate"><span class="pre">numpy</span></code>, <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, <code class="docutils literal notranslate"><span class="pre">seaborn</span></code>, <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> and <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>.</p>
<p>The versions are shown below. If you are having problems running our code, check that your packages are the same as ours and <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-U</span></code> upgrade / downgrade them if needed.</p>
<p>So let us get started with the example with some setup code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip install -U scikit-learn
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: scikit-learn in /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages (0.24.2)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages (from scikit-learn) (2.1.0)
Requirement already satisfied: joblib&gt;=0.11 in /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages (from scikit-learn) (1.0.1)
Requirement already satisfied: scipy&gt;=0.19.1 in /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages (from scikit-learn) (1.6.3)
Requirement already satisfied: numpy&gt;=1.13.3 in /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages (from scikit-learn) (1.20.3)
Requirement already satisfied: numpy&gt;=1.13.3 in /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages (from scikit-learn) (1.20.3)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip show pandas numpy scikit-learn seaborn matplotlib
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Name: pandas
Version: 1.2.4
Summary: Powerful data structures for data analysis, time series, and statistics
Home-page: https://pandas.pydata.org
Author: None
Author-email: None
License: BSD
Location: /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages
Requires: pytz, numpy, python-dateutil
Required-by: seaborn
---
Name: numpy
Version: 1.20.3
Summary: NumPy is the fundamental package for array computing with Python.
Home-page: https://www.numpy.org
Author: Travis E. Oliphant et al.
Author-email: None
License: BSD
Location: /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages
Requires: 
Required-by: torchvision, tensorflow, tensorboard, seaborn, scipy, scikit-learn, pandas, opt-einsum, mkl-random, mkl-fft, matplotlib, Keras, Keras-Preprocessing, h5py
---
Name: scikit-learn
Version: 0.24.2
Summary: A set of python modules for machine learning and data mining
Home-page: http://scikit-learn.org
Author: None
Author-email: None
License: new BSD
Location: /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages
Requires: threadpoolctl, scipy, numpy, joblib
Required-by: 
---
Name: seaborn
Version: 0.11.1
Summary: seaborn: statistical data visualization
Home-page: https://seaborn.pydata.org
Author: Michael Waskom
Author-email: mwaskom@nyu.edu
License: BSD (3-clause)
Location: /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages
Requires: scipy, pandas, matplotlib, numpy
Required-by: 
---
Name: matplotlib
Version: 3.4.2
Summary: Python plotting package
Home-page: https://matplotlib.org
Author: John D. Hunter, Michael Droettboom
Author-email: matplotlib-users@python.org
License: PSF
Location: /Users/Jacky/opt/miniconda3/lib/python3.9/site-packages
Requires: python-dateutil, kiwisolver, numpy, pillow, cycler, pyparsing
Required-by: seaborn
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">PoissonRegressor</span>

<span class="kn">from</span> <span class="nn">sklearn.experimental</span> <span class="kn">import</span> <span class="n">enable_hist_gradient_boosting</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span><span class="p">,</span> <span class="n">plot_tree</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="n">GradientBoostingRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingRegressor</span>

<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">TransformedTargetRegressor</span>

<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">all_estimators</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">LogNorm</span>

<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">itertools</span>

<span class="c1"># Set the seed for the random number generation so</span>
<span class="c1"># results are reproducible.</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># colours for plots - not all are used</span>
<span class="c1"># We will also use viridis for some plots</span>
<span class="n">plot_colors</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s2">&quot;dblue&quot;</span>  <span class="p">:</span> <span class="s2">&quot;#113458&quot;</span><span class="p">,</span>
  <span class="s2">&quot;mblue&quot;</span>  <span class="p">:</span> <span class="s2">&quot;#4096b8&quot;</span><span class="p">,</span>
  <span class="s2">&quot;gold&quot;</span>   <span class="p">:</span> <span class="s2">&quot;#d9ab16&quot;</span><span class="p">,</span>
  <span class="s2">&quot;lgrey&quot;</span>  <span class="p">:</span> <span class="s2">&quot;#dcddd9&quot;</span><span class="p">,</span>
  <span class="s2">&quot;dgrey&quot;</span>  <span class="p">:</span> <span class="s2">&quot;#3f4548&quot;</span><span class="p">,</span>
  <span class="s2">&quot;black&quot;</span>  <span class="p">:</span> <span class="s2">&quot;#3F4548&quot;</span><span class="p">,</span>
  <span class="s2">&quot;red&quot;</span>    <span class="p">:</span> <span class="s2">&quot;#d01e45&quot;</span><span class="p">,</span>
  <span class="s2">&quot;purple&quot;</span> <span class="p">:</span> <span class="s2">&quot;#8f4693&quot;</span><span class="p">,</span>
  <span class="s2">&quot;orange&quot;</span> <span class="p">:</span> <span class="s2">&quot;#ee741d&quot;</span><span class="p">,</span>
  <span class="s2">&quot;fuscia&quot;</span> <span class="p">:</span> <span class="s2">&quot;#e9458c&quot;</span><span class="p">,</span>
  <span class="s2">&quot;violet&quot;</span> <span class="p">:</span> <span class="s2">&quot;#8076cf&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="data-preparation">
<h1>Data preparation<a class="headerlink" href="#data-preparation" title="Permalink to this headline">¶</a></h1>
<p>We’re using a simulated data triangle with all values (past and future).</p>
<p><img alt="past and future" src="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-07-mlr3example/index_files/figure-html/unnamed-chunk-3-1.png" /></p>
<p>The data set we use is the simulated data set 3 from this <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3241906">paper</a>.
It is available in CSV form <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/csv/lasso_simdata3.csv">here</a>.</p>
<p>This is a 40x40 triangle of payments:</p>
<ul class="simple">
<li><p>That vary by accident quarter</p></li>
<li><p>That vary by development quarter</p></li>
<li><p>Have varying rates of superimposed inflation in payment size by calendar quarter (including no inflation)</p></li>
<li><p>Have a step-up in payment size for accident quarters &gt; 16 and development quarters &gt; 20.</p></li>
</ul>
<p>The first two effects are captured by a Chainladder model but the last two effects depart from Chainladder assumptions.</p>
<div class="section" id="the-pandas-package">
<h2>The pandas package<a class="headerlink" href="#the-pandas-package" title="Permalink to this headline">¶</a></h2>
<p>We’ll be using the <strong>pandas</strong> package for manipulating the data. There’s an introduction to <strong>pandas</strong> on its <a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html">website</a> if you are unfamiliar with it.</p>
</div>
<div class="section" id="load-the-data">
<h2>Load the data<a class="headerlink" href="#load-the-data" title="Permalink to this headline">¶</a></h2>
<p>First, load in the data and have a look at it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1-4 will bring up different datasets. Covered in &quot;next steps&quot; note at the end of the article</span>
<span class="n">dataset_number</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">dat</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;https://institute-and-faculty-of-actuaries.github.io/mlr-blog/csv/lasso_simdata</span><span class="si">{</span><span class="n">dataset_number</span><span class="si">}</span><span class="s2">.csv&quot;</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;pmts&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="s2">&quot;dev&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="s2">&quot;cal&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="s2">&quot;mu&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="s2">&quot;train_ind&quot;</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">})</span>

<span class="n">dat</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

<span class="c1"># create the num_periods variable - number of acc/dev periods</span>
<span class="n">num_periods</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dat</span><span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>As you can see, the data is not in triangular form as per the diagram above, but is instead in long form where:</p>
<ul class="simple">
<li><p>each row of the data set consists of one observation</p></li>
<li><p>each observation has the accident(<code class="docutils literal notranslate"><span class="pre">acc</span></code>), development(<code class="docutils literal notranslate"><span class="pre">dev</span></code>) and calendar(<code class="docutils literal notranslate"><span class="pre">cal</span></code>) period associated with it</p></li>
<li><p>the <code class="docutils literal notranslate"><span class="pre">mu</span></code> value is the mean value of the distribution from which <code class="docutils literal notranslate"><span class="pre">pmts</span></code> was simulated - this won’t form part of the analysis below so can be ignored</p></li>
<li><p>the final variable, <code class="docutils literal notranslate"><span class="pre">train_ind</span></code> is <code class="docutils literal notranslate"><span class="pre">TRUE</span></code> for past values and <code class="docutils literal notranslate"><span class="pre">FALSE</span></code> for future values.</p></li>
</ul>
<p>This long format of data is standard for a lot of modelling and data analysis.</p>
<p>We can also show a visualisation of the data. This contains both past and future data (with a diagonal line marking the boundary). The plot uses shading to indicate the size of the payments (specifically, log(payments)).
The step-up in claim size (acc &gt; 16 and dev &gt; 20) is quite obvious in this graphic. What is also apparent is this this increase only affects a small part of the past triangle (10 cells out of 820 to be exact), but impacts much of the future lower triangle.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_triangle</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> 
    <span class="n">mask_bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Claims Development&quot;</span><span class="p">,</span> 
    <span class="n">cmap</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;viridis_r&quot;</span><span class="p">,</span> <span class="n">as_cmap</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="c1"># Use Viridis colour palette just like the R code</span>
    <span class="n">center</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">fig</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot; Plots a claims triangle heatmap</span>
<span class="sd">    data:</span>
<span class="sd">        Pandas DataFrame, indexed by date</span>
<span class="sd">    mask_bottom:</span>
<span class="sd">        Hide bottom half of triangle. Assumes origin and development periods</span>
<span class="sd">        are the same. e.g. both months</span>
<span class="sd">    cmap:</span>
<span class="sd">        Seaborn colour palette</span>
<span class="sd">    center:</span>
<span class="sd">        Centre for heatmap</span>
<span class="sd">    fig, ax: optional fig and ax</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>

    <span class="c1"># Generate a mask for the triangle</span>
    <span class="k">if</span> <span class="n">mask_bottom</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>

        <span class="n">mask</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">tril_indices_from</span><span class="p">(</span><span class="n">mask</span><span class="p">)]</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flipud</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Set up the matplotlib figure</span>
    <span class="k">if</span> <span class="n">fig</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="c1"># Draw the heatmap with the mask and correct aspect ratio vmax=.3,</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span>
        <span class="n">data</span><span class="p">,</span>
        <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span>
        <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span>
        <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;.0f&quot;</span><span class="p">,</span>
        <span class="n">cbar_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;shrink&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">},</span>
        <span class="n">norm</span><span class="o">=</span><span class="n">LogNorm</span><span class="p">()</span>  <span class="c1"># Log scale</span>
    <span class="p">)</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Partial triangle - the model only sees this for training</span>
<span class="n">plot_triangle</span><span class="p">(</span>
    <span class="n">dat</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s2">&quot;acc&quot;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s2">&quot;pmts&quot;</span><span class="p">),</span>
    <span class="n">mask_bottom</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_23_0.png" src="../_images/MLRWP_Py_triangles_example_23_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Whole triangle - with test dataset</span>
<span class="n">plot_triangle</span><span class="p">(</span>
    <span class="n">dat</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s2">&quot;acc&quot;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s2">&quot;pmts&quot;</span><span class="p">),</span>
    <span class="n">mask_bottom</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_24_0.png" src="../_images/MLRWP_Py_triangles_example_24_0.png" />
</div>
</div>
<p>The step-up in payments can be modelled as an interaction between accident and development quarters.
Because it affects only a small number of past values, it may be difficult for some of the modelling approaches to capture this.
It’s likely, therefore, that the main differentiator between the performance of different methods for this data set will be how much well they capture this interaction.</p>
<p>One thing to note is that the Chainladder will struggle with this data set. Both the calendar period terms and the interaction are not effects that the Chainladder can model - it assumes that only accident and development period main effects (i.e. no interactions) are present. So an actuary using the Chainladder for this data set would need to overlay judgement to obtain a good result.</p>
<p>We need to modify this data a little before proceeding further and add factor versions (essentially a categorical version) of acc and dev - these are needed later for fitting the Chainladder.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Factors with zero index</span>
<span class="n">dat</span><span class="p">[</span><span class="s2">&quot;accf&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">dat</span><span class="o">.</span><span class="n">acc</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">dat</span><span class="p">[</span><span class="s2">&quot;devf&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">dat</span><span class="o">.</span><span class="n">dev</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="train-and-test">
<h2>Train and Test<a class="headerlink" href="#train-and-test" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">dat</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">dat</span><span class="o">.</span><span class="n">train_ind</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">,</span> <span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="s2">&quot;cal&quot;</span><span class="p">,</span> <span class="s2">&quot;accf&quot;</span><span class="p">,</span> <span class="s2">&quot;devf&quot;</span><span class="p">]])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">dat</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">dat</span><span class="o">.</span><span class="n">train_ind</span><span class="p">,</span> <span class="s2">&quot;pmts&quot;</span><span class="p">])</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">dat</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">dat</span><span class="o">.</span><span class="n">train_ind</span> <span class="o">==</span> <span class="kc">False</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">,</span> <span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="s2">&quot;cal&quot;</span><span class="p">,</span> <span class="s2">&quot;accf&quot;</span><span class="p">,</span> <span class="s2">&quot;devf&quot;</span><span class="p">]])</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">dat</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">dat</span><span class="o">.</span><span class="n">train_ind</span> <span class="o">==</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;pmts&quot;</span><span class="p">])</span>

<span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">dat</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">,</span> <span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="s2">&quot;cal&quot;</span><span class="p">,</span> <span class="s2">&quot;accf&quot;</span><span class="p">,</span> <span class="s2">&quot;devf&quot;</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">dat</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;pmts&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="tuning-process-for-hyper-parameters">
<h1>Tuning process for hyper-parameters<a class="headerlink" href="#tuning-process-for-hyper-parameters" title="Permalink to this headline">¶</a></h1>
<p>Machine learning models have a number of hyper-parameters that control the model fit.
Tweaking the hyper-parameters that control model fitting (e.g. for decision trees, the depth of the tree, or for random forests, the number of trees to average over) can have a significant impact on the quality of the fit.
The standard way of doing this is to use a train and test data set where:</p>
<ul class="simple">
<li><p>The model is trained (built) on the train data set</p></li>
<li><p>The performance of the model is evaluated on the test data set</p></li>
<li><p>This is repeated for a number of different combinations of hyper-parameters, and the best performing one is selected.</p></li>
</ul>
<p>Evaluating the models on a separate data set not used in the model fitting helps to control over-fitting.
Usually we would then select the hyper-parameters that lead to the best results on the test data set and use these to predict our future values.</p>
<p>There are various ways we can select the test and train data sets.
For this work we use cross-validation. We’ll give a brief overview of it below.
Note that we will be discussing validation options in a separate article.</p>
<div class="section" id="cross-validation">
<h2>Cross-validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline">¶</a></h2>
<p>The simplest implementation of a train and test partition is a random one where each point in the data set is allocated to train and test at random.
Typically, the train part might be around 70-80% of the data and the test part the remainder.</p>
<p><strong>Random test data set</strong></p>
<p><img alt="random test dataset" src="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-07-mlr3example/index_files/figure-html/unnamed-chunk-13-1.png" /></p>
<p>Cross-validation repeats this type of split a number of times.</p>
<p>In more detail, the steps are:</p>
<ul class="simple">
<li><p>Randomly partition the data into <em>k</em> equal-sized folds (between 5-10 folds are common choices). These folds are then fixed for the remainder of the algorithm.</p></li>
<li><p>Fit the model using data from <em>k</em>-1 of the folds, using the remaining fold as the test data. Do this <em>k</em> times, so each fold is used as test data once.</p></li>
<li><p>Calculate the performance metrics for each model on the corresponding test fold.</p></li>
<li><p>Average the performance metrics across all the folds.</p></li>
</ul>
<p>A simplified representation of the process is given below.</p>
<p><strong>5-fold cross validation</strong></p>
<p><img alt="n-fold CV" src="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-07-mlr3example/index_files/figure-html/unnamed-chunk-14-1.png" /></p>
<p>Cross-validation provides an estimate of out-of-sample performance even though all the data is used for training and testing.
It is often considered more robust due to its use of the full dataset for testing, but can be computationally expensive for larger datasets. Here we only have a small amount of data, so there is an advantage to using the full dataset, whilst the computational cost is manageable.</p>
</div>
<div class="section" id="cross-validation-in-scikit-learn">
<h2>Cross-validation in scikit-learn<a class="headerlink" href="#cross-validation-in-scikit-learn" title="Permalink to this headline">¶</a></h2>
<p>We will be setting up cross-validation resamplers in scikit-learn to apply to our models and find optimal hyper-parameters. As always, the documentation for <a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html">cross validation</a>, <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html">grid search CV</a> and <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html">random search CV</a> are useful starting reference point.</p>
<p>Here we will use 6-fold cross validation. As applied to our triangles, it will look something like this, where the yellow points are the training data and the blue the test data in each fold:</p>
<p><img alt="6-fold-on-triangle" src="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-07-mlr3example/index_files/figure-html/unnamed-chunk-17-1.png" /></p>
</div>
<div class="section" id="performance-measure">
<h2>Performance measure<a class="headerlink" href="#performance-measure" title="Permalink to this headline">¶</a></h2>
<p>During the cross-validation process, we need to have a performance measure to optimise.</p>
<p>Here we will use RMSE (root mean square error).</p>
</div>
<div class="section" id="searching-hyper-parameter-space">
<h2>Searching hyper-parameter space<a class="headerlink" href="#searching-hyper-parameter-space" title="Permalink to this headline">¶</a></h2>
<p>Searching hyper-parameter space needs to be customised to each model, so we can’t set up a common framework here. However, to control computations, it is useful to set a limit on the number of searches that can be performed in a tuning exercise.</p>
<p>Given how we set up the tuning process below we need 25 evaluations for the decision tree and random forest models. We need many more evaluations for the GBM model since we tune more parameters. However, this can take a long time to run. In practice, using a more powerful computer, or running things in parallel can help.</p>
<p>So we’ve set the number of evaluations to a low number here (25).</p>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="fitting-some-ml-models">
<h1>Fitting some ML models<a class="headerlink" href="#fitting-some-ml-models" title="Permalink to this headline">¶</a></h1>
<p>Now it’s time to fit and tune the following ML models using <strong>scikit-learn</strong>:</p>
<ul class="simple">
<li><p>Decision tree</p></li>
<li><p>Random forest</p></li>
<li><p>Gradient boosting machines (GBM)</p></li>
<li><p>Neural Network</p></li>
</ul>
<div class="section" id="decision-tree">
<h2>Decision tree<a class="headerlink" href="#decision-tree" title="Permalink to this headline">¶</a></h2>
<p>A decision tree is unlikely to be a very good model for this data, and tuning often doesn’t help much.
Nonetheless, it’s a simple model, so it’s useful to fit it to see what happens.</p>
<p>To illustrate the use of scikit-learn, we’ll first show how to fit a model using the default hyper-parameters.
We’ll then move onto the code needed to run hyper-parameter tuning using cross-validation.</p>
<div class="section" id="fitting-a-single-model">
<h3>Fitting a single model<a class="headerlink" href="#fitting-a-single-model" title="Permalink to this headline">¶</a></h3>
<p>First let’s fit a simple decision tree.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dtree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">dtree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">plot_tree</span><span class="p">(</span><span class="n">dtree</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_38_0.png" src="../_images/MLRWP_Py_triangles_example_38_0.png" />
</div>
</div>
<p>Unfortunately this plot is too small to read. If you are unfamiliar with decision tree diagrams, then these diagrams give splitting rules for the data. At the terminal nodes, predicted values for each segment are given. If you are able to read this plot (e.g. you are running the Jupyter notebook), then</p>
<p>Here’s a list of the parameters - <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html">the documentation</a> will have more information.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dtree</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;ccp_alpha&#39;: 0.0,
 &#39;criterion&#39;: &#39;mse&#39;,
 &#39;max_depth&#39;: 3,
 &#39;max_features&#39;: None,
 &#39;max_leaf_nodes&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_impurity_split&#39;: None,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;random_state&#39;: None,
 &#39;splitter&#39;: &#39;best&#39;}
</pre></div>
</div>
</div>
</div>
<p>Let’s try tuning the <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> and ‘ccp_alpha’ parameters.</p>
</div>
<div class="section" id="tuning-the-decision-tree">
<h3>Tuning the decision tree<a class="headerlink" href="#tuning-the-decision-tree" title="Permalink to this headline">¶</a></h3>
<p>To set up the tuning we need to specify:</p>
<ul class="simple">
<li><p>The ranges of values to search over</p></li>
<li><p>A resampling strategy (already have this - crossvalidation)</p></li>
<li><p>An evaluation measure (this is RMSE)</p></li>
<li><p>A termination criterion so searches don’t go on for ever (our <code class="docutils literal notranslate"><span class="pre">evals_trm</span></code>)</p></li>
<li><p>The search strategy (e.g. grid search, random search, etc - see, e.g., <a class="reference external" href="https://towardsdatascience.com/hyperparameter-tuning-a-practical-guide-and-template-b3bf0504f095">this post</a> )</p></li>
</ul>
<p>We first set ranges of values to consider for these:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">criterion</span></code>: whether to fit on the basis of <code class="docutils literal notranslate"><span class="pre">mse</span></code> which aligns with our evaluation metric or <code class="docutils literal notranslate"><span class="pre">poisson</span></code> to match the log-link GLMs later in the piece</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_depth</span></code>: how big the trees can be,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code>: complexity parameter for pruning.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters_tree</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;criterion&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span> <span class="s2">&quot;poisson&quot;</span><span class="p">],</span>
    <span class="s2">&quot;max_depth&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="c1"># None is unlimited depth.</span>
    <span class="s2">&quot;ccp_alpha&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>5 of each of <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> and <code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> for a 2x5x5 grid. Given we only have 50 points, we can easily evaluate every option so we’ll use a grid search strategy.</p>
<p>In practice other search strategies may be preferable - e.g. a random search often gets similar results to a grid search, in a much smaller amount of time.
Another possible choice is Bayesian optimisation search.</p>
<p>So now we have everything we need to tune our hyper-parameters so we can set this up with <strong>GridSearchCV</strong> now.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid_tree</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">DecisionTreeRegressor</span><span class="p">(),</span> <span class="n">parameters_tree</span><span class="p">)</span>
<span class="n">grid_tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GridSearchCV(estimator=DecisionTreeRegressor(),
             param_grid={&#39;ccp_alpha&#39;: [0.0, 0.2, 0.4, 0.6, 0.8],
                         &#39;criterion&#39;: [&#39;mse&#39;, &#39;poisson&#39;],
                         &#39;max_depth&#39;: [2, 3, 5, 7, None]})
</pre></div>
</div>
</div>
</div>
<p>The parameters in the best fit may be accessed here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid_tree</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;ccp_alpha&#39;: 0.8,
 &#39;criterion&#39;: &#39;mse&#39;,
 &#39;max_depth&#39;: None,
 &#39;max_features&#39;: None,
 &#39;max_leaf_nodes&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_impurity_split&#39;: None,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;random_state&#39;: None,
 &#39;splitter&#39;: &#39;best&#39;}
</pre></div>
</div>
</div>
</div>
<p>These also fit a final model to the data using these optimised parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the model</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">grid_tree</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_50_0.png" src="../_images/MLRWP_Py_triangles_example_50_0.png" />
</div>
</div>
<p>This model is much more complicated than the original and looks overfitted - we’ll see if this is the case when we evaluate the model performance later in the article.</p>
</div>
</div>
<div class="section" id="random-forest-fitting">
<h2>Random forest fitting<a class="headerlink" href="#random-forest-fitting" title="Permalink to this headline">¶</a></h2>
<p>The random forest model for regression is <code class="docutils literal notranslate"><span class="pre">sklearn.ensemble.RandomForestRegressor</span></code>.</p>
<p>Let’s have a look at the hyper-parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rforest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">()</span>
<span class="n">rforest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">rforest</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;bootstrap&#39;: True,
 &#39;ccp_alpha&#39;: 0.0,
 &#39;criterion&#39;: &#39;mse&#39;,
 &#39;max_depth&#39;: None,
 &#39;max_features&#39;: &#39;auto&#39;,
 &#39;max_leaf_nodes&#39;: None,
 &#39;max_samples&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_impurity_split&#39;: None,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;n_estimators&#39;: 100,
 &#39;n_jobs&#39;: None,
 &#39;oob_score&#39;: False,
 &#39;random_state&#39;: None,
 &#39;verbose&#39;: 0,
 &#39;warm_start&#39;: False}
</pre></div>
</div>
</div>
</div>
<p>Here, we’ll try tuning <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> and ‘ccp_alpha’ parameters again.</p>
<p>We’ll follow the same steps as for decision trees - set up a parameter space for searching, combine this with RMSE and cross-validation, tune using grid search, and get the best fit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters_forest</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;max_depth&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="c1"># None is unlimited depth.</span>
    <span class="s2">&quot;ccp_alpha&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
    <span class="s2">&quot;random_state&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">grid_forest</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">RandomForestRegressor</span><span class="p">(),</span> 
    <span class="n">parameters_forest</span><span class="p">,</span>  
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">grid_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">grid_forest</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;bootstrap&#39;: True,
 &#39;ccp_alpha&#39;: 0.0,
 &#39;criterion&#39;: &#39;mse&#39;,
 &#39;max_depth&#39;: None,
 &#39;max_features&#39;: &#39;auto&#39;,
 &#39;max_leaf_nodes&#39;: None,
 &#39;max_samples&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_impurity_split&#39;: None,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;n_estimators&#39;: 100,
 &#39;n_jobs&#39;: None,
 &#39;oob_score&#39;: False,
 &#39;random_state&#39;: 0,
 &#39;verbose&#39;: 0,
 &#39;warm_start&#39;: False}
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="gbm">
<h2>GBM<a class="headerlink" href="#gbm" title="Permalink to this headline">¶</a></h2>
<p>There are two GBM implementations in <strong>scikit-learn</strong>. The first is <code class="docutils literal notranslate"><span class="pre">sklearn.ensemble.GradientBoostingRegressor</span></code> which implements a “traditional” GBM.</p>
<p>The other is <code class="docutils literal notranslate"><span class="pre">sklearn.ensemble.HistGradientBoostingRegressor</span></code> which implements a number of performance tricks to speed up model fitting, similar to <code class="docutils literal notranslate"><span class="pre">LightGBM</span></code> or the fast histogram method in <code class="docutils literal notranslate"><span class="pre">XGBoost</span></code>. This is considered an experimental feature with the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> version we are using.</p>
<p>Here are the GBM parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gbm</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">()</span>
<span class="n">gbm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">gbm</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;alpha&#39;: 0.9,
 &#39;ccp_alpha&#39;: 0.0,
 &#39;criterion&#39;: &#39;friedman_mse&#39;,
 &#39;init&#39;: None,
 &#39;learning_rate&#39;: 0.1,
 &#39;loss&#39;: &#39;ls&#39;,
 &#39;max_depth&#39;: 3,
 &#39;max_features&#39;: None,
 &#39;max_leaf_nodes&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_impurity_split&#39;: None,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;n_estimators&#39;: 100,
 &#39;n_iter_no_change&#39;: None,
 &#39;random_state&#39;: None,
 &#39;subsample&#39;: 1.0,
 &#39;tol&#39;: 0.0001,
 &#39;validation_fraction&#39;: 0.1,
 &#39;verbose&#39;: 0,
 &#39;warm_start&#39;: False}
</pre></div>
</div>
</div>
</div>
<p>With gradient boosting, it is often helpful to optimise over hyper-parameters, so we will vary some parameters and select the best performing set.</p>
<p>For speed reasons we will just consider <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>, <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>, <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> , and <code class="docutils literal notranslate"><span class="pre">subsample</span></code> but in practice, we could consider doing more.</p>
<p>The steps are the generally same as before for decision trees and random forests. However as we are searching over a greater number of parameters, we’ve switched to a random search to try to achieve better results.</p>
<p>Depending on your computer, this step takes a while to run since we are running the slower GradientBoostingRegressor - even though we have a low <code class="docutils literal notranslate"><span class="pre">n_iter</span></code> - so it might be a good point to grab a cup of coffee or tea!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters_gbm</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;n_estimators&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">500</span><span class="p">],</span>
    <span class="s2">&quot;max_depth&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="s2">&quot;subsample&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>

<span class="n">grid_gbm</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span>
    <span class="n">GradientBoostingRegressor</span><span class="p">(),</span> 
    <span class="n">parameters_gbm</span><span class="p">,</span> 
    <span class="n">n_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> 
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># Run in parallel</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>

<span class="n">grid_gbm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ran in </span><span class="si">{</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ran in 7.4581 seconds
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the best fit:</span>
<span class="n">grid_gbm</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;alpha&#39;: 0.9,
 &#39;ccp_alpha&#39;: 0.0,
 &#39;criterion&#39;: &#39;friedman_mse&#39;,
 &#39;init&#39;: None,
 &#39;learning_rate&#39;: 0.01,
 &#39;loss&#39;: &#39;ls&#39;,
 &#39;max_depth&#39;: 5,
 &#39;max_features&#39;: None,
 &#39;max_leaf_nodes&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_impurity_split&#39;: None,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;n_estimators&#39;: 400,
 &#39;n_iter_no_change&#39;: None,
 &#39;random_state&#39;: None,
 &#39;subsample&#39;: 0.5,
 &#39;tol&#39;: 0.0001,
 &#39;validation_fraction&#39;: 0.1,
 &#39;verbose&#39;: 0,
 &#39;warm_start&#39;: False}
</pre></div>
</div>
</div>
</div>
<p>However, remember that these results are from 25 evaluations only, and with 4 hyper-parameters, we’d really like to use more evaluations. The model above isn’t a particularly good one.</p>
<p>Next, the HistGradientBooster which is most similar to LightGBM, which was in turn based on XGBoost. It’s quite possible to install those packages too to include more full featured models but to make this tutorial code easy to install and use, we won’t introduce the extra dependencies and just use the scikit-learn implementation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters_hgbm</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;least_squares&quot;</span><span class="p">,</span> <span class="s2">&quot;poisson&quot;</span><span class="p">],</span>
    <span class="s2">&quot;max_iter&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">500</span><span class="p">],</span>
    <span class="s2">&quot;max_depth&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="c1"># This regressor is generally quite fast so we can run a few more.</span>
<span class="n">grid_hgbm</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span>
    <span class="n">HistGradientBoostingRegressor</span><span class="p">(),</span> 
    <span class="n">parameters_hgbm</span><span class="p">,</span> 
    <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># Run in parallel</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>

<span class="n">grid_hgbm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ran in </span><span class="si">{</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
<span class="c1"># It was not faster in this case with this dataset.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ran in 38.4765 seconds
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the best fit:</span>
<span class="n">grid_hgbm</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;categorical_features&#39;: None,
 &#39;early_stopping&#39;: &#39;auto&#39;,
 &#39;l2_regularization&#39;: 0.0,
 &#39;learning_rate&#39;: 0.1,
 &#39;loss&#39;: &#39;poisson&#39;,
 &#39;max_bins&#39;: 255,
 &#39;max_depth&#39;: 6,
 &#39;max_iter&#39;: 400,
 &#39;max_leaf_nodes&#39;: 31,
 &#39;min_samples_leaf&#39;: 20,
 &#39;monotonic_cst&#39;: None,
 &#39;n_iter_no_change&#39;: 10,
 &#39;random_state&#39;: None,
 &#39;scoring&#39;: &#39;loss&#39;,
 &#39;tol&#39;: 1e-07,
 &#39;validation_fraction&#39;: 0.1,
 &#39;verbose&#39;: 0,
 &#39;warm_start&#39;: False}
</pre></div>
</div>
</div>
</div>
<p>In the R code, we mentioned a previously tuned XGBoost model with hyperparameters tuned over a larger search. Can we replicate the model in Python here?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gbm_prev</span> <span class="o">=</span> <span class="n">HistGradientBoostingRegressor</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;poisson&#39;</span><span class="p">,</span>  <span class="c1"># Poisson is close to Tweedie with variance power 1.01</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="mi">233</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span>
<span class="p">)</span>

<span class="n">gbm_prev</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>HistGradientBoostingRegressor(learning_rate=0.3, loss=&#39;poisson&#39;, max_depth=3,
                              max_iter=233)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="neural-network">
<h2>Neural Network<a class="headerlink" href="#neural-network" title="Permalink to this headline">¶</a></h2>
<p>Neural networks - also known as deep learning models - are models based on interconnected neurons which take in inputs, apply a function and produce outputs used by other neurons, ultimately to predict response variables.</p>
<p>In the <strong>R</strong> article with <strong>mlr3</strong> we skipped neural networks as these required keras which can be a little tricky to install. However, <strong>scikit-learn</strong> includes a simple <a class="reference external" href="https://scikit-learn.org/stable/modules/neural_networks_supervised.html">Multi-layer Perceptron</a> model so we will try that here.</p>
<p>This is a simple feedforward model where each layer is connected in order.</p>
<p><img alt="Feedforward model diagram" src="https://upload.wikimedia.org/wikipedia/en/5/54/Feed_forward_neural_net.gif" /></p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Feedforward_neural_network#/media/File:Feed_forward_neural_net.gif">Source: Wikipedia</a></p>
<p>Unlike the diagram all neurons are just simply connected to all neurons in the following layer. But there are many variations on how those neurons can be connected which we may explore in future articles.</p>
<p>With the features, we will transform accident, development and calendar periods with one-hot encoding to treat them as continuous variables, and let the model figure the rest out. With the target, given the average value is approx $300m, we’ll scale it so the model can hopefully find convergence faster - and we will try a few different learning rates.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters_nn</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;hidden_layer_sizes&quot;</span><span class="p">:</span> <span class="p">[</span> 
        <span class="c1"># Keep these to 50 neurons to keep things small</span>
        <span class="c1"># (We do only have 3 factors)</span>
        <span class="p">(</span><span class="mi">50</span><span class="p">,),</span>           <span class="c1"># single hidden layer</span>
        <span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>         <span class="c1"># two layers</span>
        <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>     <span class="c1"># three layers</span>
        <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="c1"># four layers</span>
        <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>     <span class="c1"># bottleneck model - reduce data to 10 final effects</span>
    <span class="p">],</span>
    <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.00001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="c1"># Activation: Relu is standard but logistic / sigmoid has been used by some</span>
    <span class="c1"># researchers in reserving applications</span>
    <span class="s2">&quot;activation&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;logistic&quot;</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">],</span> 
    <span class="s2">&quot;random_state&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;max_iter&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">200000</span><span class="p">]</span>  <span class="c1"># More iterations worked well for lasso model in R</span>
<span class="p">}</span>

<span class="n">col_transformer_nn</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;zero_to_one&#39;</span><span class="p">,</span> <span class="n">MinMaxScaler</span><span class="p">(),</span> <span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">,</span> <span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="s2">&quot;cal&quot;</span><span class="p">])</span>
    <span class="p">],</span>
    <span class="n">remainder</span><span class="o">=</span><span class="s1">&#39;drop&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="n">neural_network</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span>
    <span class="n">steps</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;transform&#39;</span><span class="p">,</span> <span class="n">col_transformer_nn</span><span class="p">),</span> 
        <span class="p">(</span><span class="s1">&#39;nn&#39;</span><span class="p">,</span> <span class="n">TransformedTargetRegressor</span><span class="p">(</span>
                <span class="n">regressor</span><span class="o">=</span><span class="n">RandomizedSearchCV</span><span class="p">(</span>
                  <span class="n">MLPRegressor</span><span class="p">(),</span>
                  <span class="n">parameters_nn</span><span class="p">,</span> 
                  <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># Run in parallel</span>
                  <span class="n">n_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="c1"># Models train slowly, so try only a few models</span>
                  <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                <span class="c1"># An output activation function of exp would have been nice</span>
                <span class="c1"># But not available in scikit, so we will fit on log-transformed </span>
                <span class="c1"># values and exponentiate.</span>
                <span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">y_train</span><span class="o">.</span><span class="n">mean</span><span class="p">()),</span> 
                <span class="n">inverse_func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">y_train</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
                <span class="n">check_inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">neural_network</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ran in </span><span class="si">{</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ran in 86.3401 seconds
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">neural_network</span><span class="p">[</span><span class="s2">&quot;nn&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">regressor_</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;activation&#39;: &#39;relu&#39;,
 &#39;alpha&#39;: 1e-05,
 &#39;batch_size&#39;: &#39;auto&#39;,
 &#39;beta_1&#39;: 0.9,
 &#39;beta_2&#39;: 0.999,
 &#39;early_stopping&#39;: False,
 &#39;epsilon&#39;: 1e-08,
 &#39;hidden_layer_sizes&#39;: (12, 12, 12, 12),
 &#39;learning_rate&#39;: &#39;constant&#39;,
 &#39;learning_rate_init&#39;: 0.001,
 &#39;max_fun&#39;: 15000,
 &#39;max_iter&#39;: 200000,
 &#39;momentum&#39;: 0.9,
 &#39;n_iter_no_change&#39;: 10,
 &#39;nesterovs_momentum&#39;: True,
 &#39;power_t&#39;: 0.5,
 &#39;random_state&#39;: 0,
 &#39;shuffle&#39;: True,
 &#39;solver&#39;: &#39;adam&#39;,
 &#39;tol&#39;: 0.0001,
 &#39;validation_fraction&#39;: 0.1,
 &#39;verbose&#39;: False,
 &#39;warm_start&#39;: False}
</pre></div>
</div>
</div>
</div>
<p>Before we do any analysis, let’s fit a couple more models to compare these results against:</p>
<ul class="simple">
<li><p>a Chainladder model</p></li>
<li><p>a LASSO model</p></li>
</ul>
</div>
<div class="section" id="chainladder-the-baseline-model">
<h2>Chainladder - the baseline model<a class="headerlink" href="#chainladder-the-baseline-model" title="Permalink to this headline">¶</a></h2>
<p>Since this is a traditional triangle, it seems natural to compare any results to the Chainladder result. So here, we will get the predicted values and reserve estimate for the Chainladder model.</p>
<p>It’s important to note that we will just use a volume-all Chainladder (i.e. include all periods in the estimation of the development factors) and will not attempt to impose any judgement over the results. In practice, of course, models like the Chainladder are often subject to additional analysis, reasonableness tests and manual assumption selections, so it’s likely the actual result would be different, perhaps significantly, from that returned here.</p>
<p>At the same time, no model, whether it be the Chainladder, or a more sophisticated ML model should be accepted without further testing, so on that basis, comparing Chainladder and ML results without modification is a reasonable thing to do. Better methods should require less human intervention to return a reasonable result.</p>
<div class="section" id="getting-the-chainladder-reserve-estimates">
<h3>Getting the Chainladder reserve estimates<a class="headerlink" href="#getting-the-chainladder-reserve-estimates" title="Permalink to this headline">¶</a></h3>
<p>The Chainladder reserve can also be calculated using a GLM with:</p>
<ul class="simple">
<li><p>Accident and development factors</p></li>
<li><p>The Poisson or over-dispersed Poisson distribution</p></li>
<li><p>The log link.</p></li>
</ul>
<p>We’ve used this method here as it’s easy to set up in Python but practical work using the Chainladder may be better done with a package like the <a class="reference external" href="https://chainladder-python.readthedocs.io/en/latest/">Python Chainladder package</a> by CASact.</p>
<p>The easiest way to do this is to use <a class="reference external" href="https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines">Pipelines</a> which allows us to have a neat package of a model that comes with its own data transformations - specifically to use <code class="docutils literal notranslate"><span class="pre">accf</span></code> and <code class="docutils literal notranslate"><span class="pre">devf</span></code> properly.</p>
<p>Here’s the code using <code class="docutils literal notranslate"><span class="pre">PoissonRegressor</span></code> and the <code class="docutils literal notranslate"><span class="pre">predict()</span></code> method to add predicted values into <code class="docutils literal notranslate"><span class="pre">model_forecasts</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">col_transformer</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span>
    <span class="p">[(</span><span class="s1">&#39;encoder&#39;</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">(),</span> <span class="p">[</span><span class="s2">&quot;accf&quot;</span><span class="p">,</span> <span class="s2">&quot;devf&quot;</span><span class="p">])],</span> 
    <span class="n">remainder</span><span class="o">=</span><span class="s1">&#39;drop&#39;</span>
<span class="p">)</span>

<span class="c1"># PoissonRegressor gives a log-link GLM. But need to set alpha=0 so it is not a LASSO.</span>
<span class="n">chain_ladder</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span>
    <span class="n">steps</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;transform&#39;</span><span class="p">,</span> <span class="n">col_transformer</span><span class="p">),</span> 
        <span class="p">(</span><span class="s1">&#39;glm&#39;</span><span class="p">,</span> <span class="n">PoissonRegressor</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">))</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">chain_ladder</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Pipeline(steps=[(&#39;transform&#39;,
                 ColumnTransformer(transformers=[(&#39;encoder&#39;, OneHotEncoder(),
                                                  [&#39;accf&#39;, &#39;devf&#39;])])),
                (&#39;glm&#39;, PoissonRegressor(alpha=0, max_iter=5000))])
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="lasso-regularised-regression">
<h2>LASSO (regularised regression)<a class="headerlink" href="#lasso-regularised-regression" title="Permalink to this headline">¶</a></h2>
<p>Finally, we will use the LASSO to fit a model. We are going to fit the same model as we did in our previous <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-lasso/">post</a>. We’ll include a bare-bones description of the model here. If you haven’t seen it before, then you may want to just take the model as given and skip ahead to the next section. Once you’ve read this entire article you may then wish to read the post on the LASSO model to understand the specifics of this model.</p>
<p>The main things to note about this model are:</p>
<ul class="simple">
<li><p>Feature engineering is needed to produce continuous functions of accident / development / calendar quarters for the LASSO to fit.</p></li>
<li><p>The blog post and related paper detail how to do this - essentially we create a large group of basis functions from the primary <code class="docutils literal notranslate"><span class="pre">acc</span></code>, <code class="docutils literal notranslate"><span class="pre">dev</span></code> and <code class="docutils literal notranslate"><span class="pre">cal</span></code> variables that are flexible enough to capture a wide variety of shapes.</p></li>
<li><p>We need to create a model that contains these basis functions as features.</p></li>
<li><p>We then need to fit and predict using the values for a particular penalty setting (the regularisation parameter) - following the paper, we use the penalty value that leads to the lowest cross-validation error.</p></li>
</ul>
<div class="section" id="basis-functions">
<h3>Basis functions<a class="headerlink" href="#basis-functions" title="Permalink to this headline">¶</a></h3>
<p>The first step is to create all the basis functions that we need. The functions below create the ramp and step functions needed (over 4000 of these!). The end result is a data.table of the original payments and all the basis functions. As noted above, all the details are in the blog post and paper.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">LinearSpline</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Linear spline function - used in data generation and in spline generation below</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">stop</span> <span class="o">-</span> <span class="n">start</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">var</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">GetScaling</span><span class="p">(</span><span class="n">vec</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function to calculate scaling factors for the basis functions</span>
<span class="sd">    scaling is discussed in the paper</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fn</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
    <span class="n">fm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
    <span class="n">fc</span> <span class="o">=</span> <span class="n">vec</span> <span class="o">-</span> <span class="n">fm</span>  

    <span class="k">return</span>  <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">fc</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span><span class="o">/</span><span class="n">fn</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span>


<span class="k">def</span> <span class="nf">GetRamps</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="n">vecname</span><span class="p">,</span> <span class="n">nperiods</span><span class="p">,</span> <span class="n">scaling</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function to create the ramps for a particular primary vector</span>
<span class="sd">    vec = fundamental regressor</span>
<span class="sd">    vecname = name of regressor</span>
<span class="sd">    np = number of periods</span>
<span class="sd">    scaling = scaling factor to use</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="sa">f</span><span class="s2">&quot;L_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">_999_</span><span class="si">{</span><span class="n">vecname</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">LinearSpline</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="mi">999</span><span class="p">)</span> <span class="o">/</span> <span class="n">scaling</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">nperiods</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>


<span class="k">def</span> <span class="nf">GetInts</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">,</span> <span class="n">vecname1</span><span class="p">,</span> <span class="n">vecname2</span><span class="p">,</span> <span class="n">nperiods</span><span class="p">,</span> <span class="n">scaling1</span><span class="p">,</span> <span class="n">scaling2</span><span class="p">,</span> <span class="n">train_ind</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create the step (heaviside) function interactions.</span>
<span class="sd">    f&quot;I_{vecname1}_ge_{i}xI_{vecname2}_ge_{j}&quot; formats the name of the column</span>
<span class="sd">    LinearSpline(vec1, 1, i+1) / scaling1 * LinearSpline(vec2, j, j + 1) / scaling2</span>
<span class="sd">    is the interaction term</span>
<span class="sd">    and we loop over all combinations of 1:nperiods and 2:nperiods</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">vecs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">nperiods</span><span class="p">),</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">nperiods</span><span class="p">)]):</span>
        <span class="n">interaction</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">LinearSpline</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="o">/</span> <span class="n">scaling1</span> <span class="o">*</span> 
            <span class="n">LinearSpline</span><span class="p">(</span><span class="n">vec2</span><span class="p">,</span> <span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="o">/</span> <span class="n">scaling2</span>
        <span class="p">)</span>

        <span class="c1"># Only include if non-constant over training data</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">interaction</span><span class="p">[</span><span class="n">train_ind</span><span class="p">]</span> <span class="o">==</span> <span class="n">interaction</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">vecs</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;I_</span><span class="si">{</span><span class="n">vecname1</span><span class="si">}</span><span class="s2">_ge_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">xI_</span><span class="si">{</span><span class="n">vecname2</span><span class="si">}</span><span class="s2">_ge_</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">interaction</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">vecs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
</div>
<p>Now the functions are defined, we’ll create a data.table of basis functions here which we’ll use later when fitting the LASSO.
The <code class="docutils literal notranslate"><span class="pre">dat_plus</span></code> table will hold them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># get the scaling values</span>
<span class="n">rho_factor_list</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">v</span><span class="p">:</span> <span class="n">GetScaling</span><span class="p">(</span><span class="n">dat</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">dat</span><span class="o">.</span><span class="n">train_ind</span><span class="p">,</span> <span class="n">v</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> 
    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">,</span> <span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="s2">&quot;cal&quot;</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">rho_factor_list</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;acc&#39;: 9.539392014169456, &#39;dev&#39;: 9.539392014169456, &#39;cal&#39;: 9.539392014169456}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># main effects - matrix of values</span>
<span class="n">main_effects_acc</span> <span class="o">=</span> <span class="n">GetRamps</span><span class="p">(</span><span class="n">vec</span> <span class="o">=</span> <span class="n">dat</span><span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">],</span> <span class="n">vecname</span> <span class="o">=</span> <span class="s2">&quot;acc&quot;</span><span class="p">,</span> <span class="n">nperiods</span> <span class="o">=</span> <span class="n">num_periods</span><span class="p">,</span> <span class="n">scaling</span> <span class="o">=</span> <span class="n">rho_factor_list</span><span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">])</span>
<span class="n">main_effects_dev</span> <span class="o">=</span> <span class="n">GetRamps</span><span class="p">(</span><span class="n">vec</span> <span class="o">=</span> <span class="n">dat</span><span class="p">[</span><span class="s2">&quot;dev&quot;</span><span class="p">],</span> <span class="n">vecname</span> <span class="o">=</span> <span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="n">nperiods</span> <span class="o">=</span> <span class="n">num_periods</span><span class="p">,</span> <span class="n">scaling</span> <span class="o">=</span> <span class="n">rho_factor_list</span><span class="p">[</span><span class="s2">&quot;dev&quot;</span><span class="p">])</span>
<span class="n">main_effects_cal</span> <span class="o">=</span> <span class="n">GetRamps</span><span class="p">(</span><span class="n">vec</span> <span class="o">=</span> <span class="n">dat</span><span class="p">[</span><span class="s2">&quot;cal&quot;</span><span class="p">],</span> <span class="n">vecname</span> <span class="o">=</span> <span class="s2">&quot;cal&quot;</span><span class="p">,</span> <span class="n">nperiods</span> <span class="o">=</span> <span class="n">num_periods</span><span class="p">,</span> <span class="n">scaling</span> <span class="o">=</span> <span class="n">rho_factor_list</span><span class="p">[</span><span class="s2">&quot;cal&quot;</span><span class="p">])</span>

<span class="n">main_effects</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">main_effects_acc</span><span class="p">,</span> <span class="n">main_effects_dev</span><span class="p">,</span> <span class="n">main_effects_cal</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">main_effects</span></code> looks like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">main_effects</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>L_1_999_acc</th>
      <th>L_2_999_acc</th>
      <th>L_3_999_acc</th>
      <th>L_4_999_acc</th>
      <th>L_5_999_acc</th>
      <th>L_6_999_acc</th>
      <th>L_7_999_acc</th>
      <th>L_8_999_acc</th>
      <th>L_9_999_acc</th>
      <th>L_10_999_acc</th>
      <th>...</th>
      <th>L_30_999_cal</th>
      <th>L_31_999_cal</th>
      <th>L_32_999_cal</th>
      <th>L_33_999_cal</th>
      <th>L_34_999_cal</th>
      <th>L_35_999_cal</th>
      <th>L_36_999_cal</th>
      <th>L_37_999_cal</th>
      <th>L_38_999_cal</th>
      <th>L_39_999_cal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1595</th>
      <td>4.088311</td>
      <td>3.983482</td>
      <td>3.878654</td>
      <td>3.773825</td>
      <td>3.668997</td>
      <td>3.564168</td>
      <td>3.45934</td>
      <td>3.354511</td>
      <td>3.249683</td>
      <td>3.144854</td>
      <td>...</td>
      <td>4.717281</td>
      <td>4.612453</td>
      <td>4.507625</td>
      <td>4.402796</td>
      <td>4.297967</td>
      <td>4.193139</td>
      <td>4.088311</td>
      <td>3.983482</td>
      <td>3.878654</td>
      <td>3.773825</td>
    </tr>
    <tr>
      <th>1596</th>
      <td>4.088311</td>
      <td>3.983482</td>
      <td>3.878654</td>
      <td>3.773825</td>
      <td>3.668997</td>
      <td>3.564168</td>
      <td>3.45934</td>
      <td>3.354511</td>
      <td>3.249683</td>
      <td>3.144854</td>
      <td>...</td>
      <td>4.822110</td>
      <td>4.717281</td>
      <td>4.612453</td>
      <td>4.507625</td>
      <td>4.402796</td>
      <td>4.297967</td>
      <td>4.193139</td>
      <td>4.088311</td>
      <td>3.983482</td>
      <td>3.878654</td>
    </tr>
    <tr>
      <th>1597</th>
      <td>4.088311</td>
      <td>3.983482</td>
      <td>3.878654</td>
      <td>3.773825</td>
      <td>3.668997</td>
      <td>3.564168</td>
      <td>3.45934</td>
      <td>3.354511</td>
      <td>3.249683</td>
      <td>3.144854</td>
      <td>...</td>
      <td>4.926939</td>
      <td>4.822110</td>
      <td>4.717281</td>
      <td>4.612453</td>
      <td>4.507625</td>
      <td>4.402796</td>
      <td>4.297967</td>
      <td>4.193139</td>
      <td>4.088311</td>
      <td>3.983482</td>
    </tr>
    <tr>
      <th>1598</th>
      <td>4.088311</td>
      <td>3.983482</td>
      <td>3.878654</td>
      <td>3.773825</td>
      <td>3.668997</td>
      <td>3.564168</td>
      <td>3.45934</td>
      <td>3.354511</td>
      <td>3.249683</td>
      <td>3.144854</td>
      <td>...</td>
      <td>5.031767</td>
      <td>4.926939</td>
      <td>4.822110</td>
      <td>4.717281</td>
      <td>4.612453</td>
      <td>4.507625</td>
      <td>4.402796</td>
      <td>4.297967</td>
      <td>4.193139</td>
      <td>4.088311</td>
    </tr>
    <tr>
      <th>1599</th>
      <td>4.088311</td>
      <td>3.983482</td>
      <td>3.878654</td>
      <td>3.773825</td>
      <td>3.668997</td>
      <td>3.564168</td>
      <td>3.45934</td>
      <td>3.354511</td>
      <td>3.249683</td>
      <td>3.144854</td>
      <td>...</td>
      <td>5.136595</td>
      <td>5.031767</td>
      <td>4.926939</td>
      <td>4.822110</td>
      <td>4.717281</td>
      <td>4.612453</td>
      <td>4.507625</td>
      <td>4.402796</td>
      <td>4.297967</td>
      <td>4.193139</td>
    </tr>
  </tbody>
</table>
<p>1600 rows × 117 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Export to file for debugging</span>
<span class="c1"># main_effects.to_csv(&quot;main_effects_py.csv&quot;)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># interaction effects</span>
<span class="n">int_effects</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
    <span class="n">GetInts</span><span class="p">(</span><span class="n">vec1</span><span class="o">=</span><span class="n">dat</span><span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">],</span> <span class="n">vecname1</span><span class="o">=</span><span class="s2">&quot;acc&quot;</span><span class="p">,</span> <span class="n">scaling1</span><span class="o">=</span><span class="n">rho_factor_list</span><span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">],</span>  
            <span class="n">vec2</span><span class="o">=</span><span class="n">dat</span><span class="p">[</span><span class="s2">&quot;dev&quot;</span><span class="p">],</span> <span class="n">vecname2</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="n">scaling2</span><span class="o">=</span><span class="n">rho_factor_list</span><span class="p">[</span><span class="s2">&quot;dev&quot;</span><span class="p">],</span> 
            <span class="n">nperiods</span><span class="o">=</span><span class="n">num_periods</span><span class="p">,</span> <span class="n">train_ind</span><span class="o">=</span><span class="n">dat</span><span class="p">[</span><span class="s2">&quot;train_ind&quot;</span><span class="p">]),</span>

    <span class="n">GetInts</span><span class="p">(</span><span class="n">vec1</span><span class="o">=</span><span class="n">dat</span><span class="p">[</span><span class="s2">&quot;dev&quot;</span><span class="p">],</span> <span class="n">vecname1</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="n">scaling1</span><span class="o">=</span><span class="n">rho_factor_list</span><span class="p">[</span><span class="s2">&quot;dev&quot;</span><span class="p">],</span>  
            <span class="n">vec2</span><span class="o">=</span><span class="n">dat</span><span class="p">[</span><span class="s2">&quot;cal&quot;</span><span class="p">],</span> <span class="n">vecname2</span><span class="o">=</span><span class="s2">&quot;cal&quot;</span><span class="p">,</span> <span class="n">scaling2</span><span class="o">=</span><span class="n">rho_factor_list</span><span class="p">[</span><span class="s2">&quot;cal&quot;</span><span class="p">],</span> 
            <span class="n">nperiods</span><span class="o">=</span><span class="n">num_periods</span><span class="p">,</span> <span class="n">train_ind</span><span class="o">=</span><span class="n">dat</span><span class="p">[</span><span class="s2">&quot;train_ind&quot;</span><span class="p">]),</span>
    
    <span class="n">GetInts</span><span class="p">(</span><span class="n">vec1</span><span class="o">=</span><span class="n">dat</span><span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">],</span> <span class="n">vecname1</span><span class="o">=</span><span class="s2">&quot;acc&quot;</span><span class="p">,</span> <span class="n">scaling1</span><span class="o">=</span><span class="n">rho_factor_list</span><span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">],</span>  
            <span class="n">vec2</span><span class="o">=</span><span class="n">dat</span><span class="p">[</span><span class="s2">&quot;cal&quot;</span><span class="p">],</span> <span class="n">vecname2</span><span class="o">=</span><span class="s2">&quot;cal&quot;</span><span class="p">,</span> <span class="n">scaling2</span><span class="o">=</span><span class="n">rho_factor_list</span><span class="p">[</span><span class="s2">&quot;cal&quot;</span><span class="p">],</span> 
            <span class="n">nperiods</span><span class="o">=</span><span class="n">num_periods</span><span class="p">,</span> <span class="n">train_ind</span><span class="o">=</span><span class="n">dat</span><span class="p">[</span><span class="s2">&quot;train_ind&quot;</span><span class="p">])</span>
    <span class="p">],</span> 
    <span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The table looks like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">int_effects</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>I_acc_ge_2xI_dev_ge_2</th>
      <th>I_acc_ge_2xI_dev_ge_3</th>
      <th>I_acc_ge_2xI_dev_ge_4</th>
      <th>I_acc_ge_2xI_dev_ge_5</th>
      <th>I_acc_ge_2xI_dev_ge_6</th>
      <th>I_acc_ge_2xI_dev_ge_7</th>
      <th>I_acc_ge_2xI_dev_ge_8</th>
      <th>I_acc_ge_2xI_dev_ge_9</th>
      <th>I_acc_ge_2xI_dev_ge_10</th>
      <th>I_acc_ge_2xI_dev_ge_11</th>
      <th>...</th>
      <th>I_acc_ge_39xI_cal_ge_30</th>
      <th>I_acc_ge_39xI_cal_ge_31</th>
      <th>I_acc_ge_39xI_cal_ge_32</th>
      <th>I_acc_ge_39xI_cal_ge_33</th>
      <th>I_acc_ge_39xI_cal_ge_34</th>
      <th>I_acc_ge_39xI_cal_ge_35</th>
      <th>I_acc_ge_39xI_cal_ge_36</th>
      <th>I_acc_ge_39xI_cal_ge_37</th>
      <th>I_acc_ge_39xI_cal_ge_38</th>
      <th>I_acc_ge_39xI_cal_ge_39</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1595</th>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>...</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
    </tr>
    <tr>
      <th>1596</th>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>...</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
    </tr>
    <tr>
      <th>1597</th>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>...</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
    </tr>
    <tr>
      <th>1598</th>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>...</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
    </tr>
    <tr>
      <th>1599</th>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>...</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
      <td>0.010989</td>
    </tr>
  </tbody>
</table>
<p>1600 rows × 3629 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Export to file for debugging</span>
<span class="c1"># int_effects.to_csv(&quot;int_effects_py.csv&quot;)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">varset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">main_effects</span><span class="p">,</span> <span class="n">int_effects</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span><span class="p">)</span>
<span class="n">varset_train</span> <span class="o">=</span> <span class="n">varset</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">dat</span><span class="o">.</span><span class="n">train_ind</span><span class="p">]</span>

<span class="c1"># drop any constant columns over the training data set</span>
<span class="c1"># do this by identifying the constant columns and dropping them</span>
<span class="n">keep_cols</span> <span class="o">=</span> <span class="p">(</span><span class="n">varset</span> <span class="o">!=</span> <span class="n">varset</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>

<span class="n">varset</span> <span class="o">=</span> <span class="n">varset</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">keep_cols</span><span class="p">]</span> 
<span class="n">varset_train</span> <span class="o">=</span> <span class="n">varset_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">keep_cols</span><span class="p">]</span>

<span class="c1"># now add these variables into an extended data object</span>
<span class="c1"># remove anything not used in modelling</span>
<span class="n">dat_plus</span> <span class="o">=</span>  <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">dat</span><span class="p">[[</span><span class="s2">&quot;pmts&quot;</span><span class="p">,</span> <span class="s2">&quot;train_ind&quot;</span><span class="p">]],</span> 
        <span class="n">varset</span>
    <span class="p">],</span> 
    <span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="setup-for-lasso">
<h3>Setup for LASSO<a class="headerlink" href="#setup-for-lasso" title="Permalink to this headline">¶</a></h3>
<p>First we need to set up the data for the model. This differs from the tree-based models task as follows in that the input variables are all the basis functions we created and not the raw accident / development / calendar quarter terms - the <code class="docutils literal notranslate"><span class="pre">dat_plus</span></code> data.table.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_plus</span> <span class="o">=</span> <span class="n">varset</span><span class="p">[</span><span class="n">dat</span><span class="o">.</span><span class="n">train_ind</span><span class="p">]</span>
<span class="n">X_test_plus</span> <span class="o">=</span> <span class="n">varset</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">dat</span><span class="o">.</span><span class="n">train_ind</span> <span class="o">==</span> <span class="kc">False</span><span class="p">]</span>
<span class="n">X_plus</span> <span class="o">=</span> <span class="n">varset</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="diy-glmnet-with-pytorch">
<h3>DIY GLMnet with Pytorch<a class="headerlink" href="#diy-glmnet-with-pytorch" title="Permalink to this headline">¶</a></h3>
<p>The results will not be identical to those in the paper since the Python implementation is different and uses different hyper-parameters.</p>
<p>We want to use GLM with LASSO regularization and CV, but at the time of writing, the implementation in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> has major limitations.</p>
<p>The <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PoissonRegressor.html">PoissonRegressor</a> does not have “warm restarts” which allows re-using the coefficients as it tests through different regularisation factors, so CV is slow. More importantly, its <code class="docutils literal notranslate"><span class="pre">alpha</span></code> regularization parameter is a l2 ridge regression penalty, not the l1 LASSO penalty.</p>
<p>As far as alternatives go, the following are unlikely to meet our needs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> seemed incredibly slow in our early experiments, taking over a day to train a single model on this data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pyglmnet</span></code> also seemed incredibly slow.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">glmnet_py</span></code> requires Linux. It should in theory run via <a class="reference external" href="https://colab.research.google.com/">Google Colab</a> or <a class="reference external" href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">Windows Subsystem for Linux</a> as well, but for this particular tutorial we wanted to share a multi-platform solution.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">python-glmnet</span></code> only implements linear and logistic regression.</p></li>
</ul>
<p>Our options seem to be using the cluster computing framework <a class="reference external" href="https://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/intro.html">h2o.ai</a> or creating our own LASSO using a neural network framework like <strong>pytorch</strong>.</p>
<p>Both seem a little overkill but, but since we’re planning to cover more on neural networks in future articles, we can use <strong>pytorch</strong> to introduce some concepts here.</p>
<p>Firstly we will just import pytorch. If you are running this on your pc and have a GPU you would like to utilise, make sure you <a class="reference external" href="https://pytorch.org/get-started/locally/">install the right version for best performance</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Normal</span><span class="p">,</span> <span class="n">OneHotCategorical</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="kn">import</span> <span class="n">check_X_y</span><span class="p">,</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">check_is_fitted</span>
</pre></div>
</div>
</div>
</div>
<p>First we will create a <strong>Pytorch Module</strong> for the GLM. This is pretty straightforward - a neural network with no hidden layers is a LM, and by applying an exponential to the output, we have a log-link GLM.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LogLinkGLM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># Define the parameters in __init__</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">n_input</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span> <span class="c1"># number of inputs</span>

        
        <span class="nb">super</span><span class="p">(</span><span class="n">LogLinkGLM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_input</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># These will be the coefficients</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>         <span class="c1"># Initialise these to zero</span>


    <span class="c1"># forward defines how you get y from X.</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># log(Y) = XB -&gt; Y = exp(XB)</span>
</pre></div>
</div>
</div>
</div>
<p>Next, to fit it within our <strong>scikit-learn</strong> framework, we will make a <strong>scikit-learn</strong> Regressor. The key components are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>: Define any hyperparameters here. At a minimum, this would include the lasso regularization penalty.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fit</span></code>: Define the training process for the model. For <code class="docutils literal notranslate"><span class="pre">pytorch</span></code> we have a training loop where we read data, calculated the loss, backpropogate it and update the weights.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">predict</span></code>: Define how to get predictions - i.e. apply the <code class="docutils literal notranslate"><span class="pre">forward</span></code> method of the Module.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">score</span></code>: Define performance. Here we will use RMSE for consistency with the notebook.</p></li>
</ul>
<p>There is also some added boilerplate about getting data into the right format.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LogLinkGLMNetRegressor</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">l1_penalty</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>          <span class="c1"># lambda is a reserved word</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="mi">200000</span><span class="p">,</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">min_improvement</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">check_improvement_every_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
        <span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">init_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">init_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">target_device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)):</span>
        <span class="sd">&quot;&quot;&quot; Log Link GLM LASSO Regressor</span>

<span class="sd">        This trains a GLM with Poisson loss, Log Link and l1 LASSO penalties</span>
<span class="sd">        using Pytorch. It has early stopping </span>

<span class="sd">        Note:</span>
<span class="sd">            Do not include the `self` parameter in the ``Args`` section.</span>

<span class="sd">        Args:</span>
<span class="sd">            l1_penalty (float): l1 penalty factor (we use l1_penalty because lambda is</span>
<span class="sd">                a reserved word in Python for anonymous functions)</span>

<span class="sd">            weight_decay (float): weight decay - analogous to l2 penalty factor</span>
<span class="sd">            </span>
<span class="sd">            max_iter (int): Maximum number of epochs before training stops</span>

<span class="sd">            lr (float): Learning rate</span>

<span class="sd">            min_improvement (float), patience (int), check_improvement_every_iter (int): </span>
<span class="sd">                RMSE must improve by this percent otherwise patience will decrement. </span>
<span class="sd">                Once patience reaches zero, training stops.</span>

<span class="sd">            verbose (int): 0 means don&#39;t print. 1 means do print.</span>

<span class="sd">            init_weight, init_bias (torch tensor): initial weight (coefficient) </span>
<span class="sd">                and bias (intercept) for a warm start. </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1_penalty</span> <span class="o">=</span> <span class="n">l1_penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_device</span> <span class="o">=</span> <span class="n">target_device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_improvement</span> <span class="o">=</span> <span class="n">min_improvement</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="o">=</span> <span class="n">patience</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_weight</span> <span class="o">=</span> <span class="n">init_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_bias</span> <span class="o">=</span> <span class="n">init_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_improvement_every_iter</span> <span class="o">=</span> <span class="n">check_improvement_every_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>

        <span class="c1"># Check that X and y have correct shape</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1"># Skorch is picky about format</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">):</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">values</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">values</span>

        <span class="c1"># Convert to Pytorch Tensor</span>
        <span class="n">X_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_device</span><span class="p">)</span>
        <span class="n">y_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_device</span><span class="p">)</span>

        <span class="n">n_input</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module_</span> <span class="o">=</span> <span class="n">LogLinkGLM</span><span class="p">(</span><span class="n">n_input</span><span class="o">=</span><span class="n">n_input</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_device</span><span class="p">)</span>

        <span class="c1"># Set initial weight</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module_</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_weight</span>

        <span class="c1"># Set initial intercept </span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module_</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_bias</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># To something close so gradients do not explode</span>
            <span class="n">avg_log_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module_</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">avg_log_y</span><span class="p">])</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
            <span class="n">params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module_</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
            <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span>
        <span class="p">)</span>
        <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PoissonNLLLoss</span><span class="p">(</span><span class="n">log_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_device</span><span class="p">)</span>

        <span class="n">last_rmse</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">patience</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span>

        <span class="c1"># Training loop</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">):</span>        <span class="c1"># Repeat max_iter times</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">)</span>  <span class="c1"># Apply current model</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_tensor</span><span class="p">)</span> <span class="c1"># What is the loss on it?</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_penalty</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>  <span class="c1"># lasso        </span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>            <span class="c1"># Reset optimizer</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>                  <span class="c1"># Apply back propagation</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>                 <span class="c1"># Update model parameters</span>

            <span class="c1"># Every 5,000 steps (or setting) get RMSE </span>
            <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_improvement_every_iter</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>   
                <span class="n">rmse</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_tensor</span><span class="p">)))</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train RMSE: &quot;</span><span class="p">,</span> <span class="n">rmse</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>    <span class="c1"># Print loss</span>

                <span class="c1"># Stop if not </span>
                <span class="k">if</span> <span class="n">rmse</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">last_rmse</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_improvement</span><span class="p">):</span>
                    <span class="n">patience</span> <span class="o">+=</span> <span class="o">-</span><span class="mi">1</span>
                
                <span class="k">if</span> <span class="n">patience</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Insufficient improvement found, stopping training&quot;</span><span class="p">)</span>
                    <span class="k">break</span>

                <span class="n">last_rmse</span> <span class="o">=</span> <span class="n">rmse</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Return the regressor</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>

        <span class="c1"># Check is fit had been called</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="c1"># Input validation</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># Skorch is picky about format</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">):</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">values</span>     

        <span class="c1"># Convert to Pytorch Tensor</span>
        <span class="n">X_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_device</span><span class="p">)</span>

        <span class="c1"># Apply current model and return</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>


    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># Negative RMSE score (higher needs to be better)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="previous-best-fit">
<h3>Previous best fit<a class="headerlink" href="#previous-best-fit" title="Permalink to this headline">¶</a></h3>
<p>So, can we replicate the previously tuned model from R in Python?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>

<span class="n">lasso_prev</span> <span class="o">=</span> <span class="n">LogLinkGLMNetRegressor</span><span class="p">(</span><span class="n">l1_penalty</span><span class="o">=</span><span class="mi">7674</span><span class="p">)</span>
<span class="n">lasso_prev</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_plus</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ran in </span><span class="si">{</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train RMSE:  428498688.0
Train RMSE:  53182680.0
Train RMSE:  52894192.0
Train RMSE:  48454752.0
Train RMSE:  47999504.0
Insufficient improvement found, stopping training
Ran in 64.2999 seconds
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id1">
<h3>Cross Validation<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>With our new Regressor we can now do CV with something that resembles warm starts by putting an initial fit (here we will recycle the previous best GLM from R) as the starting weights for the search.</p>
<p>The key parameter to tune in lasso is the regularisation penalty. With <strong>glmnet</strong> in R and in most academic literature, this is typically called <code class="docutils literal notranslate"><span class="pre">lambda</span></code>. However, with scikit-learn in Python, with <code class="docutils literal notranslate"><span class="pre">lambda</span></code> being a reserved keyword for anonymous functions, we will call the same parameter <code class="docutils literal notranslate"><span class="pre">l1_penalty</span></code>.</p>
<p>With our DIY “semi warm start” the training is faster but still takes a while. To keep a reasonable running time for running the example we will keep it to ten runs near the optimal lambda from the R run, but if we were building the model from scratch we would be testing many more values of lambda.</p>
<p>If running time is a critical factor and it is taking too long, three considerations:</p>
<ol class="simple">
<li><p><strong>pytorch</strong> runs faster with a GPU,</p></li>
<li><p>You can get much smarter with the warm start logic.</p></li>
<li><p>You can also consider the GLM implementation in the <a class="reference external" href="http://h2o.ai">h2o.ai</a> package mentioned earlier which is quite fast.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lambdas</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">86</span><span class="p">,</span> <span class="mi">96</span><span class="p">)]</span>
<span class="n">lambdas</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[5431.659591362978,
 6002.912217261029,
 6634.24400627789,
 7331.973539155995,
 8103.083927575384,
 8955.292703482508,
 9897.129058743927,
 10938.019208165191,
 12088.380730216988,
 13359.726829661873]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters_lasso</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;l1_penalty&quot;</span><span class="p">:</span> <span class="n">lambdas</span><span class="p">,</span> 
    <span class="s2">&quot;init_weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">lasso_prev</span><span class="o">.</span><span class="n">module_</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">],</span>
    <span class="s2">&quot;init_bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">lasso_prev</span><span class="o">.</span><span class="n">module_</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">],</span>
    <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">],</span> <span class="c1"># Fine tuning?</span>
    <span class="s2">&quot;check_improvement_every_iter&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">],</span> 
    <span class="s2">&quot;verbose&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>

<span class="c1"># N.b. this will not run in parallel.</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">LogLinkGLMNetRegressor</span><span class="p">(),</span> 
    <span class="n">parameters_lasso</span><span class="p">,</span> 
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_plus</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ran in </span><span class="si">{</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting 5 folds for each of 10 candidates, totalling 50 fits
Ran in 34.7665 seconds
</pre></div>
</div>
</div>
</div>
<p>Let’s plot the test error vs different regularisation parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check the path includes a minimum value of error</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
  <span class="s2">&quot;log(lambda)&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="s2">&quot;l1_penalty&quot;</span><span class="p">])</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">lasso</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]],</span>
  <span class="s2">&quot;mean_test_score&quot;</span><span class="p">:</span> <span class="n">lasso</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s2">&quot;mean_test_score&quot;</span><span class="p">]</span>
<span class="p">})</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;log(lambda)&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;mean_test_score&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:xlabel=&#39;log(lambda)&#39;&gt;
</pre></div>
</div>
<img alt="../_images/MLRWP_Py_triangles_example_109_1.png" src="../_images/MLRWP_Py_triangles_example_109_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best regularisation parameter [log(lambda)] was:&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">get_params</span><span class="p">()[</span><span class="s2">&quot;l1_penalty&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best regularisation parameter [log(lambda)] was:
9.1
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the best fit:</span>
<span class="n">lasso</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;check_improvement_every_iter&#39;: 100,
 &#39;init_bias&#39;: tensor([15.3488]),
 &#39;init_weight&#39;: tensor([[-1.1317e+00, -8.1524e-01, -5.3453e-01,  ...,  1.3915e-04,
           1.3895e-04,  1.3895e-04]]),
 &#39;l1_penalty&#39;: 8955.292703482508,
 &#39;lr&#39;: 0.001,
 &#39;max_iter&#39;: 200000,
 &#39;min_improvement&#39;: 0.01,
 &#39;patience&#39;: 3,
 &#39;target_device&#39;: device(type=&#39;cpu&#39;),
 &#39;verbose&#39;: 0,
 &#39;weight_decay&#39;: 0.0}
</pre></div>
</div>
</div>
</div>
<p>We arrive back at the original value of <code class="docutils literal notranslate"><span class="pre">lambda</span></code>.</p>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="model-analysis">
<h1>Model analysis<a class="headerlink" href="#model-analysis" title="Permalink to this headline">¶</a></h1>
<p>This is the interesting bit - let’s look at all the models to see how they’ve performed (but remember not to draw too many conclusions about model performance from this example as discussed earlier!).</p>
<div class="section" id="consolidate-results">
<h2>Consolidate results<a class="headerlink" href="#consolidate-results" title="Permalink to this headline">¶</a></h2>
<p>Now we will consolidate results for these models. We will gather together:</p>
<ul class="simple">
<li><p>RMSE for past and future data</p></li>
<li><p>predictions for each model.</p></li>
</ul>
<p>Note that <strong>scikit-learn</strong> has various tuning tools which we haven’t used here yet - partially because we want to focus on the concepts rather than the code and partially because we want to compare the results to a couple of other models not fitted in the same framework. If you’re interested in learning more then the section on <a class="reference external" href="https://scikit-learn.org/stable/modules/grid_search.html">tuning</a> in the documentation is a good start.</p>
<p>First, we’ll set up a data frame to hold the model projections for each model and populate these projections for each model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dataset of all forecasts</span>
<span class="n">model_forecasts</span> <span class="o">=</span> <span class="n">dat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">model_forecasts</span><span class="p">[</span><span class="s2">&quot;Decision Tree&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grid_tree</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">model_forecasts</span><span class="p">[</span><span class="s2">&quot;Random Forest&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grid_forest</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">model_forecasts</span><span class="p">[</span><span class="s2">&quot;Classic GBM&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grid_gbm</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">model_forecasts</span><span class="p">[</span><span class="s2">&quot;Histogram GBM&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grid_hgbm</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">model_forecasts</span><span class="p">[</span><span class="s2">&quot;Hist GBM (prev)&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gbm_prev</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">model_forecasts</span><span class="p">[</span><span class="s2">&quot;Neural Network&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">neural_network</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">model_forecasts</span><span class="p">[</span><span class="s2">&quot;Chain Ladder&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">chain_ladder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">model_forecasts</span><span class="p">[</span><span class="s2">&quot;LASSO&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_plus</span><span class="p">)</span>
<span class="n">model_forecasts</span><span class="p">[</span><span class="s2">&quot;LASSO (prev)&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso_prev</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_plus</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s how our collated forecast DataFrame looks so far:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_forecasts</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pmts</th>
      <th>acc</th>
      <th>dev</th>
      <th>cal</th>
      <th>mu</th>
      <th>train_ind</th>
      <th>accf</th>
      <th>devf</th>
      <th>Decision Tree</th>
      <th>Random Forest</th>
      <th>Classic GBM</th>
      <th>Histogram GBM</th>
      <th>Hist GBM (prev)</th>
      <th>Neural Network</th>
      <th>Chain Ladder</th>
      <th>LASSO</th>
      <th>LASSO (prev)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.426712e+05</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>7.165313e+04</td>
      <td>True</td>
      <td>0</td>
      <td>0</td>
      <td>2.426712e+05</td>
      <td>1.887456e+05</td>
      <td>6.014116e+06</td>
      <td>3.500558e+04</td>
      <td>4.569216e+04</td>
      <td>2.034437e+04</td>
      <td>3.585367e+04</td>
      <td>4633215.0</td>
      <td>4641892.5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.640013e+05</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.042776e+06</td>
      <td>True</td>
      <td>0</td>
      <td>1</td>
      <td>1.640013e+05</td>
      <td>2.087371e+05</td>
      <td>6.072307e+06</td>
      <td>5.235618e+05</td>
      <td>7.127655e+05</td>
      <td>2.364422e+05</td>
      <td>1.234291e+06</td>
      <td>5844869.0</td>
      <td>5853403.5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.224478e+06</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>4.362600e+06</td>
      <td>True</td>
      <td>0</td>
      <td>2</td>
      <td>3.224478e+06</td>
      <td>3.578825e+06</td>
      <td>6.259892e+06</td>
      <td>2.676693e+06</td>
      <td>2.217522e+06</td>
      <td>2.121035e+06</td>
      <td>3.324359e+06</td>
      <td>8970644.0</td>
      <td>8977765.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.682531e+06</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>1.095567e+07</td>
      <td>True</td>
      <td>0</td>
      <td>3</td>
      <td>3.682531e+06</td>
      <td>5.627768e+06</td>
      <td>9.945203e+06</td>
      <td>5.691426e+06</td>
      <td>5.253524e+06</td>
      <td>4.810498e+06</td>
      <td>8.746143e+06</td>
      <td>15132947.0</td>
      <td>15134997.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.014937e+07</td>
      <td>1.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>2.080055e+07</td>
      <td>True</td>
      <td>0</td>
      <td>4</td>
      <td>1.014937e+07</td>
      <td>1.605141e+07</td>
      <td>2.417595e+07</td>
      <td>1.582105e+07</td>
      <td>1.564311e+07</td>
      <td>1.021459e+07</td>
      <td>1.715827e+07</td>
      <td>24922094.0</td>
      <td>24914536.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1595</th>
      <td>6.265737e+07</td>
      <td>40.0</td>
      <td>36.0</td>
      <td>75.0</td>
      <td>8.285330e+07</td>
      <td>False</td>
      <td>39</td>
      <td>35</td>
      <td>3.276252e+09</td>
      <td>3.088414e+09</td>
      <td>2.819504e+09</td>
      <td>3.895124e+08</td>
      <td>3.132752e+08</td>
      <td>1.119816e+08</td>
      <td>2.794976e+08</td>
      <td>155724928.0</td>
      <td>140357952.0</td>
    </tr>
    <tr>
      <th>1596</th>
      <td>6.346768e+07</td>
      <td>40.0</td>
      <td>37.0</td>
      <td>76.0</td>
      <td>6.268272e+07</td>
      <td>False</td>
      <td>39</td>
      <td>36</td>
      <td>3.276252e+09</td>
      <td>3.088414e+09</td>
      <td>2.819504e+09</td>
      <td>3.895124e+08</td>
      <td>3.132752e+08</td>
      <td>1.005221e+08</td>
      <td>1.896219e+09</td>
      <td>200310144.0</td>
      <td>182843872.0</td>
    </tr>
    <tr>
      <th>1597</th>
      <td>2.604198e+07</td>
      <td>40.0</td>
      <td>38.0</td>
      <td>77.0</td>
      <td>4.722784e+07</td>
      <td>False</td>
      <td>39</td>
      <td>37</td>
      <td>3.276252e+09</td>
      <td>3.088414e+09</td>
      <td>2.819504e+09</td>
      <td>3.895124e+08</td>
      <td>3.132752e+08</td>
      <td>9.023538e+07</td>
      <td>4.350021e+08</td>
      <td>257674688.0</td>
      <td>238155184.0</td>
    </tr>
    <tr>
      <th>1598</th>
      <td>3.394727e+07</td>
      <td>40.0</td>
      <td>39.0</td>
      <td>78.0</td>
      <td>3.544488e+07</td>
      <td>False</td>
      <td>39</td>
      <td>38</td>
      <td>3.276252e+09</td>
      <td>3.088414e+09</td>
      <td>2.819504e+09</td>
      <td>3.895124e+08</td>
      <td>3.132752e+08</td>
      <td>8.100099e+07</td>
      <td>5.732754e+09</td>
      <td>331492544.0</td>
      <td>310216256.0</td>
    </tr>
    <tr>
      <th>1599</th>
      <td>3.725869e+07</td>
      <td>40.0</td>
      <td>40.0</td>
      <td>79.0</td>
      <td>2.650330e+07</td>
      <td>False</td>
      <td>39</td>
      <td>39</td>
      <td>3.276252e+09</td>
      <td>3.088414e+09</td>
      <td>2.819504e+09</td>
      <td>3.895124e+08</td>
      <td>3.132752e+08</td>
      <td>7.271181e+07</td>
      <td>6.280085e+08</td>
      <td>426433856.0</td>
      <td>404176352.0</td>
    </tr>
  </tbody>
</table>
<p>1600 rows × 17 columns</p>
</div></div></div>
</div>
</div>
<div class="section" id="rmse">
<h2>RMSE<a class="headerlink" href="#rmse" title="Permalink to this headline">¶</a></h2>
<p>We are going to do these calculations by using the <strong>scikit-learn</strong> benchmark tools. Since the LASSO model is the only one using the <strong>plus</strong> datasets, we will use that as the starting example</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Empty list to store results.</span>
<span class="n">scikit_results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">y_predicted_full_results</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">y_predicted_test_results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Actuals&quot;</span><span class="p">:</span> <span class="n">y_test</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># LASSO uses the &quot;plus&quot; datasets</span>
<span class="c1"># Train:</span>
<span class="n">y_predicted_train</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_plus</span><span class="p">)</span>
<span class="n">train_rmse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_predicted_train</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Test:</span>
<span class="n">y_predicted_test</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_plus</span><span class="p">)</span>
<span class="n">test_rmse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_predicted_test</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Append to the list of results.</span>
<span class="n">y_predicted_full_results</span><span class="p">[</span><span class="s2">&quot;LASSO&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_plus</span><span class="p">)</span>
<span class="n">y_predicted_test_results</span><span class="p">[</span><span class="s2">&quot;LASSO&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_predicted_test</span>

<span class="n">scikit_results</span> <span class="o">+=</span> <span class="p">[{</span>
    <span class="s2">&quot;Name&quot;</span><span class="p">:</span> <span class="s2">&quot;LASSO&quot;</span><span class="p">,</span> 
    <span class="s2">&quot;Train RMSE&quot;</span><span class="p">:</span> <span class="n">train_rmse</span><span class="p">,</span>
    <span class="s2">&quot;Test RMSE&quot;</span><span class="p">:</span> <span class="n">test_rmse</span>
<span class="p">}]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># LASSO (prev) also uses the &quot;plus&quot; datasets</span>
<span class="c1"># Train:</span>
<span class="n">y_predicted_train</span> <span class="o">=</span> <span class="n">lasso_prev</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_plus</span><span class="p">)</span>
<span class="n">train_rmse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_predicted_train</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Test:</span>
<span class="n">y_predicted_test</span> <span class="o">=</span> <span class="n">lasso_prev</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_plus</span><span class="p">)</span>
<span class="n">test_rmse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_predicted_test</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Append to the list of results.</span>
<span class="n">y_predicted_full_results</span><span class="p">[</span><span class="s2">&quot;LASSO (prev)&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso_prev</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_plus</span><span class="p">)</span>
<span class="n">y_predicted_test_results</span><span class="p">[</span><span class="s2">&quot;LASSO (prev)&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_predicted_test</span>

<span class="n">scikit_results</span> <span class="o">+=</span> <span class="p">[{</span>
    <span class="s2">&quot;Name&quot;</span><span class="p">:</span> <span class="s2">&quot;LASSO (prev)&quot;</span><span class="p">,</span> 
    <span class="s2">&quot;Train RMSE&quot;</span><span class="p">:</span> <span class="n">train_rmse</span><span class="p">,</span>
    <span class="s2">&quot;Test RMSE&quot;</span><span class="p">:</span> <span class="n">test_rmse</span>
<span class="p">}]</span>
</pre></div>
</div>
</div>
</div>
<p>and then for the rest we put it in a loop running on the <strong>“non-plussed”</strong> datasets:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Using zip in a for loop means pipe and name will run through </span>
<span class="c1"># the tuples of the two lists in that order.</span>
<span class="k">for</span> <span class="n">pipe</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
        <span class="p">[</span><span class="n">grid_tree</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">,</span> <span class="n">grid_forest</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">,</span> <span class="n">grid_gbm</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">,</span> 
         <span class="n">grid_hgbm</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">,</span> <span class="n">gbm_prev</span><span class="p">,</span> <span class="n">neural_network</span><span class="p">,</span> <span class="n">chain_ladder</span><span class="p">],</span>
        <span class="p">[</span><span class="s2">&quot;Decision Tree&quot;</span><span class="p">,</span> <span class="s2">&quot;Random Forest&quot;</span><span class="p">,</span> <span class="s2">&quot;Classic GBM&quot;</span><span class="p">,</span> <span class="s2">&quot;Histogram GBM&quot;</span><span class="p">,</span> <span class="s2">&quot;Hist GBM (prev)&quot;</span><span class="p">,</span> <span class="s2">&quot;Neural Network&quot;</span><span class="p">,</span> <span class="s2">&quot;Chain Ladder&quot;</span><span class="p">]</span>
    <span class="p">):</span>

    <span class="n">y_predicted_train</span> <span class="o">=</span> <span class="n">pipe</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">train_rmse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_predicted_train</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">y_predicted_test</span> <span class="o">=</span> <span class="n">pipe</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">test_rmse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_predicted_test</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">y_predicted_full_results</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">pipe</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_predicted_test_results</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_predicted_test</span>

    <span class="n">scikit_results</span> <span class="o">+=</span> <span class="p">[{</span>
    <span class="s2">&quot;Name&quot;</span><span class="p">:</span> <span class="n">name</span><span class="p">,</span> 
    <span class="s2">&quot;Train RMSE&quot;</span><span class="p">:</span> <span class="n">train_rmse</span><span class="p">,</span>
    <span class="s2">&quot;Test RMSE&quot;</span><span class="p">:</span> <span class="n">test_rmse</span>
    <span class="p">}]</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s have a look at these results for past and future data sets with results ranked by test RMSE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scikit_results</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;Test RMSE&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.float_format&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{0:,.0f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df_results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Name</th>
      <th>Train RMSE</th>
      <th>Test RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>LASSO</td>
      <td>48,283,020</td>
      <td>252,009,808</td>
    </tr>
    <tr>
      <th>1</th>
      <td>LASSO (prev)</td>
      <td>49,082,112</td>
      <td>269,974,336</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Hist GBM (prev)</td>
      <td>40,110,165</td>
      <td>313,377,932</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Histogram GBM</td>
      <td>34,464,022</td>
      <td>455,524,543</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Neural Network</td>
      <td>274,930,720</td>
      <td>930,978,112</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Classic GBM</td>
      <td>42,979,008</td>
      <td>1,603,949,161</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Chain Ladder</td>
      <td>138,528,934</td>
      <td>1,722,351,650</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Random Forest</td>
      <td>30,698,739</td>
      <td>1,789,873,775</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Decision Tree</td>
      <td>0</td>
      <td>1,916,509,816</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The key indicator is performance on a hold-out data set, in this case the future data. Otherwise, looking at Train RMSE, the decision tree would be perfect, having overfit the data to a loss of zero.</p>
<p>For the future data, we see a very different story. As expected, the tuned decision tree does appear to be over-fitted, performing the worst.</p>
<p>The LASSO model performs the best. The CV tuned model is pretty similar to the previous model from the R run.</p>
<p>The histogram GBM model follows. The “previously tuned” version has essentially identical performance to the XGBoost model from the R version of this article.</p>
<p>The simple neural network follows that.</p>
</div>
<div class="section" id="visualising-the-fit">
<h2>Visualising the fit<a class="headerlink" href="#visualising-the-fit" title="Permalink to this headline">¶</a></h2>
<p>Although useful, the RMSE is just a single number so it’s helpful to visualise the fit.
In particular, because these are models for a reserving data set, we can take advantage of that structure when analysing how well each model is performing.</p>
<div class="section" id="fitted-values">
<h3>Fitted values<a class="headerlink" href="#fitted-values" title="Permalink to this headline">¶</a></h3>
<p>First, lets show visualise the fitted payments.
The graphs below show the log(fitted values), or log(payments) in the case of the actual values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">y_pred</span> <span class="ow">in</span> <span class="n">y_predicted_test_results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="c1"># Skip prev models - these are similar enough to last version</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;LASSO (prev)&quot;</span><span class="p">,</span> <span class="s2">&quot;Hist GBM (prev)&quot;</span><span class="p">]:</span>  
        <span class="c1"># Plot</span>
        <span class="n">dat_</span> <span class="o">=</span> <span class="n">dat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> 
        <span class="n">dat_</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">dat_</span><span class="o">.</span><span class="n">train_ind</span> <span class="o">==</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;pmts&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred</span>
        
        <span class="n">plot_triangle</span><span class="p">(</span>
            <span class="n">dat_</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s2">&quot;acc&quot;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s2">&quot;pmts&quot;</span><span class="p">),</span>
            <span class="n">mask_bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">title</span><span class="o">=</span><span class="n">name</span>
        <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_130_0.png" src="../_images/MLRWP_Py_triangles_example_130_0.png" />
<img alt="../_images/MLRWP_Py_triangles_example_130_1.png" src="../_images/MLRWP_Py_triangles_example_130_1.png" />
<img alt="../_images/MLRWP_Py_triangles_example_130_2.png" src="../_images/MLRWP_Py_triangles_example_130_2.png" />
<img alt="../_images/MLRWP_Py_triangles_example_130_3.png" src="../_images/MLRWP_Py_triangles_example_130_3.png" />
<img alt="../_images/MLRWP_Py_triangles_example_130_4.png" src="../_images/MLRWP_Py_triangles_example_130_4.png" />
<img alt="../_images/MLRWP_Py_triangles_example_130_5.png" src="../_images/MLRWP_Py_triangles_example_130_5.png" />
<img alt="../_images/MLRWP_Py_triangles_example_130_6.png" src="../_images/MLRWP_Py_triangles_example_130_6.png" />
<img alt="../_images/MLRWP_Py_triangles_example_130_7.png" src="../_images/MLRWP_Py_triangles_example_130_7.png" />
</div>
</div>
<p>Some things are apparent from this:</p>
<ul class="simple">
<li><p>The lack of interactions or diagonal effects in the Chain ladder</p></li>
<li><p>All the machine learning models do detect and project the interaction</p></li>
<li><p>The blocky nature of the decision tree and the overfit to the past data</p></li>
<li><p>The random forest and GBMs fit look smoother since they are a function of a number of trees.</p></li>
<li><p>The LASSO and neural network fits are the smoothest, which is not surprising since it consists of continuous functions.</p></li>
<li><p>Visually, the LASSO seems to capture the interaction the best.</p></li>
</ul>
</div>
<div class="section" id="actual-vs-fitted-heat-maps">
<h3>Actual vs Fitted heat maps<a class="headerlink" href="#actual-vs-fitted-heat-maps" title="Permalink to this headline">¶</a></h3>
<p>We can also look at the model fits via heat maps of the actual/fitted values.</p>
<p>For triangular reserving data, these types of plots are very helpful when examining plot fit.</p>
<p>First, here’s a function to draw the heatmaps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">y_pred</span> <span class="ow">in</span> <span class="n">y_predicted_full_results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="c1"># Skip prev models - these are similar enough to last version</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;LASSO (prev)&quot;</span><span class="p">,</span> <span class="s2">&quot;Hist GBM (prev)&quot;</span><span class="p">]:</span> 

        <span class="c1"># Plot</span>
        <span class="n">dat_</span> <span class="o">=</span> <span class="n">dat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> 
        <span class="n">dat_</span><span class="p">[</span><span class="s2">&quot;pmts_ratio&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dat_</span><span class="o">.</span><span class="n">pmts</span> <span class="o">/</span> <span class="n">y_pred</span>
        
        <span class="n">cmap</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">diverging_palette</span><span class="p">(</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">sep</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">as_cmap</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># to be similar to R colouring</span>
        
        <span class="n">plot_triangle</span><span class="p">(</span>
            <span class="n">dat_</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s2">&quot;acc&quot;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s2">&quot;pmts_ratio&quot;</span><span class="p">),</span>
            <span class="n">mask_bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">title</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
            <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span>  <span class="c1"># vlag also not a bad palette for this</span>
        <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_133_0.png" src="../_images/MLRWP_Py_triangles_example_133_0.png" />
<img alt="../_images/MLRWP_Py_triangles_example_133_1.png" src="../_images/MLRWP_Py_triangles_example_133_1.png" />
<img alt="../_images/MLRWP_Py_triangles_example_133_2.png" src="../_images/MLRWP_Py_triangles_example_133_2.png" />
<img alt="../_images/MLRWP_Py_triangles_example_133_3.png" src="../_images/MLRWP_Py_triangles_example_133_3.png" />
<img alt="../_images/MLRWP_Py_triangles_example_133_4.png" src="../_images/MLRWP_Py_triangles_example_133_4.png" />
<img alt="../_images/MLRWP_Py_triangles_example_133_5.png" src="../_images/MLRWP_Py_triangles_example_133_5.png" />
<img alt="../_images/MLRWP_Py_triangles_example_133_6.png" src="../_images/MLRWP_Py_triangles_example_133_6.png" />
</div>
</div>
<p>All models have shortcomings, but the LASSO and previously tuned XGBoost models outperform the others.</p>
</div>
<div class="section" id="quarterly-tracking">
<h3>Quarterly tracking<a class="headerlink" href="#quarterly-tracking" title="Permalink to this headline">¶</a></h3>
<p>Finally, we will look at the predictions for specific parts of the data set.
Quarterly tracking graphs:</p>
<ul class="simple">
<li><p>Plot the actual and fitted payments, including future values, by one of accident / development / calendar period</p></li>
<li><p>Hold one of the other triangle directions fixed, meaning that the third direction is then determined.</p></li>
<li><p>Additionally plot the underlying mean from which the data were simulated.</p></li>
</ul>
<p>We’ll use a couple of functions because we want to do this repeatedly.</p>
<p>The first function (<code class="docutils literal notranslate"><span class="pre">GraphModelVals</span></code>) is from the blog on the LASSO model, and draws a single tracking graph.
The second function (<code class="docutils literal notranslate"><span class="pre">QTracking</span></code>) generates the graphs for all models and uses the <strong>patchwork</strong> package to wrap them into a single plot.</p>
<p>Let’s have a look at the tracking for accident quarter when development quarter = 5.
We’ll wrap this in another function since we want to call this for a few different combinations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;Name&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">scikit_results</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;LASSO (prev)&quot;</span><span class="p">,</span> <span class="s2">&quot;Hist GBM (prev)&quot;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">QTrack</span><span class="p">(</span><span class="n">model_forecasts</span><span class="p">,</span> <span class="n">model_names</span><span class="p">,</span> <span class="n">period_to_filter</span><span class="p">,</span> <span class="n">period</span><span class="p">,</span> <span class="n">total_num_periods</span><span class="p">,</span> <span class="n">plot_mu</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plots model vs payments and mu</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">    model_forecasts: Pandas DataFrame. Should have dev, acc, mu, pmts </span>
<span class="sd">    model_names: List of model names to plot Should be columns in model_forecasts</span>
<span class="sd">    period_to_filter: &quot;dev&quot; or &quot;acc&quot;. The period to filter by</span>
<span class="sd">    period: integer. filter value for period_to_filter. I.e. period_to_filter=&quot;acc&quot;, period=5 filters for acc=5.</span>
<span class="sd">    total_num_periods: total number of periods. First total_num_periods - period periods are greyed out.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model_names</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mf">1.8</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_names</span><span class="p">)))</span>
    
    <span class="n">other_period</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="s2">&quot;acc&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="n">period_to_filter</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">model_names</span><span class="p">,</span> <span class="n">axs</span><span class="p">):</span>
        <span class="c1"># Shade historical quarters</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">total_num_periods</span> <span class="o">-</span> <span class="n">period</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="n">plot_colors</span><span class="p">[</span><span class="s2">&quot;lgrey&quot;</span><span class="p">])</span>
        
        <span class="c1"># Plot mu</span>
        <span class="k">if</span> <span class="n">plot_mu</span><span class="p">:</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
            <span class="n">model_forecasts</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">model_forecasts</span><span class="p">[</span><span class="n">period_to_filter</span><span class="p">]</span> <span class="o">==</span> <span class="n">period</span><span class="p">,</span> <span class="n">other_period</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
            <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model_forecasts</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">model_forecasts</span><span class="p">[</span><span class="n">period_to_filter</span><span class="p">]</span> <span class="o">==</span> <span class="n">period</span><span class="p">,</span> <span class="s2">&quot;mu&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">),</span>
            <span class="n">color</span><span class="o">=</span><span class="n">plot_colors</span><span class="p">[</span><span class="s2">&quot;dgrey&quot;</span><span class="p">],</span>
            <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
        
        <span class="c1"># Plot Payments</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">model_forecasts</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">model_forecasts</span><span class="p">[</span><span class="n">period_to_filter</span><span class="p">]</span> <span class="o">==</span> <span class="n">period</span><span class="p">,</span> <span class="n">other_period</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> 
        <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model_forecasts</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">model_forecasts</span><span class="p">[</span><span class="n">period_to_filter</span><span class="p">]</span> <span class="o">==</span> <span class="n">period</span><span class="p">,</span> <span class="s2">&quot;pmts&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">),</span> 
        <span class="n">color</span><span class="o">=</span><span class="n">plot_colors</span><span class="p">[</span><span class="s2">&quot;dblue&quot;</span><span class="p">])</span>
        
        <span class="c1"># Plot model</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">model_forecasts</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">model_forecasts</span><span class="p">[</span><span class="n">period_to_filter</span><span class="p">]</span> <span class="o">==</span> <span class="n">period</span><span class="p">,</span> <span class="n">other_period</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> 
        <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model_forecasts</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">model_forecasts</span><span class="p">[</span><span class="n">period_to_filter</span><span class="p">]</span> <span class="o">==</span> <span class="n">period</span><span class="p">,</span> <span class="n">model_name</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">),</span> 
        <span class="n">color</span><span class="o">=</span><span class="n">plot_colors</span><span class="p">[</span><span class="s2">&quot;red&quot;</span><span class="p">],</span>
        <span class="n">linewidth</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
            
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Log(Payments)&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Modelled Values for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">period_to_filter</span> <span class="o">==</span> <span class="s2">&quot;acc&quot;</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Development Quarter&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Accident Quarter&#39;</span><span class="p">)</span>  
    
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">QTrack</span><span class="p">(</span><span class="n">model_forecasts</span><span class="p">,</span> <span class="n">model_names</span><span class="p">,</span> <span class="n">period_to_filter</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="n">period</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">total_num_periods</span><span class="o">=</span><span class="n">num_periods</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_138_0.png" src="../_images/MLRWP_Py_triangles_example_138_0.png" />
</div>
</div>
<p>Because this is mostly old data and because it is not affected by the interaction, the tracking should be generally good.</p>
<p>That being said, the predictions are a bit all over the place.</p>
<p>The decision tree, random forest and classic GBM seem underfit, as does the neural network. The histogram GBM is decent, slightly overfit. The Chain Ladder final quarter is a bit wild as can happen with a Chain Ladder. The LASSO model is a bit rough - especially if compared to the fit from the R implementation - but isn’t too bad.</p>
<p>Here are a few more tracking plots to help further illustrate the model fits.</p>
<p><strong>Tracking for accident quarter when development quarter = 24</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">QTrack</span><span class="p">(</span><span class="n">model_forecasts</span><span class="p">,</span> <span class="n">model_names</span><span class="p">,</span> <span class="n">period_to_filter</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="n">period</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">total_num_periods</span><span class="o">=</span><span class="n">num_periods</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_140_0.png" src="../_images/MLRWP_Py_triangles_example_140_0.png" />
</div>
</div>
<p>Development quarter 24 is impacted by the interactions for accident quarters&gt;17.</p>
<p>LASSO and the histogram GBM reflects this the best. The other tree models extrapolate a flat curve, whilst the neural network continues an upward trend. The Chain Ladder overpredicts the last value.</p>
<p><strong>Tracking for accident quarter when development quarter = 35</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">QTrack</span><span class="p">(</span><span class="n">model_forecasts</span><span class="p">,</span> <span class="n">model_names</span><span class="p">,</span> <span class="n">period_to_filter</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="n">period</span><span class="o">=</span><span class="mi">35</span><span class="p">,</span> <span class="n">total_num_periods</span><span class="o">=</span><span class="n">num_periods</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_143_0.png" src="../_images/MLRWP_Py_triangles_example_143_0.png" />
</div>
</div>
<p>This is quite hard for the models since most of the values are in the future. The neural network fits a smooth curve. The histogram GBM and the LASSO work well here.</p>
<p>We can look at similar plots by development quarter for older and newer accident quarters</p>
<p><strong>Tracking for development quarter when accident quarter = 5</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">QTrack</span><span class="p">(</span><span class="n">model_forecasts</span><span class="p">,</span> <span class="n">model_names</span><span class="p">,</span> <span class="n">period_to_filter</span><span class="o">=</span><span class="s2">&quot;acc&quot;</span><span class="p">,</span> <span class="n">period</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">total_num_periods</span><span class="o">=</span><span class="n">num_periods</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_147_0.png" src="../_images/MLRWP_Py_triangles_example_147_0.png" />
</div>
</div>
<p>The decision tree, random forest and classical GBM all project a flat line, but the others generally capture the downward trend.</p>
<p><strong>Tracking for development quarter when accident quarter = 20</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">QTrack</span><span class="p">(</span><span class="n">model_forecasts</span><span class="p">,</span> <span class="n">model_names</span><span class="p">,</span> <span class="n">period_to_filter</span><span class="o">=</span><span class="s2">&quot;acc&quot;</span><span class="p">,</span> <span class="n">period</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">total_num_periods</span><span class="o">=</span><span class="n">num_periods</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/MLRWP_Py_triangles_example_150_0.png" src="../_images/MLRWP_Py_triangles_example_150_0.png" />
</div>
</div>
<p>Again, the LASSO, the histogram GBM, and neural network are all able to capture the trend.</p>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="reserves">
<h1>Reserves<a class="headerlink" href="#reserves" title="Permalink to this headline">¶</a></h1>
<p>Finally, we’ll look at the reserve estimates from the different models:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_reserve_estimates</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">model_forecasts</span>
    <span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">model_forecasts</span><span class="o">.</span><span class="n">train_ind</span><span class="o">==</span><span class="kc">False</span><span class="p">]</span>
    <span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;acc&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;pmts&quot;</span><span class="p">,</span> <span class="s2">&quot;accf&quot;</span><span class="p">,</span> <span class="s2">&quot;devf&quot;</span><span class="p">,</span> <span class="s2">&quot;dev&quot;</span><span class="p">,</span> <span class="s2">&quot;cal&quot;</span><span class="p">,</span> <span class="s2">&quot;train_ind&quot;</span><span class="p">])</span>
<span class="p">)</span>

<span class="n">df_reserve_estimates</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0e11</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:xlabel=&#39;acc&#39;&gt;
</pre></div>
</div>
<img alt="../_images/MLRWP_Py_triangles_example_153_1.png" src="../_images/MLRWP_Py_triangles_example_153_1.png" />
</div>
</div>
<p>Finally, here are the overall reserves, summed over all accident quarters</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_reserve_summary</span><span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="n">name</span><span class="p">:</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="mi">10</span><span class="o">**</span><span class="mi">9</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">y_pred</span> <span class="ow">in</span> <span class="n">y_predicted_test_results</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span>
    <span class="n">orient</span><span class="o">=</span><span class="s2">&quot;index&quot;</span><span class="p">,</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Reserves ($B)&quot;</span><span class="p">]</span>
    <span class="p">)</span>

<span class="n">df_reserve_summary</span><span class="p">[</span><span class="s2">&quot;Ratio to actual(%)&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">df_reserve_summary</span><span class="p">[</span><span class="s2">&quot;Reserves ($B)&quot;</span><span class="p">]</span> <span class="o">/</span> 
    <span class="n">y_predicted_test_results</span><span class="p">[</span><span class="s2">&quot;Actuals&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span> <span class="mi">10</span><span class="o">**</span><span class="mi">11</span>
<span class="p">)</span>

<span class="n">df_reserve_summary</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Reserves ($B)</th>
      <th>Ratio to actual(%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Actuals</th>
      <td>608</td>
      <td>100</td>
    </tr>
    <tr>
      <th>LASSO</th>
      <td>596</td>
      <td>98</td>
    </tr>
    <tr>
      <th>LASSO (prev)</th>
      <td>579</td>
      <td>95</td>
    </tr>
    <tr>
      <th>Decision Tree</th>
      <td>1,678</td>
      <td>276</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>1,610</td>
      <td>265</td>
    </tr>
    <tr>
      <th>Classic GBM</th>
      <td>1,497</td>
      <td>246</td>
    </tr>
    <tr>
      <th>Histogram GBM</th>
      <td>683</td>
      <td>112</td>
    </tr>
    <tr>
      <th>Hist GBM (prev)</th>
      <td>704</td>
      <td>116</td>
    </tr>
    <tr>
      <th>Neural Network</th>
      <td>300</td>
      <td>49</td>
    </tr>
    <tr>
      <th>Chain Ladder</th>
      <td>563</td>
      <td>93</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The overall reserve for the Chain Ladder model hides the fact that this result is actually significant under-estimation in most accident quarters balanced by significant over-estimation in the last.</p>
<p>The neural network seems to be underfit overall, whilst the flat line projections by the decision tree, random forest and classic GBM are too conservative.</p>
<p>Taking this into account, the best performers on this basis are the LASSO followed by the histogram GBM.</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="commentary">
<h1>Commentary<a class="headerlink" href="#commentary" title="Permalink to this headline">¶</a></h1>
<p>What’s going on with the results? Can we improve matters?</p>
<p>Success in machine learning often comes down to optimising the following:</p>
<ul class="simple">
<li><p>What you model</p></li>
<li><p>How you model (i.e. what features you use)</p></li>
<li><p>Your performance measure</p></li>
<li><p>Your train/test split.</p></li>
</ul>
<div class="section" id="models">
<h2>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h2>
<p><strong>scikit-learn</strong> is incredibly convenient in that it gives access to so many different models. The Python syntax is also quite clean and easy to use compared to R.</p>
<p>But we ran into some limitations:</p>
<ul class="simple">
<li><p>The <strong>sci-kit</strong> GLM model didn’t have all the features we needed.</p></li>
<li><p>The classic GBM and Random Forest just performed poorly.</p></li>
<li><p>The histogram GBM held up well well. There are many more customisable functionality if we were to use XGBoost, LightGBM or CatBoost, but the performance was similar to the two XGBoost models from the R version.</p></li>
<li><p>Similarly the neural network is quite basic with the notable absence of the selection of activation functions. The target transform is a workaround but a full-featured package like <strong>tensorflow</strong> or <strong>pytorch</strong> may do better.</p></li>
</ul>
</div>
<div class="section" id="additional-features">
<h2>Additional Features<a class="headerlink" href="#additional-features" title="Permalink to this headline">¶</a></h2>
<p>More complex models such as GBMs and Neural Networks tend to perform better with bigger datasets. Our datasets in the example only include three factors - only two if you consider calendar quarter is just accident plus development. In real-world examples, you should be able to augment with additional rating factors or claims features to get a more predictive result.</p>
</div>
<div class="section" id="tuning-method-and-cross-validation">
<h2>Tuning Method and Cross Validation<a class="headerlink" href="#tuning-method-and-cross-validation" title="Permalink to this headline">¶</a></h2>
<p>As we noted earlier, there are a few components of randomness from one run to another, with different seeds:</p>
<ol class="simple">
<li><p>The cross validation folds will be different</p></li>
<li><p>The random search parameters will be different.</p></li>
</ol>
<p>For a small data set like this one, this has the potential to alter the results - afterall, getting a good result on the future data depends on whether the tuning gets lucky in terms of estimating the magnitude of the interaction.</p>
<p>We could reduce the variability by using a grid-search over a fixed set of parameters but - full disclosure - that led to even worse results.</p>
<div class="section" id="so-what-s-going-on">
<h3>So what’s going on?<a class="headerlink" href="#so-what-s-going-on" title="Permalink to this headline">¶</a></h3>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>It might be helpful to have another look at folds used in a cross-validation search.
</pre></div>
</div>
<p>These are shown below for six-fold CV - the yellow dots represent training data points in each fold, the blue dots are the test data.
The grey rectangle marks the interaction. These are actually the folds used in the R article; the folds used here will be different in specifics, but the same general ideas hold.</p>
<p><img alt="interaction compared to available triangle" src="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/f-07-mlr3example/index_files/figure-html/unnamed-chunk-65-1.png" /></p>
<p>The first thing that is apparent is that the interaction only applies to a very small number of points (10) in the past data. So in that sense, it’s quite remarkable that the models perform as well as they actually do - they all detect the interaction. Where they start to slip up is that they do not necessarily follow the development quarter shape in the tail.</p>
</div>
<div class="section" id="finer-fine-tuning">
<h3>Finer fine-tuning<a class="headerlink" href="#finer-fine-tuning" title="Permalink to this headline">¶</a></h3>
<p>It should be noted that to cut down the run time of this example, we only tuned over 25 sets of parameters for each model, kept models coarse via less trees, less rounds, less neurons, higher learn rates, and used five-fold cross-validation. Performance can possibly be improved by tuning over more sets of parameters, bigger models, and higher fold count CV.</p>
</div>
<div class="section" id="time-based-hold-out-testing">
<h3>Time-based hold-out testing<a class="headerlink" href="#time-based-hold-out-testing" title="Permalink to this headline">¶</a></h3>
<p>Reserving is a forecasting problem - we have a time series of payments and we want to forecast into the future.</p>
<p>This suggests that cross-validation may not be an ideal method for tuning reserving models - we would be better with a train/test split where the test data is in the future relative to the past data.</p>
<p>This problem has been discussed in the context of time series modelling, where the use of rolling window cross-validation techniques has been advocated. We’ll be discussing this in a future article.</p>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="conclusion">
<h1>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h1>
<p>We set out to demonstrate how to fit a number of reserving models to a data set.
Our results have been mixed - but we expected that in advance.
This work lays a baseline for our upcoming articles where we will look at ways of improving the results through:</p>
<ul class="simple">
<li><p>expanding the range of models</p></li>
<li><p>considering more appropriate validation techniques.</p></li>
</ul>
<div class="section" id="what-next">
<h2>What next?<a class="headerlink" href="#what-next" title="Permalink to this headline">¶</a></h2>
<p>You can try running this code and changing some things - e.g. the hyper-parameters tuned, search strategy, number of iterations etc.</p>
<p>For those looking to use <strong>scikit-learn</strong>, it is worthwhile to read about some other limitations of the models <a class="reference external" href="https://ryxcommar.com/2019/08/30/scikit-learns-defaults-are-wrong/">here</a>.</p>
<p>You can also try other data sets as described in the <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3241906">LASSO paper</a>  by changing <code class="docutils literal notranslate"><span class="pre">dataset_number</span></code> near the start of the code. More details are in the paper, but in brief:</p>
<ul class="simple">
<li><p>Data set 1 = a Chain Ladder model</p></li>
<li><p>Data set 2 = a Chain Ladder model + calendar period effects</p></li>
<li><p>Data set 3 = the data set used here</p></li>
<li><p>Data set 4 = data set 2 but where the strength of the calendar period effect varies by development period.</p></li>
</ul>
<p>There will be more Python content coming out on the blog, so keep an eye out for further <strong>developments</strong>!</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "ActuariesInstitute/cookbook",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="MLRWP_R_mlr3.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">R: Machine Learning Triangles</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="DAA_M06_CS1.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Py: Socio-Economic Index Construction</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By YDAWG, DAPC, YAC and other contributors.<br/>
    
      <div class="extra_footer">
         The Actuaries' Analytical Cookbook is a series of data and analytics recipes to help actuaries quickly get started with a new project.   This site is intended to be a resource to actuaries in both data science and traditional fields.  Opinions expressed in this publication are the opinions of contributors and do not necessarily represent those of either the Institute of Actuaries of Australia (the ‘Institute’), its members, directors, officers, employees, agents, or that of the employers of the contributors. <br>© Institute of Actuaries of Australia and Contributors 2021. All rights reserved.
      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>