

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Py: Investigating Neural Network Biases with ChatGPT &#8212; Actuaries&#39; Analytical Cookbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script data-website-id="f2934c40-e673-4ab5-9658-4b594221551e" defer="defer" src="https://analytics.umami.is/script.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/NN_bias_w_ChatGPT';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="R: Life Modelling Recipes" href="LifeRecipeBook.html" />
    <link rel="prev" title="Py: Explainable Models with SHAP" href="py_shap_values.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/actuaries-logo.svg" class="logo__image only-light" alt="Actuaries' Analytical Cookbook - Home"/>
    <script>document.write(`<img src="../_static/actuaries-logo.svg" class="logo__image only-dark" alt="Actuaries' Analytical Cookbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to Python</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="about_py.html">About Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Setting up Your Python Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_by_example.html">An Introductory Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="learn_more.html">Learn More</a></li>
<li class="toctree-l1"><a class="reference internal" href="Useful_Python_packages.html">Useful Python packages for Data Science</a></li>
<li class="toctree-l1"><a class="reference internal" href="Performance_gottagofaster.html">Py: Python for Performance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to R</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="about_R.html">About R</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started_R.html">Setting Up R</a></li>
<li class="toctree-l1"><a class="reference internal" href="introductory_R.html">Introduction to R</a></li>
<li class="toctree-l1"><a class="reference internal" href="intermediate_R.html">Next Steps With R</a></li>
<li class="toctree-l1"><a class="reference internal" href="top_ten_r_packages.html">Useful Packages</a></li>





<li class="toctree-l1"><a class="reference internal" href="MLRWP_R_DataTable.html">R: data.table for actuaries</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Workflow Management</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="version_control.html">Version control</a></li>
<li class="toctree-l1"><a class="reference internal" href="predict_API_gradio.html">Py: Productionising predictive models as API</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Regression, Classification and Technical Price</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DAA_M05_CS1.html">Py: Customer Churn Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="multitasking_risk_pricing.html">Py/R: Multitasking Risk Pricing Using Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="py_shap_values.html">Py: Explainable Models with SHAP</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Py: Investigating Neural Network Biases with ChatGPT</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Life Insurance</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="LifeRecipeBook.html">R: Life Modelling Recipes</a></li>






<li class="toctree-l1"><a class="reference internal" href="life_stats.html">R: Life Industry Stats in Tableau and R</a></li>




<li class="toctree-l1"><a class="reference internal" href="bayesian-applications.html">R: Bayesian Applications</a></li>




<li class="toctree-l1"><a class="reference internal" href="trees.html">R: Decision Tree Applications</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">General Insurance</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="SQL%20Query%20for%20Triangles.html">SQL: Queries to Create Triangles</a></li>
<li class="toctree-l1"><a class="reference internal" href="MLRWP_R_GLMs.html">R: Reserving with GLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="MLRWP_R_Lasso.html">R: Reserving with LASSO</a></li>





<li class="toctree-l1"><a class="reference internal" href="MLRWP_R_mlr3.html">R: Machine Learning Triangles</a></li>











<li class="toctree-l1"><a class="reference internal" href="MLRWP_Py_triangles_example.html">Py: Machine Learning Triangles</a></li>










<li class="toctree-l1"><a class="reference internal" href="MLRWP_R_Baudry_Notebook_1_SimulateData_v1.html">R: Baudry ML Reserving Pt 1</a></li>






<li class="toctree-l1"><a class="reference internal" href="MLRWP_R_Baudry_Notebook_2_CreateReservingDatabase_v1.html">R: Baudry ML Reserving Pt 2</a></li>








<li class="toctree-l1"><a class="reference internal" href="MLRWP_R_Baudry_Notebook_3_ApplyMachineLearningReserving_v1.html">R: Baudry ML Reserving Pt 3</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unsupervised Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="SEIFA_IER_index_2021_replicate.html">Py: Socio-Economic Index Construction</a></li>
<li class="toctree-l1"><a class="reference internal" href="DAA_M06_CS2.html">Py: Clustering Credit Card Fraud</a></li>
<li class="toctree-l1"><a class="reference internal" href="DAA_M06_Ex4.html">Py: K-means clustering of COVID dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="DAA_M06_Ex5.html">Py: Hierarchical clustering on COVID dataset</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Natural Language Processing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="R_case_study_word_cloud.html">R: Word Cloud Case Study</a></li>
<li class="toctree-l1"><a class="reference internal" href="textClassificationEntry.html">Py: Text Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="DAA_M05_Ex10.html">Py: Decision Tree Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="DAA_M05_Ex18.html">Py: Neural Net Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="DAA_M07_CS1.html">Py: Classifying review sentiment</a></li>
<li class="toctree-l1"><a class="reference internal" href="DAA_M07_CS2.html">Py: Customer Sentiment Analysis</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Business Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DAA_2021_S2_Tutorial10_exercise_scipy.html">Py: Linear Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Image Recognition</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DAA_M05_CS2.html">Py: Image Recognition</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Ethics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="automated_decision.html">Automated Decision-Making Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="zbibliography.html">Bibliography</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Contributing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/ActuariesInstitute/cookbook/blob/main/cookbook/docs/NN_bias_w_ChatGPT.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ActuariesInstitute/cookbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ActuariesInstitute/cookbook/edit/main/cookbook/docs/NN_bias_w_ChatGPT.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/docs/NN_bias_w_ChatGPT.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Py: Investigating Neural Network Biases with ChatGPT</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chatgpt-prompt">ChatGPT prompt:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#response">Response:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-on-bias">Experiment on Bias</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-the-bias-issue">Solving the bias issue</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learn-rates">Learn Rates</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#just-change-the-bias">Just change the bias</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusions">Conclusions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-unedited-chatgpt-code-output">Appendix: Unedited ChatGPT code output</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="py-investigating-neural-network-biases-with-chatgpt">
<h1>Py: Investigating Neural Network Biases with ChatGPT<a class="headerlink" href="#py-investigating-neural-network-biases-with-chatgpt" title="Permalink to this heading">#</a></h1>
<p><strong>But not in the way you think…</strong></p>
<p><em>An Analytics Snippet By Jacky Poon.</em></p>
<p>In this notebook we will use the assistance of ChatGPT to use neural network models with early stopping to predict claims data and see if it is biased.</p>
<p>Here, bias is referring to the model making predictions that are systematically higher or lower. This is important because in many practical actuarial applications, whilst it is helpful to be able to predict which are the high risk segments and which are the low risk segments, having the whole portfolio systematically underpriced or overpriced will lead to unfortunate outcomes.</p>
<p>We will see if we can replicate similar findings to <a class="reference external" href="https://deliverypdf.ssrn.com/delivery.php?ID=448116089123102082100070125118102081056087054032028010066007099074002097073008122030002122118022114055124103026123081099112073014010033010014103098079094065083077001060023103066064099006069008009080006124020127072025027006000127097096066026103089113&amp;EXT=pdf&amp;INDEX=TRUE">Wuthrich 2019</a>. In the paper Wuthrich describes how use of the early stopping technique to limit over-fitting introduced biases into estimates of the mean, and considers the use of regularisation techniques to prevent this.</p>
<p>Early stopping is a common method in machine learning to prevent overfitting. You select a validation dataset and then during the model training process, if we observe at some point that further training is not improving performance, we stop.</p>
<p>Our investigation follows <a class="reference external" href="https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/research/chain_ladder_to_individual_mdn/">my earlier notebook</a>. In the article we described how to make incremental changes to a chain ladder to progress to a probabilistic neural network. We also shared several techniques that help the model training process, with faster convergence and better stability. In that notebook, early stopping was not used, so estimates aligned well to the mean within the training data. Instead, regularisation was used to try to prevent overfitting. However, regularisation requires hyperparameter searches to find the value that finds the right balance between under- and over-fitting, whereas early stopping would seem to automatically find the right balance.</p>
<p>So it would be interesting to test to see if the Wuthrich findings of biases apply to our approach as well.</p>
<section id="chatgpt-prompt">
<h2>ChatGPT prompt:<a class="headerlink" href="#chatgpt-prompt" title="Permalink to this heading">#</a></h2>
<p>We used the following prompt to start our code.</p>
<blockquote>
<div><p>Write a python script that fits a feedforward neural network with one hidden layer on a <code class="docutils literal notranslate"><span class="pre">pandas</span></code> tabular dataset.
Use <code class="docutils literal notranslate"><span class="pre">pytorch</span></code>, with early stopping.
The response column is <code class="docutils literal notranslate"><span class="pre">claims</span></code>.
There is an exposure weight column <code class="docutils literal notranslate"><span class="pre">expo</span></code>.
Numerical features are <code class="docutils literal notranslate"><span class="pre">age</span></code>, <code class="docutils literal notranslate"><span class="pre">ac</span></code>, <code class="docutils literal notranslate"><span class="pre">power</span></code>, <code class="docutils literal notranslate"><span class="pre">dens</span></code>
Categorical features are <code class="docutils literal notranslate"><span class="pre">gas</span></code>, <code class="docutils literal notranslate"><span class="pre">brand</span></code>, <code class="docutils literal notranslate"><span class="pre">area</span></code>, and <code class="docutils literal notranslate"><span class="pre">ct</span></code>.
Explain the code.</p>
</div></blockquote>
<p>This was run on ChatGPT’s free version in mid May 2023. You may get different output as the large language model does not appear to be deterministic - different runs will produce different code, sometimes a different style, and often different bugs or functionality.</p>
<p>We will include ChatGPT’s code descriptions, which were originally at the end, in-line with the code in <em>italics</em> to make it easier to follow, and annotate any adjustments we made.</p>
</section>
<section id="response">
<h2>Response:<a class="headerlink" href="#response" title="Permalink to this heading">#</a></h2>
<p><em>Certainly! Below is a Python script that fits a feedforward neural network with embeddings on categorical features and one hidden layer using PyTorch on a Pandas tabular dataset:</em></p>
<p><em>Import the necessary libraries, including torch for deep learning operations, numpy for numerical computations, and pandas for data manipulation.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span><span class="p">,</span> <span class="n">MinMaxScaler</span>  <span class="c1"># We added MinMaxScaler for later</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>  <span class="c1"># Added this for extra diagnostics. </span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_models</span> <span class="o">=</span> <span class="mi">50</span>
</pre></div>
</div>
</div>
</div>
<p><em>Define a class called <code class="docutils literal notranslate"><span class="pre">FeedForwardNet</span></code>, which represents the feedforward neural network model. It inherits from the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class, the base class for all neural network modules in PyTorch.</em></p>
<p><em>Inside the FeedForwardNet class, define the neural network architecture in the <strong>init</strong> method. It consists of an embedding layer for each categorical feature, followed by a hidden layer and an output layer.</em></p>
<p><em>Implement the forward pass of the neural network in the forward method. The forward pass involves embedding the categorical features, concatenating them with the numerical features, and passing the concatenated tensor through the hidden and output layers.</em></p>
<p>The prompt does a good job and produces what is asked for. There is an undocumented dropout layer with 20% dropout which was not part of the ask, which we will take out to simplify the logic.</p>
<p>In Wuthrich’s paper, a multi-layer network is used with (20, 15, 10) hidden neurons, in our prompt we have only asked for a single hidden layer. We can change this to take a list of hidden sizes by asking:</p>
<blockquote>
<div><p>Modify this code to take in a list of hidden layer sizes for a multi-layer model:
and including the code for the <code class="docutils literal notranslate"><span class="pre">FeedForwardNet</span></code> class.</p>
</div></blockquote>
<p>We also make some amendments based on our earlier work.</p>
<p>In our prompt, we neglected to mention we wanted to add an exponential transform at the end to have non-negative estimates analogous to a log-link GLM, so we amend the code to add <code class="docutils literal notranslate"><span class="pre">torch.exp</span></code> at end.</p>
<p>Additionally, we initialise as follows to improve model training:</p>
<ul class="simple">
<li><p>Final layer weights to zero</p></li>
<li><p>Final layer bias to an additional parameter <code class="docutils literal notranslate"><span class="pre">init_bias</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the neural network class</span>
<span class="k">class</span> <span class="nc">FeedForwardNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_numerical_feats</span><span class="p">,</span> <span class="n">num_categorical_feats</span><span class="p">,</span> <span class="n">embedding_sizes</span><span class="p">,</span> <span class="n">hidden_sizes</span><span class="p">,</span> <span class="n">init_bias</span><span class="p">):</span>  <span class="c1"># was hidden_size originally</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FeedForwardNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">emb_size</span> <span class="ow">in</span> <span class="n">embedding_sizes</span>
        <span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_numerical_feats</span> <span class="o">=</span> <span class="n">num_numerical_feats</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_categorical_feats</span> <span class="o">=</span> <span class="n">num_categorical_feats</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_embed_size</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">emb_size</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">emb_size</span> <span class="ow">in</span> <span class="n">embedding_sizes</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_numerical_feats</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_embed_size</span>

        <span class="c1"># self.fc1 = nn.Linear(self.input_size, hidden_size)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">hidden_sizes</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">hidden_size</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="c1"># self.fc2 = nn.Linear(hidden_size, 1)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>                          <span class="c1"># Initialise to zero</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">init_bias</span><span class="p">)</span>
        <span class="c1"># self.dropout = nn.Dropout(p=0.2)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_numerical</span><span class="p">,</span> <span class="n">x_categorical</span><span class="p">):</span>
        <span class="n">embedded_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">embedding</span><span class="p">(</span><span class="n">x_categorical</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">)]</span>
        <span class="n">embedded_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">embedded_x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">embedded_x</span><span class="p">,</span> <span class="n">x_numerical</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># x = self.dropout(x)</span>
        <span class="c1"># x = torch.relu(self.fc1(x))</span>

        <span class="k">for</span> <span class="n">hidden_layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">hidden_layer</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1"># x = self.dropout(x)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>      <span class="c1">#Exp output</span>
</pre></div>
</div>
</div>
</div>
<p><em>Load the dataset using <code class="docutils literal notranslate"><span class="pre">pd.read_csv()</span></code> and split it into training and validation sets using <code class="docutils literal notranslate"><span class="pre">train_test_split()</span></code> from <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>.</em></p>
<p>We did not give the dataset a file, so it was naming it by a placeholder <code class="docutils literal notranslate"><span class="pre">'your_dataset.csv'</span></code>. We are using the data from <a class="reference external" href="https://deliverypdf.ssrn.com/delivery.php?ID=843106086008086077095022066099066010017051024001008020124118114123013023071031125091049101001060018038058118064065031070081123109034011046042121019015100116121004041055094020008081109098014124081007088083009088118030071102107101005121114105115096099&amp;EXT=pdf&amp;INDEX=TRUE">Wu ̈thrich–Buser</a>, the link is <a class="reference external" href="https://people.math.ethz.ch/~wmario/Lecture/MTPL_data.csv">here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;MTPL_data.csv&quot;</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">&quot;;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">)</span>

<span class="c1"># Split the dataset into training and validation sets</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The dataset looks like this by the way:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>claims</th>
      <th>expo</th>
      <th>age</th>
      <th>ac</th>
      <th>power</th>
      <th>gas</th>
      <th>brand</th>
      <th>area</th>
      <th>dens</th>
      <th>ct</th>
      <th>truefreq</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0.33</td>
      <td>66</td>
      <td>4</td>
      <td>3</td>
      <td>Regular</td>
      <td>B12</td>
      <td>B</td>
      <td>83</td>
      <td>BE</td>
      <td>0.059941</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0.08</td>
      <td>31</td>
      <td>1</td>
      <td>7</td>
      <td>Diesel</td>
      <td>B1</td>
      <td>A</td>
      <td>34</td>
      <td>BL</td>
      <td>0.119216</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0.92</td>
      <td>60</td>
      <td>6</td>
      <td>5</td>
      <td>Diesel</td>
      <td>B1</td>
      <td>C</td>
      <td>223</td>
      <td>AG</td>
      <td>0.074344</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>1.00</td>
      <td>66</td>
      <td>4</td>
      <td>2</td>
      <td>Diesel</td>
      <td>B1</td>
      <td>C</td>
      <td>283</td>
      <td>FR</td>
      <td>0.092829</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0.63</td>
      <td>63</td>
      <td>3</td>
      <td>5</td>
      <td>Regular</td>
      <td>B12</td>
      <td>B</td>
      <td>74</td>
      <td>VS</td>
      <td>0.049981</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>499996</th>
      <td>0</td>
      <td>0.15</td>
      <td>32</td>
      <td>3</td>
      <td>2</td>
      <td>Diesel</td>
      <td>B1</td>
      <td>C</td>
      <td>204</td>
      <td>FR</td>
      <td>0.100843</td>
    </tr>
    <tr>
      <th>499997</th>
      <td>0</td>
      <td>0.86</td>
      <td>40</td>
      <td>2</td>
      <td>2</td>
      <td>Regular</td>
      <td>B12</td>
      <td>D</td>
      <td>617</td>
      <td>FR</td>
      <td>0.167433</td>
    </tr>
    <tr>
      <th>499998</th>
      <td>0</td>
      <td>0.74</td>
      <td>46</td>
      <td>7</td>
      <td>4</td>
      <td>Regular</td>
      <td>B1</td>
      <td>C</td>
      <td>238</td>
      <td>VD</td>
      <td>0.084457</td>
    </tr>
    <tr>
      <th>499999</th>
      <td>0</td>
      <td>1.00</td>
      <td>72</td>
      <td>5</td>
      <td>3</td>
      <td>Diesel</td>
      <td>B2</td>
      <td>B</td>
      <td>94</td>
      <td>VS</td>
      <td>0.070113</td>
    </tr>
    <tr>
      <th>500000</th>
      <td>0</td>
      <td>0.64</td>
      <td>31</td>
      <td>1</td>
      <td>1</td>
      <td>Diesel</td>
      <td>B12</td>
      <td>C</td>
      <td>323</td>
      <td>BL</td>
      <td>0.165031</td>
    </tr>
  </tbody>
</table>
<p>500000 rows × 11 columns</p>
</div></div></div>
</div>
<p><em>Define the categorical features and numerical features based on the dataset columns.</em></p>
<p><em>Encode the categorical features using <code class="docutils literal notranslate"><span class="pre">LabelEncoder()</span></code> from scikit-learn. This is necessary to convert the categorical features into numerical representations suitable for training the neural network.</em></p>
<p>ChatGPT lists the features as instructed, but we discover after the fact that Wuthrich has area as continuous, so we’ll adjust the code accordingly.</p>
<p>We’ll create the label encoders based on the full dataset instead of the training dataset only, to avoid errors from missing levels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the categorical features and numerical features</span>
<span class="n">categorical_feats</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;gas&#39;</span><span class="p">,</span> <span class="s1">&#39;brand&#39;</span><span class="p">,</span> <span class="s1">&#39;area&#39;</span><span class="p">,</span> <span class="s1">&#39;ct&#39;</span><span class="p">]</span>
<span class="n">numerical_feats</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;ac&#39;</span><span class="p">,</span> <span class="s1">&#39;power&#39;</span><span class="p">,</span> <span class="s1">&#39;dens&#39;</span><span class="p">]</span>

<span class="c1"># Encode the categorical features using LabelEncoder</span>
<span class="n">label_encoders</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">categorical_feats</span><span class="p">:</span>
    <span class="n">label_encoders</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
    <span class="n">label_encoders</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">feat</span><span class="p">])</span>
    <span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoders</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">])</span>
    <span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoders</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>ChatGPT only label encodes the categorical variables, but we should standardise our numerical inputs:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Standardize the categorical features using LabelEncoder</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">numerical_feats</span><span class="p">:</span>
    <span class="n">scaler</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
    <span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p><em>Define the embedding sizes for each categorical feature. These sizes determine the dimensions of the embedding vectors for each categorical feature. Usually, the embedding size is chosen to be smaller than the number of unique values in the feature but larger than half of it.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the embedding sizes for categorical features</span>
<span class="n">embedding_sizes</span> <span class="o">=</span> <span class="p">[(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()),</span> <span class="nb">min</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span> <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">categorical_feats</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p><em>Convert the dataset columns into PyTorch tensors, which are required for training the neural network model.</em></p>
<p>We neglected to mention this in the prompt, but we actually want to divide claims by exposure here to get y.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert the dataset to PyTorch tensors</span>
<span class="n">x_train_numerical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">numerical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x_train_categorical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">categorical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="c1"># y_train = torch.tensor(train_data[&#39;claims&#39;].values, dtype=torch.float32)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;claims&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">/</span> <span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">exposure_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">x_val_numerical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">numerical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x_val_categorical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">categorical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="c1"># y_val = torch.tensor(val_data[&#39;claims&#39;].values, dtype=torch.float32)</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="s1">&#39;claims&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">/</span> <span class="n">val_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">exposure_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We did not mention it in the prompt, but let’s calculate our mean so that we can see that it is aligning to that value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">avg_claim</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;claims&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">avg_claim</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.10265948983820225
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">y_train</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.1027)
</pre></div>
</div>
</div>
</div>
<p><em>Define the hyperparameters such as the hidden layer size, learning rate, batch size, number of epochs, and early stopping epochs.</em></p>
<p>The <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is generated, but it does not look like it is actually used in the later generated code. We prefer using the whole dataset, so this is fine.</p>
<p>We make a few adjustments to hyperparams.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the hyperparameters</span>
<span class="c1"># hidden_size = 64</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>  <span class="c1"># Replace with the multi-layer parameters.</span>

<span class="c1"># learning_rate = 0.001</span>
<span class="c1"># batch_size = 32</span>
<span class="c1"># num_epochs = 100</span>
<span class="c1"># early_stopping_epochs = 10</span>

<span class="c1"># Overwrite those hyperparameters with these</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">9999</span>  <span class="c1"># should not be a factor, we train until early stopping kicks in</span>
<span class="n">early_stopping_epochs</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
</div>
</div>
<p><em>Create an instance of the FeedForwardNet model.</em></p>
<p>We pass on the <code class="docutils literal notranslate"><span class="pre">init_bias</span></code> parameter here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create an instance of the FeedForwardNet model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FeedForwardNet</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">numerical_feats</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">categorical_feats</span><span class="p">),</span> <span class="n">embedding_sizes</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">init_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">avg_claim</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="c1"># Test that the init_bias works, what is the initial mean?</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train_numerical</span><span class="p">,</span> <span class="n">x_train_categorical</span><span class="p">)</span>

<span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.1027, grad_fn=&lt;DivBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p><em>Define the loss function (mean squared error) and the optimizer (Adam optimizer) to train the model.</em></p>
<p>We would like to do the exposure weights a little differently. We will have reduction=’none’ and keep the loss output as individual loss values per row instead of the sum. This allows us to multiply the loss per record by the exposure weight per record later.</p>
<p>We’ll also swap to Poisson Loss.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the loss function</span>
<span class="c1"># criterion = nn.MSELoss(reduction=&#39;sum&#39;)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PoissonNLLLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">log_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Define the optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Train the model for the specified number of epochs. In each epoch, perform forward and backward passes, update the model’s parameters, and calculate the validation loss. If the validation loss is the lowest so far, save the model’s state.</em></p>
<p>We add some debug output for diagnostics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train the model</span>
<span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
<span class="n">best_epoch</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train_numerical</span><span class="p">,</span> <span class="n">x_train_categorical</span><span class="p">)</span>
    <span class="c1"># loss = criterion(y_pred.squeeze() * exposure_train, y_train * exposure_train)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_val_numerical</span><span class="p">,</span> <span class="n">x_val_categorical</span><span class="p">)</span>
        <span class="c1"># val_loss = criterion(y_val_pred.squeeze() * exposure_val, y_val * exposure_val)</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">y_val_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">y_val</span><span class="p">)</span> <span class="o">*</span> <span class="n">exposure_val</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_val</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="n">val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>
        <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">val_loss</span>
        <span class="n">best_epoch</span> <span class="o">=</span> <span class="n">epoch</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;best_model.pt&#39;</span><span class="p">)</span>
    
    <span class="c1"># Added for diagnostics</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="s2">&quot;Epoch:&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span>
            <span class="s2">&quot;Train_Diff:&quot;</span><span class="p">,</span> <span class="p">((</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_train</span> <span class="o">-</span> <span class="n">y_train</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>            
            <span class="s2">&quot;Train_Mean:&quot;</span><span class="p">,</span> <span class="p">((</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> 
            <span class="s2">&quot;Train_Loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="s2">&quot;Val_mean&quot;</span><span class="p">,</span> <span class="p">((</span><span class="n">y_val_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_val</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_val</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> 
            <span class="s2">&quot;Val_loss&quot;</span><span class="p">,</span> <span class="n">val_loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">-</span> <span class="n">best_epoch</span> <span class="o">&gt;=</span> <span class="n">early_stopping_epochs</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Stopping at:&quot;</span><span class="p">,</span> <span class="n">best_epoch</span><span class="p">)</span>      
        <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 0 Train_Diff: 3.9570383081333205e-10 Train_Mean: 0.10265947878360748 Train_Loss 0.33634716272354126 Val_mean 0.10276821255683899 Val_loss 0.336696594953537
Epoch: 10 Train_Diff: 0.0005192473763599992 Train_Mean: 0.10317873954772949 Train_Loss 0.3340966999530792 Val_mean 0.10280803591012955 Val_loss 0.3345138132572174
Epoch: 20 Train_Diff: 0.000555766629986465 Train_Mean: 0.10321525484323502 Train_Loss 0.3329228162765503 Val_mean 0.10316438227891922 Val_loss 0.3333604633808136
Epoch: 30 Train_Diff: 0.00022463459754362702 Train_Mean: 0.10288412868976593 Train_Loss 0.33167240023612976 Val_mean 0.10189294815063477 Val_loss 0.33199846744537354
Epoch: 40 Train_Diff: 0.00515005411580205 Train_Mean: 0.10780954360961914 Train_Loss 0.33076417446136475 Val_mean 0.09964833408594131 Val_loss 0.33111655712127686
Epoch: 50 Train_Diff: -0.0002511663769837469 Train_Mean: 0.10240831226110458 Train_Loss 0.3297358453273773 Val_mean 0.10654579848051071 Val_loss 0.33056363463401794
Epoch: 60 Train_Diff: 0.002754278015345335 Train_Mean: 0.1054137572646141 Train_Loss 0.32892730832099915 Val_mean 0.09867089241743088 Val_loss 0.32982489466667175
Epoch: 70 Train_Diff: 3.5053705005339e-07 Train_Mean: 0.10265983641147614 Train_Loss 0.32785457372665405 Val_mean 0.09791050851345062 Val_loss 0.32911568880081177
Epoch: 80 Train_Diff: 0.005469439085572958 Train_Mean: 0.10812892764806747 Train_Loss 0.32726752758026123 Val_mean 0.09609229862689972 Val_loss 0.3288998305797577
Epoch: 90 Train_Diff: 0.0024685768876224756 Train_Mean: 0.10512805730104446 Train_Loss 0.3264632225036621 Val_mean 0.09715571999549866 Val_loss 0.3284188508987427
Epoch: 100 Train_Diff: -0.002204097807407379 Train_Mean: 0.1004553884267807 Train_Loss 0.3260534703731537 Val_mean 0.10135945677757263 Val_loss 0.32796788215637207
Epoch: 110 Train_Diff: -0.004026252310723066 Train_Mean: 0.09863324463367462 Train_Loss 0.32590925693511963 Val_mean 0.10565565526485443 Val_loss 0.3279320001602173
Epoch: 120 Train_Diff: 0.003851202316582203 Train_Mean: 0.1065106987953186 Train_Loss 0.3255639374256134 Val_mean 0.0999462902545929 Val_loss 0.32771018147468567
Epoch: 130 Train_Diff: -0.0011717552551999688 Train_Mean: 0.10148773342370987 Train_Loss 0.32516470551490784 Val_mean 0.10182797163724899 Val_loss 0.3276842534542084
Stopping at: 121
</pre></div>
</div>
</div>
</div>
<p><em>Load the best model’s state from the saved file.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the best model</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;best_model.pt&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;All keys matched successfully&gt;
</pre></div>
</div>
</div>
</div>
<p><em>Prepare the test set data and make predictions on it using the best model.</em></p>
<p>ChatGPT does not actually prepare the test set data, so we need to code this up ourselves. For this test we’ll just use our original full dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">categorical_feats</span><span class="p">:</span>
    <span class="n">test_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoders</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="n">feat</span><span class="p">])</span>

<span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">numerical_feats</span><span class="p">:</span>
    <span class="n">test_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p><em>Calculate the test loss (mean squared error) and the root mean squared error (RMSE) for the test set.</em></p>
<p><em>Print the test RMSE.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make predictions on the test set</span>
<span class="n">x_test_numerical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="n">numerical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x_test_categorical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="n">categorical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="s1">&#39;claims&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">exposure_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_test_numerical</span><span class="p">,</span> <span class="n">x_test_categorical</span><span class="p">)</span>

<span class="c1"># Calculate the test loss</span>
<span class="c1"># test_loss = criterion(y_test_pred.squeeze() * exposure_test, y_test * exposure_test)</span>
<span class="n">test_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">y_test_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">*</span> <span class="n">exposure_test</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_test</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">test_rmse</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">test_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Test RMSE: </span><span class="si">{</span><span class="n">test_rmse</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test RMSE: 0.0007
</pre></div>
</div>
</div>
</div>
<p><em>This code fits a feedforward neural network with embeddings on categorical features and one hidden layer using PyTorch. It uses early stopping to prevent overfitting and achieves predictions for the response column “claims” in a tabular dataset.</em></p>
<p>Let us run some diagnostics on this model, on the validation data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_val_numerical</span><span class="p">,</span> <span class="n">x_val_categorical</span><span class="p">)</span>

<span class="n">val_data</span><span class="p">[</span><span class="s2">&quot;predicted&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_val_pred</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">*</span> <span class="n">val_data</span><span class="o">.</span><span class="n">expo</span>
<span class="n">val_data</span><span class="p">[</span><span class="s2">&quot;trueclaims&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">val_data</span><span class="o">.</span><span class="n">truefreq</span> <span class="o">*</span> <span class="n">val_data</span><span class="o">.</span><span class="n">expo</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">numerical_feats</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;predicted&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">numerical_feats</span><span class="p">):</span>
    <span class="n">dat_copy</span> <span class="o">=</span> <span class="n">val_data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">dat_copy</span><span class="p">[</span><span class="s2">&quot;decile&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">qcut</span><span class="p">(</span><span class="n">dat_copy</span><span class="p">[</span><span class="n">f</span><span class="p">],</span> <span class="mi">10</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">duplicates</span><span class="o">=</span><span class="s1">&#39;drop&#39;</span><span class="p">)</span>
    <span class="n">X_sum</span> <span class="o">=</span> <span class="n">dat_copy</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;decile&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
    
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_sum</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">trueclaims</span> <span class="o">/</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">expo</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_sum</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">predicted</span> <span class="o">/</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">expo</span><span class="p">)</span>    
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>


<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">categorical_feats</span><span class="p">):</span>
    <span class="n">dat_copy</span> <span class="o">=</span> <span class="n">val_data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">X_sum</span> <span class="o">=</span> <span class="n">dat_copy</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="s2">&quot;sum&quot;</span><span class="p">)[[</span><span class="s2">&quot;trueclaims&quot;</span><span class="p">,</span> <span class="s2">&quot;predicted&quot;</span><span class="p">,</span> <span class="s2">&quot;expo&quot;</span><span class="p">]]</span>
    <span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;trueclaims&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;trueclaims&quot;</span><span class="p">]</span><span class="o">/</span><span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;expo&quot;</span><span class="p">]</span>
    <span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;predicted&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;predicted&quot;</span><span class="p">]</span><span class="o">/</span><span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;expo&quot;</span><span class="p">]</span>
    <span class="n">X_sum</span> <span class="o">=</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s2">&quot;expo&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">rot</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Value&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/73aa33f64ef59660166a8294b295a7c4c8f8fa9b5f80cfa873f183d465e92d86.png" src="../_images/73aa33f64ef59660166a8294b295a7c4c8f8fa9b5f80cfa873f183d465e92d86.png" />
<img alt="../_images/27b88001013778e2d637fda130c9c4b190bd64156ff9e668b625cf4f35c728d1.png" src="../_images/27b88001013778e2d637fda130c9c4b190bd64156ff9e668b625cf4f35c728d1.png" />
<img alt="../_images/07758091d04b450c5a7525e38603fc567f22b51694c87f600ed219fd5f2a64ca.png" src="../_images/07758091d04b450c5a7525e38603fc567f22b51694c87f600ed219fd5f2a64ca.png" />
<img alt="../_images/94e0badabe764c1d6e9bbd7f440d39db05817c05e8194aaafb0cf3cc3f88dfc9.png" src="../_images/94e0badabe764c1d6e9bbd7f440d39db05817c05e8194aaafb0cf3cc3f88dfc9.png" />
<img alt="../_images/3d65524489306936f022e13494728e5b252aa65da07735a1be31b028d5e6ec9c.png" src="../_images/3d65524489306936f022e13494728e5b252aa65da07735a1be31b028d5e6ec9c.png" />
</div>
</div>
<p>So we have a working model with the assistance of ChatGPT. The code suggestions by the model were generally good, but sometimes subtlely wrong. We still needed an understanding of the mechanics of fitting a neural network on tabular data in order to debug the code generated by the LLM, and to recognise and include any requirements missed in our original prompt. Overall, use of generative AI made the development of this notebook considerably faster.</p>
<p>Onto the experiment on bias:</p>
</section>
<section id="experiment-on-bias">
<h2>Experiment on Bias<a class="headerlink" href="#experiment-on-bias" title="Permalink to this heading">#</a></h2>
<p>We run the training loop 50 times to get a model. We check whether the model predicts an average frequency that matches the (1) training set on which it was fitted and (2) the true underlying frequency. We resample train/test each time so that the sampling does not skew the result.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">weight_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_models</span><span class="p">):</span>
    <span class="c1"># Resample - Split the dataset into training and validation sets</span>
    <span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">categorical_feats</span><span class="p">:</span>
        <span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoders</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">])</span>
        <span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoders</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">numerical_feats</span><span class="p">:</span>
        <span class="n">scaler</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
        <span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Convert the dataset to PyTorch tensors</span>
    <span class="n">x_train_numerical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">numerical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x_train_categorical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">categorical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="c1"># y_train = torch.tensor(train_data[&#39;claims&#39;].values, dtype=torch.float32)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;claims&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">/</span> <span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">exposure_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">x_val_numerical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">numerical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x_val_categorical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">categorical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="c1"># y_val = torch.tensor(val_data[&#39;claims&#39;].values, dtype=torch.float32)</span>
    <span class="n">y_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="s1">&#39;claims&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">/</span> <span class="n">val_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">exposure_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Create an instance of the FeedForwardNet model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FeedForwardNet</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">numerical_feats</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">categorical_feats</span><span class="p">),</span> <span class="n">embedding_sizes</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">init_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">avg_claim</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PoissonNLLLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">log_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Define the optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

    <span class="c1"># Train the model</span>
    <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">best_epoch</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train_numerical</span><span class="p">,</span> <span class="n">x_train_categorical</span><span class="p">)</span>
        <span class="c1"># loss = criterion(y_pred.squeeze() * exposure_train, y_train * exposure_train)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_val_numerical</span><span class="p">,</span> <span class="n">x_val_categorical</span><span class="p">)</span>
            <span class="c1"># val_loss = criterion(y_val_pred.squeeze() * exposure_val, y_val * exposure_val)</span>
            <span class="n">val_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">y_val_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">y_val</span><span class="p">)</span> <span class="o">*</span> <span class="n">exposure_val</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_val</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="n">val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>
            <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">val_loss</span>
            <span class="n">best_epoch</span> <span class="o">=</span> <span class="n">epoch</span>
            <span class="n">best_weights</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">best_weights</span><span class="p">,</span> <span class="s1">&#39;best_model.pt&#39;</span><span class="p">)</span>

            <span class="n">best_result</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;Epoch&quot;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
                <span class="s2">&quot;Train_Diff&quot;</span><span class="p">:</span> <span class="p">((</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_train</span> <span class="o">-</span> <span class="n">y_train</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>            
                <span class="s2">&quot;Train_Mean&quot;</span><span class="p">:</span> <span class="p">((</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> 
                <span class="s2">&quot;Train_Loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="s2">&quot;Val_mean&quot;</span><span class="p">:</span> <span class="p">((</span><span class="n">y_val_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_val</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_val</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> 
                <span class="s2">&quot;Val_loss&quot;</span><span class="p">:</span> <span class="n">val_loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="s2">&quot;Mean&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="o">*</span><span class="p">((</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.2</span><span class="o">*</span><span class="p">((</span><span class="n">y_val_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_val</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_val</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="p">}</span>
        
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">-</span> <span class="n">best_epoch</span> <span class="o">&gt;=</span> <span class="n">early_stopping_epochs</span><span class="p">:</span>
            <span class="n">results</span> <span class="o">+=</span> <span class="p">[</span><span class="n">best_result</span><span class="p">]</span>
            <span class="k">break</span>

    <span class="n">weight_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_weights</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Epoch</th>
      <th>Train_Diff</th>
      <th>Train_Mean</th>
      <th>Train_Loss</th>
      <th>Val_mean</th>
      <th>Val_loss</th>
      <th>Mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>130</td>
      <td>-0.000645</td>
      <td>0.102177</td>
      <td>0.325923</td>
      <td>0.100916</td>
      <td>0.326849</td>
      <td>0.101925</td>
    </tr>
    <tr>
      <th>1</th>
      <td>112</td>
      <td>0.001128</td>
      <td>0.104126</td>
      <td>0.326204</td>
      <td>0.100970</td>
      <td>0.325138</td>
      <td>0.103495</td>
    </tr>
    <tr>
      <th>2</th>
      <td>100</td>
      <td>-0.011181</td>
      <td>0.091365</td>
      <td>0.329277</td>
      <td>0.104439</td>
      <td>0.330272</td>
      <td>0.093980</td>
    </tr>
    <tr>
      <th>3</th>
      <td>144</td>
      <td>0.003659</td>
      <td>0.106545</td>
      <td>0.326212</td>
      <td>0.101356</td>
      <td>0.326794</td>
      <td>0.105507</td>
    </tr>
    <tr>
      <th>4</th>
      <td>149</td>
      <td>-0.001328</td>
      <td>0.100622</td>
      <td>0.323909</td>
      <td>0.103777</td>
      <td>0.334216</td>
      <td>0.101253</td>
    </tr>
    <tr>
      <th>5</th>
      <td>143</td>
      <td>-0.000898</td>
      <td>0.101453</td>
      <td>0.325552</td>
      <td>0.102908</td>
      <td>0.330316</td>
      <td>0.101744</td>
    </tr>
    <tr>
      <th>6</th>
      <td>121</td>
      <td>-0.002593</td>
      <td>0.099565</td>
      <td>0.324868</td>
      <td>0.103678</td>
      <td>0.332443</td>
      <td>0.100387</td>
    </tr>
    <tr>
      <th>7</th>
      <td>99</td>
      <td>0.004969</td>
      <td>0.107395</td>
      <td>0.325997</td>
      <td>0.101939</td>
      <td>0.330895</td>
      <td>0.106304</td>
    </tr>
    <tr>
      <th>8</th>
      <td>175</td>
      <td>-0.001557</td>
      <td>0.101031</td>
      <td>0.324581</td>
      <td>0.102687</td>
      <td>0.328301</td>
      <td>0.101362</td>
    </tr>
    <tr>
      <th>9</th>
      <td>103</td>
      <td>-0.001491</td>
      <td>0.101537</td>
      <td>0.326496</td>
      <td>0.101486</td>
      <td>0.325832</td>
      <td>0.101527</td>
    </tr>
    <tr>
      <th>10</th>
      <td>129</td>
      <td>-0.000352</td>
      <td>0.101937</td>
      <td>0.324772</td>
      <td>0.103736</td>
      <td>0.331179</td>
      <td>0.102297</td>
    </tr>
    <tr>
      <th>11</th>
      <td>119</td>
      <td>-0.000176</td>
      <td>0.102137</td>
      <td>0.325486</td>
      <td>0.103809</td>
      <td>0.330387</td>
      <td>0.102471</td>
    </tr>
    <tr>
      <th>12</th>
      <td>100</td>
      <td>-0.011435</td>
      <td>0.091064</td>
      <td>0.327402</td>
      <td>0.103818</td>
      <td>0.329969</td>
      <td>0.093615</td>
    </tr>
    <tr>
      <th>13</th>
      <td>151</td>
      <td>0.001976</td>
      <td>0.105372</td>
      <td>0.326752</td>
      <td>0.100252</td>
      <td>0.321554</td>
      <td>0.104348</td>
    </tr>
    <tr>
      <th>14</th>
      <td>114</td>
      <td>-0.012920</td>
      <td>0.089249</td>
      <td>0.327542</td>
      <td>0.104257</td>
      <td>0.332765</td>
      <td>0.092251</td>
    </tr>
    <tr>
      <th>15</th>
      <td>119</td>
      <td>-0.004242</td>
      <td>0.098700</td>
      <td>0.326383</td>
      <td>0.101354</td>
      <td>0.326429</td>
      <td>0.099231</td>
    </tr>
    <tr>
      <th>16</th>
      <td>117</td>
      <td>0.002508</td>
      <td>0.104865</td>
      <td>0.324898</td>
      <td>0.101881</td>
      <td>0.331432</td>
      <td>0.104268</td>
    </tr>
    <tr>
      <th>17</th>
      <td>111</td>
      <td>-0.001113</td>
      <td>0.101655</td>
      <td>0.326419</td>
      <td>0.101905</td>
      <td>0.327278</td>
      <td>0.101705</td>
    </tr>
    <tr>
      <th>18</th>
      <td>86</td>
      <td>0.019172</td>
      <td>0.122249</td>
      <td>0.330964</td>
      <td>0.102479</td>
      <td>0.325229</td>
      <td>0.118295</td>
    </tr>
    <tr>
      <th>19</th>
      <td>170</td>
      <td>0.000330</td>
      <td>0.103443</td>
      <td>0.326102</td>
      <td>0.101849</td>
      <td>0.324363</td>
      <td>0.103124</td>
    </tr>
    <tr>
      <th>20</th>
      <td>136</td>
      <td>-0.002997</td>
      <td>0.099309</td>
      <td>0.324692</td>
      <td>0.103050</td>
      <td>0.332011</td>
      <td>0.100057</td>
    </tr>
    <tr>
      <th>21</th>
      <td>124</td>
      <td>-0.002397</td>
      <td>0.100309</td>
      <td>0.326027</td>
      <td>0.102895</td>
      <td>0.327030</td>
      <td>0.100826</td>
    </tr>
    <tr>
      <th>22</th>
      <td>127</td>
      <td>0.001637</td>
      <td>0.104221</td>
      <td>0.325922</td>
      <td>0.103401</td>
      <td>0.328156</td>
      <td>0.104057</td>
    </tr>
    <tr>
      <th>23</th>
      <td>109</td>
      <td>0.003718</td>
      <td>0.106884</td>
      <td>0.327108</td>
      <td>0.101984</td>
      <td>0.324299</td>
      <td>0.105904</td>
    </tr>
    <tr>
      <th>24</th>
      <td>104</td>
      <td>0.006168</td>
      <td>0.108760</td>
      <td>0.326227</td>
      <td>0.100372</td>
      <td>0.330046</td>
      <td>0.107083</td>
    </tr>
    <tr>
      <th>25</th>
      <td>124</td>
      <td>-0.000329</td>
      <td>0.102373</td>
      <td>0.325716</td>
      <td>0.102718</td>
      <td>0.327641</td>
      <td>0.102442</td>
    </tr>
    <tr>
      <th>26</th>
      <td>104</td>
      <td>0.004465</td>
      <td>0.107011</td>
      <td>0.325303</td>
      <td>0.101300</td>
      <td>0.329909</td>
      <td>0.105869</td>
    </tr>
    <tr>
      <th>27</th>
      <td>97</td>
      <td>0.003505</td>
      <td>0.106148</td>
      <td>0.326087</td>
      <td>0.102803</td>
      <td>0.328730</td>
      <td>0.105479</td>
    </tr>
    <tr>
      <th>28</th>
      <td>152</td>
      <td>0.003044</td>
      <td>0.106007</td>
      <td>0.325944</td>
      <td>0.101053</td>
      <td>0.325613</td>
      <td>0.105016</td>
    </tr>
    <tr>
      <th>29</th>
      <td>119</td>
      <td>0.000471</td>
      <td>0.103239</td>
      <td>0.325658</td>
      <td>0.101029</td>
      <td>0.328178</td>
      <td>0.102797</td>
    </tr>
    <tr>
      <th>30</th>
      <td>105</td>
      <td>-0.010499</td>
      <td>0.092104</td>
      <td>0.327594</td>
      <td>0.102481</td>
      <td>0.329511</td>
      <td>0.094180</td>
    </tr>
    <tr>
      <th>31</th>
      <td>101</td>
      <td>0.000356</td>
      <td>0.103143</td>
      <td>0.326779</td>
      <td>0.101786</td>
      <td>0.328151</td>
      <td>0.102872</td>
    </tr>
    <tr>
      <th>32</th>
      <td>97</td>
      <td>-0.007540</td>
      <td>0.094523</td>
      <td>0.325637</td>
      <td>0.106127</td>
      <td>0.335037</td>
      <td>0.096844</td>
    </tr>
    <tr>
      <th>33</th>
      <td>110</td>
      <td>-0.007851</td>
      <td>0.094789</td>
      <td>0.326994</td>
      <td>0.103555</td>
      <td>0.327416</td>
      <td>0.096542</td>
    </tr>
    <tr>
      <th>34</th>
      <td>130</td>
      <td>0.007680</td>
      <td>0.110865</td>
      <td>0.327033</td>
      <td>0.102342</td>
      <td>0.324362</td>
      <td>0.109160</td>
    </tr>
    <tr>
      <th>35</th>
      <td>106</td>
      <td>-0.000806</td>
      <td>0.101983</td>
      <td>0.325870</td>
      <td>0.102793</td>
      <td>0.326325</td>
      <td>0.102145</td>
    </tr>
    <tr>
      <th>36</th>
      <td>116</td>
      <td>0.001485</td>
      <td>0.104698</td>
      <td>0.326662</td>
      <td>0.101559</td>
      <td>0.323815</td>
      <td>0.104070</td>
    </tr>
    <tr>
      <th>37</th>
      <td>107</td>
      <td>0.001997</td>
      <td>0.104687</td>
      <td>0.325665</td>
      <td>0.104222</td>
      <td>0.328122</td>
      <td>0.104594</td>
    </tr>
    <tr>
      <th>38</th>
      <td>90</td>
      <td>-0.002557</td>
      <td>0.100565</td>
      <td>0.327322</td>
      <td>0.102357</td>
      <td>0.325118</td>
      <td>0.100923</td>
    </tr>
    <tr>
      <th>39</th>
      <td>141</td>
      <td>0.003426</td>
      <td>0.106498</td>
      <td>0.325961</td>
      <td>0.099360</td>
      <td>0.324842</td>
      <td>0.105071</td>
    </tr>
    <tr>
      <th>40</th>
      <td>121</td>
      <td>-0.002862</td>
      <td>0.099332</td>
      <td>0.324686</td>
      <td>0.103656</td>
      <td>0.332309</td>
      <td>0.100197</td>
    </tr>
    <tr>
      <th>41</th>
      <td>87</td>
      <td>-0.008324</td>
      <td>0.094025</td>
      <td>0.327252</td>
      <td>0.102607</td>
      <td>0.331421</td>
      <td>0.095741</td>
    </tr>
    <tr>
      <th>42</th>
      <td>119</td>
      <td>-0.000314</td>
      <td>0.101241</td>
      <td>0.323135</td>
      <td>0.107160</td>
      <td>0.337909</td>
      <td>0.102425</td>
    </tr>
    <tr>
      <th>43</th>
      <td>164</td>
      <td>-0.000449</td>
      <td>0.101789</td>
      <td>0.324233</td>
      <td>0.102688</td>
      <td>0.331431</td>
      <td>0.101968</td>
    </tr>
    <tr>
      <th>44</th>
      <td>136</td>
      <td>-0.001592</td>
      <td>0.100898</td>
      <td>0.324840</td>
      <td>0.102806</td>
      <td>0.329460</td>
      <td>0.101279</td>
    </tr>
    <tr>
      <th>45</th>
      <td>111</td>
      <td>-0.003606</td>
      <td>0.098748</td>
      <td>0.325302</td>
      <td>0.105494</td>
      <td>0.330357</td>
      <td>0.100097</td>
    </tr>
    <tr>
      <th>46</th>
      <td>203</td>
      <td>-0.002093</td>
      <td>0.100181</td>
      <td>0.324759</td>
      <td>0.105140</td>
      <td>0.330792</td>
      <td>0.101173</td>
    </tr>
    <tr>
      <th>47</th>
      <td>158</td>
      <td>0.000963</td>
      <td>0.103939</td>
      <td>0.325994</td>
      <td>0.101919</td>
      <td>0.325458</td>
      <td>0.103535</td>
    </tr>
    <tr>
      <th>48</th>
      <td>107</td>
      <td>-0.008366</td>
      <td>0.095289</td>
      <td>0.330197</td>
      <td>0.103185</td>
      <td>0.320212</td>
      <td>0.096868</td>
    </tr>
    <tr>
      <th>49</th>
      <td>120</td>
      <td>-0.004596</td>
      <td>0.097606</td>
      <td>0.325099</td>
      <td>0.105453</td>
      <td>0.331423</td>
      <td>0.099175</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>So, are our 50 neural networks making biased predictions vs their training sets?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">Train_Diff</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">kde</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkgreen&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.lines.Line2D at 0x2bf030070&gt;
</pre></div>
</div>
<img alt="../_images/9a30d3813fdfc2f6467b495ff29c2b37aefa44042b0fedbe8cb097a07f492336.png" src="../_images/9a30d3813fdfc2f6467b495ff29c2b37aefa44042b0fedbe8cb097a07f492336.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bias_std</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">Train_Diff</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">Train_Diff</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">Train_Diff</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-0.0008891218338976614, 0.005435967840328695)
</pre></div>
</div>
</div>
</div>
<p>Training runs do appear to match on average, the mean frequency of the training dataset.
However early stopping appears to lead to models that are sometimes predicting higher and sometimes predicting lower.</p>
<p>How does the model go in matching the true and dataset means?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">Mean</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">kde</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">avg_claim</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkgreen&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.lines.Line2D at 0x2bf0339d0&gt;
</pre></div>
</div>
<img alt="../_images/6672c39cbe3b480e5570c28340354cbde228768f4eda3ea720f97c8ec200d322.png" src="../_images/6672c39cbe3b480e5570c28340354cbde228768f4eda3ea720f97c8ec200d322.png" />
</div>
</div>
<p>Models predict near the mean values, but there is some variabilty of the predictions around it.</p>
<p>Overall, with help from ChatGPT, we appear to have replicated the finding from the Wuthrich paper that early stopping can lead to the biased models. On average, models predict at near the correct average levels, but individual models may predict with some variability around that level - they are biased. None of this is new or original but ChatGPT was quite helpful in being to recreate this analysis in Python (Wuthrich appears to have used R).</p>
<p>Currently our average validation loss is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">avg_val_loss</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">Val_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">avg_val_loss</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.32853454232215884
</pre></div>
</div>
</div>
</div>
</section>
<section id="solving-the-bias-issue">
<h2>Solving the bias issue<a class="headerlink" href="#solving-the-bias-issue" title="Permalink to this heading">#</a></h2>
<p>Wuthrich suggests regularization approaches to reduce the bias. See Section 4 in his <a class="reference external" href="https://deliverypdf.ssrn.com/delivery.php?ID=312025116122114123096127007073121120117035019009034090074026123081067081117096067101096034023121015125114118022102077067077095059082053065068066072013007113086088071086042026121006066004001110017094030007030086079127114066096116006064022127099103006&amp;EXT=pdf&amp;INDEX=TRUE">paper</a>.</p>
<p>Wuthrich in his <a class="reference external" href="https://deliverypdf.ssrn.com/delivery.php?ID=576121020003020092097100025085064064032043077033024024075121124067085065070083096000011050026016049049112005112104113127090028048026026076017093095012093109083070084075000006018069083020001093127069023083082095067068083098094095090115031073119125083&amp;EXT=pdf&amp;INDEX=TRUE">other work</a> also discusses ensembling approach in Section 5.1.6. Here the 50 models have a variability in the overall predicted levels, but on average they are right. So if we were to take the final model as the ensemble average of the 50 models, it would average out the bias issue.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Average neural network prediction:</span>
<span class="n">avg_nn_prediction</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">Mean</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1"># Average frequency in dataset</span>
<span class="n">avg_frequency</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;claims&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="c1"># True underlying frequency </span>
<span class="n">true_frequency</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;truefreq&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Average neural network prediction:&quot;</span><span class="p">,</span> <span class="n">avg_nn_prediction</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Average frequency in dataset:&quot;</span><span class="p">,</span> <span class="n">avg_frequency</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True underlying frequency:&quot;</span><span class="p">,</span> <span class="n">true_frequency</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Averaged prediction vs true frequency:&quot;</span><span class="p">,</span> <span class="n">avg_nn_prediction</span> <span class="o">/</span> <span class="n">true_frequency</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Average neural network prediction: 0.10194956490397454
Average frequency in dataset: 0.10269062235781508
True underlying frequency: 0.10199107887858037
Averaged prediction vs true frequency: 0.9995929646488468
</pre></div>
</div>
</div>
</div>
<section id="learn-rates">
<h3>Learn Rates<a class="headerlink" href="#learn-rates" title="Permalink to this heading">#</a></h3>
<p>Another idea from other neural network work is to <a class="reference external" href="https://datascience.stackexchange.com/questions/23549/why-is-the-learning-rate-for-the-bias-usually-twice-as-large-as-the-the-lr-for-t">apply a higher learn rate solely on the bias</a>.</p>
<p>The idea is to allow the bias to converge on its true value faster, before the other weights begin to overfit, triggering the early stopping and halting training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hidden_size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>  <span class="c1"># Replace with the multi-layer parameters.</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">99999</span>  <span class="c1"># should not be a factor, we train until early stopping kicks in</span>
<span class="n">early_stopping_epochs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span>      <span class="c1"># Half base learning rate</span>
<span class="n">bias_learning_rate</span> <span class="o">=</span> <span class="mf">0.05</span>  <span class="c1"># Increased bias learning rate</span>
</pre></div>
</div>
</div>
</div>
<p>The training loop is updated with the optimizer applying this higher learning rate to the one bias value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">weight_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_models</span><span class="p">):</span>
    <span class="c1"># Resample - Split the dataset into training and validation sets</span>
    <span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">categorical_feats</span><span class="p">:</span>
        <span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoders</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">])</span>
        <span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoders</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">numerical_feats</span><span class="p">:</span>
        <span class="n">scaler</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
        <span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Convert the dataset to PyTorch tensors</span>
    <span class="n">x_train_numerical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">numerical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x_train_categorical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">categorical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="c1"># y_train = torch.tensor(train_data[&#39;claims&#39;].values, dtype=torch.float32)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;claims&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">/</span> <span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">exposure_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">x_val_numerical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">numerical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x_val_categorical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">categorical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="c1"># y_val = torch.tensor(val_data[&#39;claims&#39;].values, dtype=torch.float32)</span>
    <span class="n">y_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="s1">&#39;claims&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">/</span> <span class="n">val_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">exposure_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Create an instance of the FeedForwardNet model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FeedForwardNet</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">numerical_feats</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">categorical_feats</span><span class="p">),</span> <span class="n">embedding_sizes</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">init_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">avg_claim</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PoissonNLLLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">log_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Bias specific learn rates</span>
    <span class="n">my_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;fc2.bias&#39;</span><span class="p">]</span>
    <span class="n">bias_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">kv</span><span class="p">:</span> <span class="n">kv</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">my_list</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()))</span>
    <span class="n">base_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">kv</span><span class="p">:</span> <span class="n">kv</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">my_list</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()))</span>

    <span class="c1"># Define the optimizer</span>
    <span class="c1"># optimizer = optim.Adam(model.parameters(), lr=learning_rate)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span>
                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">temp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">temp</span> <span class="ow">in</span> <span class="n">base_params</span><span class="p">]},</span>
                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">temp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">temp</span> <span class="ow">in</span> <span class="n">bias_params</span><span class="p">],</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">bias_learning_rate</span><span class="p">}</span>
            <span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

    <span class="c1"># Train the model</span>
    <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">best_epoch</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train_numerical</span><span class="p">,</span> <span class="n">x_train_categorical</span><span class="p">)</span>
        <span class="c1"># loss = criterion(y_pred.squeeze() * exposure_train, y_train * exposure_train)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_val_numerical</span><span class="p">,</span> <span class="n">x_val_categorical</span><span class="p">)</span>
            <span class="c1"># val_loss = criterion(y_val_pred.squeeze() * exposure_val, y_val * exposure_val)</span>
            <span class="n">val_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">y_val_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">y_val</span><span class="p">)</span> <span class="o">*</span> <span class="n">exposure_val</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_val</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="n">val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>
            <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">val_loss</span>
            <span class="n">best_epoch</span> <span class="o">=</span> <span class="n">epoch</span>
            <span class="n">best_weights</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">best_weights</span><span class="p">,</span> <span class="s1">&#39;best_model.pt&#39;</span><span class="p">)</span>

            <span class="n">best_result</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;Epoch&quot;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
                <span class="s2">&quot;Train_Diff&quot;</span><span class="p">:</span> <span class="p">((</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_train</span> <span class="o">-</span> <span class="n">y_train</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>            
                <span class="s2">&quot;Train_Mean&quot;</span><span class="p">:</span> <span class="p">((</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> 
                <span class="s2">&quot;Train_Loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="s2">&quot;Val_mean&quot;</span><span class="p">:</span> <span class="p">((</span><span class="n">y_val_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_val</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_val</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> 
                <span class="s2">&quot;Val_loss&quot;</span><span class="p">:</span> <span class="n">val_loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="s2">&quot;Mean&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="o">*</span><span class="p">((</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.2</span><span class="o">*</span><span class="p">((</span><span class="n">y_val_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_val</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_val</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="p">}</span>
        
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">-</span> <span class="n">best_epoch</span> <span class="o">&gt;=</span> <span class="n">early_stopping_epochs</span><span class="p">:</span>
            <span class="n">results</span> <span class="o">+=</span> <span class="p">[</span><span class="n">best_result</span><span class="p">]</span>            
            <span class="k">break</span>

    <span class="n">weight_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_weights</span><span class="p">)</span>        
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># check we have the right parameter</span>
<span class="n">bias_params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;fc2.bias&#39;,
  Parameter containing:
  tensor(-2.1823, requires_grad=True))]
</pre></div>
</div>
</div>
</div>
<p>So how do results look with this?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Epoch</th>
      <th>Train_Diff</th>
      <th>Train_Mean</th>
      <th>Train_Loss</th>
      <th>Val_mean</th>
      <th>Val_loss</th>
      <th>Mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>205</td>
      <td>0.002327</td>
      <td>0.104974</td>
      <td>0.325422</td>
      <td>0.101117</td>
      <td>0.328724</td>
      <td>0.104203</td>
    </tr>
    <tr>
      <th>1</th>
      <td>168</td>
      <td>0.000035</td>
      <td>0.102497</td>
      <td>0.325165</td>
      <td>0.101048</td>
      <td>0.330128</td>
      <td>0.102208</td>
    </tr>
    <tr>
      <th>2</th>
      <td>208</td>
      <td>0.001107</td>
      <td>0.103627</td>
      <td>0.324849</td>
      <td>0.102306</td>
      <td>0.330064</td>
      <td>0.103363</td>
    </tr>
    <tr>
      <th>3</th>
      <td>194</td>
      <td>-0.002439</td>
      <td>0.100004</td>
      <td>0.325384</td>
      <td>0.104841</td>
      <td>0.329106</td>
      <td>0.100971</td>
    </tr>
    <tr>
      <th>4</th>
      <td>216</td>
      <td>-0.002707</td>
      <td>0.099631</td>
      <td>0.324795</td>
      <td>0.104469</td>
      <td>0.329759</td>
      <td>0.100599</td>
    </tr>
    <tr>
      <th>5</th>
      <td>155</td>
      <td>-0.004103</td>
      <td>0.098337</td>
      <td>0.325339</td>
      <td>0.103473</td>
      <td>0.330643</td>
      <td>0.099364</td>
    </tr>
    <tr>
      <th>6</th>
      <td>173</td>
      <td>0.001059</td>
      <td>0.104135</td>
      <td>0.326621</td>
      <td>0.102555</td>
      <td>0.323889</td>
      <td>0.103819</td>
    </tr>
    <tr>
      <th>7</th>
      <td>239</td>
      <td>0.000795</td>
      <td>0.103905</td>
      <td>0.325969</td>
      <td>0.101931</td>
      <td>0.323872</td>
      <td>0.103510</td>
    </tr>
    <tr>
      <th>8</th>
      <td>219</td>
      <td>-0.000637</td>
      <td>0.102157</td>
      <td>0.325523</td>
      <td>0.102171</td>
      <td>0.325915</td>
      <td>0.102160</td>
    </tr>
    <tr>
      <th>9</th>
      <td>168</td>
      <td>0.002321</td>
      <td>0.105407</td>
      <td>0.326163</td>
      <td>0.101906</td>
      <td>0.324060</td>
      <td>0.104707</td>
    </tr>
    <tr>
      <th>10</th>
      <td>143</td>
      <td>0.000277</td>
      <td>0.103440</td>
      <td>0.327099</td>
      <td>0.103935</td>
      <td>0.323553</td>
      <td>0.103539</td>
    </tr>
    <tr>
      <th>11</th>
      <td>138</td>
      <td>-0.003321</td>
      <td>0.099526</td>
      <td>0.326536</td>
      <td>0.104895</td>
      <td>0.325924</td>
      <td>0.100600</td>
    </tr>
    <tr>
      <th>12</th>
      <td>139</td>
      <td>-0.002427</td>
      <td>0.099488</td>
      <td>0.324715</td>
      <td>0.105711</td>
      <td>0.334265</td>
      <td>0.100732</td>
    </tr>
    <tr>
      <th>13</th>
      <td>155</td>
      <td>-0.002495</td>
      <td>0.100064</td>
      <td>0.325782</td>
      <td>0.104031</td>
      <td>0.328607</td>
      <td>0.100857</td>
    </tr>
    <tr>
      <th>14</th>
      <td>176</td>
      <td>-0.000415</td>
      <td>0.102324</td>
      <td>0.326006</td>
      <td>0.103674</td>
      <td>0.326932</td>
      <td>0.102594</td>
    </tr>
    <tr>
      <th>15</th>
      <td>208</td>
      <td>0.002315</td>
      <td>0.104998</td>
      <td>0.325499</td>
      <td>0.102390</td>
      <td>0.327024</td>
      <td>0.104476</td>
    </tr>
    <tr>
      <th>16</th>
      <td>214</td>
      <td>0.001363</td>
      <td>0.104500</td>
      <td>0.326591</td>
      <td>0.102777</td>
      <td>0.324041</td>
      <td>0.104155</td>
    </tr>
    <tr>
      <th>17</th>
      <td>150</td>
      <td>0.001920</td>
      <td>0.104772</td>
      <td>0.326084</td>
      <td>0.101824</td>
      <td>0.325707</td>
      <td>0.104182</td>
    </tr>
    <tr>
      <th>18</th>
      <td>160</td>
      <td>-0.002173</td>
      <td>0.100638</td>
      <td>0.326172</td>
      <td>0.102994</td>
      <td>0.325890</td>
      <td>0.101109</td>
    </tr>
    <tr>
      <th>19</th>
      <td>183</td>
      <td>0.000192</td>
      <td>0.103026</td>
      <td>0.325667</td>
      <td>0.101727</td>
      <td>0.326649</td>
      <td>0.102766</td>
    </tr>
    <tr>
      <th>20</th>
      <td>180</td>
      <td>-0.000990</td>
      <td>0.101203</td>
      <td>0.325036</td>
      <td>0.103650</td>
      <td>0.331248</td>
      <td>0.101692</td>
    </tr>
    <tr>
      <th>21</th>
      <td>197</td>
      <td>-0.001338</td>
      <td>0.100844</td>
      <td>0.324968</td>
      <td>0.104099</td>
      <td>0.331851</td>
      <td>0.101495</td>
    </tr>
    <tr>
      <th>22</th>
      <td>164</td>
      <td>0.001723</td>
      <td>0.104569</td>
      <td>0.326951</td>
      <td>0.100148</td>
      <td>0.328108</td>
      <td>0.103685</td>
    </tr>
    <tr>
      <th>23</th>
      <td>207</td>
      <td>0.003274</td>
      <td>0.105323</td>
      <td>0.323811</td>
      <td>0.103895</td>
      <td>0.333068</td>
      <td>0.105037</td>
    </tr>
    <tr>
      <th>24</th>
      <td>166</td>
      <td>0.002680</td>
      <td>0.105871</td>
      <td>0.326872</td>
      <td>0.101878</td>
      <td>0.322789</td>
      <td>0.105072</td>
    </tr>
    <tr>
      <th>25</th>
      <td>180</td>
      <td>0.000940</td>
      <td>0.103496</td>
      <td>0.325236</td>
      <td>0.103305</td>
      <td>0.328780</td>
      <td>0.103458</td>
    </tr>
    <tr>
      <th>26</th>
      <td>201</td>
      <td>-0.000276</td>
      <td>0.102151</td>
      <td>0.324925</td>
      <td>0.101921</td>
      <td>0.329467</td>
      <td>0.102105</td>
    </tr>
    <tr>
      <th>27</th>
      <td>177</td>
      <td>-0.000592</td>
      <td>0.102395</td>
      <td>0.326053</td>
      <td>0.101323</td>
      <td>0.325711</td>
      <td>0.102180</td>
    </tr>
    <tr>
      <th>28</th>
      <td>215</td>
      <td>0.002475</td>
      <td>0.105416</td>
      <td>0.326046</td>
      <td>0.100961</td>
      <td>0.325818</td>
      <td>0.104525</td>
    </tr>
    <tr>
      <th>29</th>
      <td>171</td>
      <td>-0.001479</td>
      <td>0.100942</td>
      <td>0.325523</td>
      <td>0.103197</td>
      <td>0.330104</td>
      <td>0.101393</td>
    </tr>
    <tr>
      <th>30</th>
      <td>230</td>
      <td>-0.000927</td>
      <td>0.101362</td>
      <td>0.324933</td>
      <td>0.103380</td>
      <td>0.330297</td>
      <td>0.101765</td>
    </tr>
    <tr>
      <th>31</th>
      <td>162</td>
      <td>0.001179</td>
      <td>0.103946</td>
      <td>0.326290</td>
      <td>0.102380</td>
      <td>0.327654</td>
      <td>0.103633</td>
    </tr>
    <tr>
      <th>32</th>
      <td>183</td>
      <td>0.002927</td>
      <td>0.105697</td>
      <td>0.325705</td>
      <td>0.101642</td>
      <td>0.327327</td>
      <td>0.104886</td>
    </tr>
    <tr>
      <th>33</th>
      <td>194</td>
      <td>0.001614</td>
      <td>0.103999</td>
      <td>0.325319</td>
      <td>0.102904</td>
      <td>0.330006</td>
      <td>0.103780</td>
    </tr>
    <tr>
      <th>34</th>
      <td>170</td>
      <td>0.002754</td>
      <td>0.105223</td>
      <td>0.325185</td>
      <td>0.101073</td>
      <td>0.330482</td>
      <td>0.104393</td>
    </tr>
    <tr>
      <th>35</th>
      <td>144</td>
      <td>-0.002177</td>
      <td>0.100043</td>
      <td>0.325104</td>
      <td>0.102772</td>
      <td>0.331401</td>
      <td>0.100589</td>
    </tr>
    <tr>
      <th>36</th>
      <td>212</td>
      <td>0.000414</td>
      <td>0.102930</td>
      <td>0.325115</td>
      <td>0.101467</td>
      <td>0.328294</td>
      <td>0.102637</td>
    </tr>
    <tr>
      <th>37</th>
      <td>173</td>
      <td>-0.000467</td>
      <td>0.102793</td>
      <td>0.326599</td>
      <td>0.103692</td>
      <td>0.322864</td>
      <td>0.102973</td>
    </tr>
    <tr>
      <th>38</th>
      <td>200</td>
      <td>-0.001061</td>
      <td>0.101328</td>
      <td>0.325358</td>
      <td>0.102929</td>
      <td>0.329955</td>
      <td>0.101648</td>
    </tr>
    <tr>
      <th>39</th>
      <td>196</td>
      <td>-0.002276</td>
      <td>0.100090</td>
      <td>0.325243</td>
      <td>0.103280</td>
      <td>0.330101</td>
      <td>0.100728</td>
    </tr>
    <tr>
      <th>40</th>
      <td>201</td>
      <td>0.000618</td>
      <td>0.103018</td>
      <td>0.324686</td>
      <td>0.101520</td>
      <td>0.331242</td>
      <td>0.102718</td>
    </tr>
    <tr>
      <th>41</th>
      <td>188</td>
      <td>0.001549</td>
      <td>0.104363</td>
      <td>0.325910</td>
      <td>0.102523</td>
      <td>0.326799</td>
      <td>0.103995</td>
    </tr>
    <tr>
      <th>42</th>
      <td>128</td>
      <td>0.002517</td>
      <td>0.105386</td>
      <td>0.326826</td>
      <td>0.101684</td>
      <td>0.326658</td>
      <td>0.104645</td>
    </tr>
    <tr>
      <th>43</th>
      <td>150</td>
      <td>0.002720</td>
      <td>0.106185</td>
      <td>0.327632</td>
      <td>0.101504</td>
      <td>0.321514</td>
      <td>0.105249</td>
    </tr>
    <tr>
      <th>44</th>
      <td>202</td>
      <td>-0.000765</td>
      <td>0.101599</td>
      <td>0.325121</td>
      <td>0.103453</td>
      <td>0.330568</td>
      <td>0.101969</td>
    </tr>
    <tr>
      <th>45</th>
      <td>195</td>
      <td>-0.007928</td>
      <td>0.094683</td>
      <td>0.326373</td>
      <td>0.103778</td>
      <td>0.328316</td>
      <td>0.096502</td>
    </tr>
    <tr>
      <th>46</th>
      <td>179</td>
      <td>-0.002498</td>
      <td>0.099990</td>
      <td>0.325483</td>
      <td>0.103390</td>
      <td>0.329075</td>
      <td>0.100670</td>
    </tr>
    <tr>
      <th>47</th>
      <td>177</td>
      <td>-0.002789</td>
      <td>0.099647</td>
      <td>0.325664</td>
      <td>0.103856</td>
      <td>0.328823</td>
      <td>0.100489</td>
    </tr>
    <tr>
      <th>48</th>
      <td>163</td>
      <td>0.002125</td>
      <td>0.104693</td>
      <td>0.325735</td>
      <td>0.102967</td>
      <td>0.327648</td>
      <td>0.104348</td>
    </tr>
    <tr>
      <th>49</th>
      <td>237</td>
      <td>-0.000325</td>
      <td>0.102494</td>
      <td>0.325931</td>
      <td>0.104136</td>
      <td>0.325074</td>
      <td>0.102823</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">Train_Diff</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">kde</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkgreen&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.lines.Line2D at 0x2c01e7e50&gt;
</pre></div>
</div>
<img alt="../_images/ff2457edec264d0ab48a61239f1cf78920544299125513da58de77bb95308971.png" src="../_images/ff2457edec264d0ab48a61239f1cf78920544299125513da58de77bb95308971.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">new_bias_std</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">Train_Diff</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">Train_Diff</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">Train_Diff</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-6.771093656425364e-05, 0.0022470243173901318)
</pre></div>
</div>
</div>
</div>
<p>Compare the standard deviation of the bias with the bias-specific learning rate compared with the same learning rate across both.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">new_bias_std</span> <span class="o">/</span> <span class="n">bias_std</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.41336232726024763
</pre></div>
</div>
</div>
</div>
<p>We can see with the abve result that the increased bias-specific learn rate was successful in reducing the variance in the overall model bias.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">Mean</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">kde</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">avg_claim</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkgreen&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.lines.Line2D at 0x2c4911a80&gt;
</pre></div>
</div>
<img alt="../_images/7303f4fc97f11c9fdeb087efefbad40f11d55b89c34a0eb93058110afc329404.png" src="../_images/7303f4fc97f11c9fdeb087efefbad40f11d55b89c34a0eb93058110afc329404.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span>
    <span class="c1"># Average neural network prediction:</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">Mean</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> 
    <span class="c1"># Average frequency in dataset</span>
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;claims&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span>
    <span class="c1"># True underlying frequency </span>
    <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;truefreq&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.10261997532844544, 0.10269062235781508, 0.10199107887858037)
</pre></div>
</div>
</div>
</div>
<p>Predicted mean frequency for the models look to track near the training data mean more consistently.</p>
<p>Finally, the validation loss. Unlike regularisation, which is known to shape the segmentation predictions, conceptually a higher learn rate for the bias should just mean the training loop focuses on getting a more aligned bias estimate. Does this come through in our result?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fast_bias_val_loss</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">Val_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">fast_bias_val_loss</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.32791590332984927
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fast_bias_val_loss</span> <span class="o">-</span> <span class="n">avg_val_loss</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-0.0006186389923095725
</pre></div>
</div>
</div>
</div>
<p>The experiments are random so results may vary if this is re-run but for this run, val loss was similar.</p>
<p>Look at the diagnostics again. This will be on the last model of the batch that was trained.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the best model in the last run</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;best_model.pt&#39;</span><span class="p">))</span>

<span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_val_numerical</span><span class="p">,</span> <span class="n">x_val_categorical</span><span class="p">)</span>

<span class="n">val_data</span><span class="p">[</span><span class="s2">&quot;predicted&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_val_pred</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">*</span> <span class="n">val_data</span><span class="o">.</span><span class="n">expo</span>
<span class="n">val_data</span><span class="p">[</span><span class="s2">&quot;trueclaims&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">val_data</span><span class="o">.</span><span class="n">truefreq</span> <span class="o">*</span> <span class="n">val_data</span><span class="o">.</span><span class="n">expo</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">numerical_feats</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;predicted&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">numerical_feats</span><span class="p">):</span>
    <span class="n">dat_copy</span> <span class="o">=</span> <span class="n">val_data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">dat_copy</span><span class="p">[</span><span class="s2">&quot;decile&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">qcut</span><span class="p">(</span><span class="n">dat_copy</span><span class="p">[</span><span class="n">f</span><span class="p">],</span> <span class="mi">10</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">duplicates</span><span class="o">=</span><span class="s1">&#39;drop&#39;</span><span class="p">)</span>
    <span class="n">X_sum</span> <span class="o">=</span> <span class="n">dat_copy</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;decile&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
        
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_sum</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">trueclaims</span> <span class="o">/</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">expo</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_sum</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">predicted</span> <span class="o">/</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">expo</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>


<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">categorical_feats</span><span class="p">):</span>
    <span class="n">dat_copy</span> <span class="o">=</span> <span class="n">val_data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">X_sum</span> <span class="o">=</span> <span class="n">dat_copy</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="s2">&quot;sum&quot;</span><span class="p">)[[</span><span class="s2">&quot;trueclaims&quot;</span><span class="p">,</span> <span class="s2">&quot;predicted&quot;</span><span class="p">,</span> <span class="s2">&quot;expo&quot;</span><span class="p">]]</span>
    <span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;trueclaims&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;trueclaims&quot;</span><span class="p">]</span><span class="o">/</span><span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;expo&quot;</span><span class="p">]</span>
    <span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;predicted&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;predicted&quot;</span><span class="p">]</span><span class="o">/</span><span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;expo&quot;</span><span class="p">]</span>
    <span class="n">X_sum</span> <span class="o">=</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s2">&quot;expo&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">rot</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Value&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/230e744d065563f5073d90f0197aaafd8467c162e29d68406d16d20e090251b4.png" src="../_images/230e744d065563f5073d90f0197aaafd8467c162e29d68406d16d20e090251b4.png" />
<img alt="../_images/31016c749df73ae1d71254f6293ce1d4f423b98587e9661709c62a534716ab4d.png" src="../_images/31016c749df73ae1d71254f6293ce1d4f423b98587e9661709c62a534716ab4d.png" />
<img alt="../_images/7c1154c562a56486427c20d9d3da910b5a463289b7d507b7c13908449f301d6a.png" src="../_images/7c1154c562a56486427c20d9d3da910b5a463289b7d507b7c13908449f301d6a.png" />
<img alt="../_images/b64cc92fb143ae203c5fdea52fe027a1198e080e6c702f2bc7988722428d1ee9.png" src="../_images/b64cc92fb143ae203c5fdea52fe027a1198e080e6c702f2bc7988722428d1ee9.png" />
<img alt="../_images/619e05da66db01e6148ae2a898c94b7c4579ec56df387548c1a0d0aa7fe27b7f.png" src="../_images/619e05da66db01e6148ae2a898c94b7c4579ec56df387548c1a0d0aa7fe27b7f.png" />
</div>
</div>
<p>Diagnostics still look good.</p>
</section>
</section>
<section id="just-change-the-bias">
<h2>Just change the bias<a class="headerlink" href="#just-change-the-bias" title="Permalink to this heading">#</a></h2>
<p>In <a class="reference external" href="https://deliverypdf.ssrn.com/delivery.php?ID=576121020003020092097100025085064064032043077033024024075121124067085065070083096000011050026016049049112005112104113127090028048026026076017093095012093109083070084075000006018069083020001093127069023083082095067068083098094095090115031073119125083&amp;EXT=pdf&amp;INDEX=TRUE">Wuthrich 2023</a>, he proposes (Section 5.1.5) the idea of training a GLM to full convergence with the final hidden layer weights and biases from the neural network to solve the bias issue.</p>
<p>Finally, if having an unbiased model with early stopping is a must have, with <code class="docutils literal notranslate"><span class="pre">pytorch</span></code> we have full control over the training loop.</p>
<p>We could just explicitly set bias after every epoch to the value that keeps the predicted mean to be the mean of the training data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the hyperparameters</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>  <span class="c1"># Replace with the multi-layer parameters.</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">9999</span>  <span class="c1"># should not be a factor, we train until early stopping kicks in</span>
<span class="n">early_stopping_epochs</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">weight_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_models</span><span class="p">):</span>
    <span class="c1"># Resample - Split the dataset into training and validation sets</span>
    <span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">categorical_feats</span><span class="p">:</span>
        <span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoders</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">])</span>
        <span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoders</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">numerical_feats</span><span class="p">:</span>
        <span class="n">scaler</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
        <span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Convert the dataset to PyTorch tensors</span>
    <span class="n">x_train_numerical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">numerical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x_train_categorical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">categorical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

    <span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;claims&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">/</span> <span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">exposure_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">x_val_numerical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">numerical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x_val_categorical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">categorical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

    <span class="n">y_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="s1">&#39;claims&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">/</span> <span class="n">val_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">exposure_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Create an instance of the FeedForwardNet model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FeedForwardNet</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">numerical_feats</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">categorical_feats</span><span class="p">),</span> <span class="n">embedding_sizes</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">init_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">avg_claim</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PoissonNLLLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">log_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Define the optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

    <span class="c1"># Train the model</span>
    <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">best_epoch</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train_numerical</span><span class="p">,</span> <span class="n">x_train_categorical</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>     
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="c1"># Adjust the bias each epoch</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># Get predictions</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train_numerical</span><span class="p">,</span> <span class="n">x_train_categorical</span><span class="p">)</span>

            <span class="c1"># Get adjustment</span>
            <span class="n">adjustment</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="n">y_train</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>

        <span class="n">model</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="n">adjustment</span> 

        <span class="c1"># Adjusted y_pred</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train_numerical</span><span class="p">,</span> <span class="n">x_train_categorical</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_val_numerical</span><span class="p">,</span> <span class="n">x_val_categorical</span><span class="p">)</span>
            <span class="n">val_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">y_val_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">y_val</span><span class="p">)</span> <span class="o">*</span> <span class="n">exposure_val</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_val</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="n">val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>
            <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">val_loss</span>
            <span class="n">best_epoch</span> <span class="o">=</span> <span class="n">epoch</span>
            <span class="n">best_weights</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">best_weights</span><span class="p">,</span> <span class="s1">&#39;best_model.pt&#39;</span><span class="p">)</span>

            <span class="n">best_result</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;Epoch&quot;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
                <span class="s2">&quot;Train_Diff&quot;</span><span class="p">:</span> <span class="p">((</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_train</span> <span class="o">-</span> <span class="n">y_train</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>            
                <span class="s2">&quot;Train_Mean&quot;</span><span class="p">:</span> <span class="p">((</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> 
                <span class="s2">&quot;Train_Loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="s2">&quot;Val_mean&quot;</span><span class="p">:</span> <span class="p">((</span><span class="n">y_val_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_val</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_val</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> 
                <span class="s2">&quot;Val_loss&quot;</span><span class="p">:</span> <span class="n">val_loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="s2">&quot;Mean&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="o">*</span><span class="p">((</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_train</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.2</span><span class="o">*</span><span class="p">((</span><span class="n">y_val_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_val</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">exposure_val</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="p">}</span>
        
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">-</span> <span class="n">best_epoch</span> <span class="o">&gt;=</span> <span class="n">early_stopping_epochs</span><span class="p">:</span>
            <span class="n">results</span> <span class="o">+=</span> <span class="p">[</span><span class="n">best_result</span><span class="p">]</span>
            <span class="k">break</span>

    <span class="n">weight_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_weights</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Epoch</th>
      <th>Train_Diff</th>
      <th>Train_Mean</th>
      <th>Train_Loss</th>
      <th>Val_mean</th>
      <th>Val_loss</th>
      <th>Mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>122</td>
      <td>2.392140e-09</td>
      <td>0.102556</td>
      <td>0.324907</td>
      <td>0.102498</td>
      <td>0.328764</td>
      <td>0.102544</td>
    </tr>
    <tr>
      <th>1</th>
      <td>97</td>
      <td>-2.860957e-08</td>
      <td>0.102573</td>
      <td>0.325711</td>
      <td>0.102370</td>
      <td>0.329056</td>
      <td>0.102533</td>
    </tr>
    <tr>
      <th>2</th>
      <td>103</td>
      <td>-1.143877e-08</td>
      <td>0.102153</td>
      <td>0.324566</td>
      <td>0.102151</td>
      <td>0.332670</td>
      <td>0.102152</td>
    </tr>
    <tr>
      <th>3</th>
      <td>100</td>
      <td>2.330167e-08</td>
      <td>0.103220</td>
      <td>0.326741</td>
      <td>0.103121</td>
      <td>0.323224</td>
      <td>0.103200</td>
    </tr>
    <tr>
      <th>4</th>
      <td>114</td>
      <td>1.104944e-08</td>
      <td>0.102778</td>
      <td>0.326044</td>
      <td>0.103099</td>
      <td>0.326367</td>
      <td>0.102842</td>
    </tr>
    <tr>
      <th>5</th>
      <td>137</td>
      <td>-3.339519e-08</td>
      <td>0.102181</td>
      <td>0.324399</td>
      <td>0.102149</td>
      <td>0.333316</td>
      <td>0.102175</td>
    </tr>
    <tr>
      <th>6</th>
      <td>105</td>
      <td>5.588781e-08</td>
      <td>0.102945</td>
      <td>0.326395</td>
      <td>0.103071</td>
      <td>0.325628</td>
      <td>0.102970</td>
    </tr>
    <tr>
      <th>7</th>
      <td>151</td>
      <td>-4.614875e-08</td>
      <td>0.102484</td>
      <td>0.324409</td>
      <td>0.102470</td>
      <td>0.329613</td>
      <td>0.102481</td>
    </tr>
    <tr>
      <th>8</th>
      <td>97</td>
      <td>4.902244e-10</td>
      <td>0.102397</td>
      <td>0.325335</td>
      <td>0.101848</td>
      <td>0.329740</td>
      <td>0.102287</td>
    </tr>
    <tr>
      <th>9</th>
      <td>109</td>
      <td>-5.120140e-08</td>
      <td>0.102658</td>
      <td>0.325921</td>
      <td>0.102671</td>
      <td>0.328305</td>
      <td>0.102660</td>
    </tr>
    <tr>
      <th>10</th>
      <td>116</td>
      <td>-8.096723e-09</td>
      <td>0.103427</td>
      <td>0.326815</td>
      <td>0.103234</td>
      <td>0.321770</td>
      <td>0.103388</td>
    </tr>
    <tr>
      <th>11</th>
      <td>101</td>
      <td>-2.164153e-08</td>
      <td>0.102033</td>
      <td>0.324435</td>
      <td>0.101915</td>
      <td>0.333433</td>
      <td>0.102010</td>
    </tr>
    <tr>
      <th>12</th>
      <td>117</td>
      <td>-3.639235e-08</td>
      <td>0.102620</td>
      <td>0.325797</td>
      <td>0.102445</td>
      <td>0.328296</td>
      <td>0.102585</td>
    </tr>
    <tr>
      <th>13</th>
      <td>112</td>
      <td>-1.901544e-08</td>
      <td>0.102983</td>
      <td>0.326010</td>
      <td>0.103124</td>
      <td>0.324312</td>
      <td>0.103011</td>
    </tr>
    <tr>
      <th>14</th>
      <td>103</td>
      <td>-6.258782e-08</td>
      <td>0.102355</td>
      <td>0.324778</td>
      <td>0.102871</td>
      <td>0.330884</td>
      <td>0.102458</td>
    </tr>
    <tr>
      <th>15</th>
      <td>110</td>
      <td>3.401983e-08</td>
      <td>0.102748</td>
      <td>0.325920</td>
      <td>0.102963</td>
      <td>0.325666</td>
      <td>0.102791</td>
    </tr>
    <tr>
      <th>16</th>
      <td>111</td>
      <td>-1.676353e-08</td>
      <td>0.102496</td>
      <td>0.325475</td>
      <td>0.102318</td>
      <td>0.328534</td>
      <td>0.102460</td>
    </tr>
    <tr>
      <th>17</th>
      <td>91</td>
      <td>7.000872e-08</td>
      <td>0.102985</td>
      <td>0.326366</td>
      <td>0.103263</td>
      <td>0.324911</td>
      <td>0.103040</td>
    </tr>
    <tr>
      <th>18</th>
      <td>103</td>
      <td>-4.030646e-08</td>
      <td>0.102205</td>
      <td>0.324718</td>
      <td>0.102024</td>
      <td>0.331843</td>
      <td>0.102169</td>
    </tr>
    <tr>
      <th>19</th>
      <td>99</td>
      <td>4.145939e-08</td>
      <td>0.102938</td>
      <td>0.326770</td>
      <td>0.103006</td>
      <td>0.324792</td>
      <td>0.102952</td>
    </tr>
    <tr>
      <th>20</th>
      <td>97</td>
      <td>-1.510715e-08</td>
      <td>0.102897</td>
      <td>0.326182</td>
      <td>0.102669</td>
      <td>0.326991</td>
      <td>0.102852</td>
    </tr>
    <tr>
      <th>21</th>
      <td>131</td>
      <td>-5.966714e-08</td>
      <td>0.101933</td>
      <td>0.323772</td>
      <td>0.101948</td>
      <td>0.334276</td>
      <td>0.101936</td>
    </tr>
    <tr>
      <th>22</th>
      <td>120</td>
      <td>-2.709646e-08</td>
      <td>0.102465</td>
      <td>0.325010</td>
      <td>0.102429</td>
      <td>0.330330</td>
      <td>0.102458</td>
    </tr>
    <tr>
      <th>23</th>
      <td>105</td>
      <td>-2.102355e-08</td>
      <td>0.102337</td>
      <td>0.325117</td>
      <td>0.102071</td>
      <td>0.331128</td>
      <td>0.102284</td>
    </tr>
    <tr>
      <th>24</th>
      <td>108</td>
      <td>-4.034199e-08</td>
      <td>0.102668</td>
      <td>0.326064</td>
      <td>0.102556</td>
      <td>0.327703</td>
      <td>0.102646</td>
    </tr>
    <tr>
      <th>25</th>
      <td>119</td>
      <td>-2.227432e-08</td>
      <td>0.103057</td>
      <td>0.326187</td>
      <td>0.102746</td>
      <td>0.325391</td>
      <td>0.102994</td>
    </tr>
    <tr>
      <th>26</th>
      <td>101</td>
      <td>2.083051e-08</td>
      <td>0.102695</td>
      <td>0.326309</td>
      <td>0.102215</td>
      <td>0.327150</td>
      <td>0.102599</td>
    </tr>
    <tr>
      <th>27</th>
      <td>121</td>
      <td>1.541380e-08</td>
      <td>0.102893</td>
      <td>0.326008</td>
      <td>0.103202</td>
      <td>0.325928</td>
      <td>0.102955</td>
    </tr>
    <tr>
      <th>28</th>
      <td>113</td>
      <td>3.449376e-08</td>
      <td>0.102807</td>
      <td>0.325954</td>
      <td>0.102774</td>
      <td>0.325762</td>
      <td>0.102800</td>
    </tr>
    <tr>
      <th>29</th>
      <td>126</td>
      <td>2.174281e-08</td>
      <td>0.103070</td>
      <td>0.326702</td>
      <td>0.103266</td>
      <td>0.324199</td>
      <td>0.103109</td>
    </tr>
    <tr>
      <th>30</th>
      <td>117</td>
      <td>-1.656592e-08</td>
      <td>0.102850</td>
      <td>0.326272</td>
      <td>0.102875</td>
      <td>0.327013</td>
      <td>0.102855</td>
    </tr>
    <tr>
      <th>31</th>
      <td>129</td>
      <td>6.832808e-08</td>
      <td>0.102650</td>
      <td>0.325482</td>
      <td>0.102521</td>
      <td>0.327829</td>
      <td>0.102624</td>
    </tr>
    <tr>
      <th>32</th>
      <td>77</td>
      <td>-1.616476e-08</td>
      <td>0.102999</td>
      <td>0.326902</td>
      <td>0.103152</td>
      <td>0.326036</td>
      <td>0.103029</td>
    </tr>
    <tr>
      <th>33</th>
      <td>88</td>
      <td>5.655543e-09</td>
      <td>0.102623</td>
      <td>0.326193</td>
      <td>0.102884</td>
      <td>0.329003</td>
      <td>0.102675</td>
    </tr>
    <tr>
      <th>34</th>
      <td>162</td>
      <td>-2.356158e-09</td>
      <td>0.102348</td>
      <td>0.324156</td>
      <td>0.102377</td>
      <td>0.331616</td>
      <td>0.102353</td>
    </tr>
    <tr>
      <th>35</th>
      <td>86</td>
      <td>4.196311e-08</td>
      <td>0.102868</td>
      <td>0.327524</td>
      <td>0.103084</td>
      <td>0.326439</td>
      <td>0.102911</td>
    </tr>
    <tr>
      <th>36</th>
      <td>89</td>
      <td>-5.090045e-08</td>
      <td>0.101892</td>
      <td>0.324506</td>
      <td>0.102245</td>
      <td>0.334591</td>
      <td>0.101962</td>
    </tr>
    <tr>
      <th>37</th>
      <td>108</td>
      <td>2.277418e-08</td>
      <td>0.102124</td>
      <td>0.324321</td>
      <td>0.102444</td>
      <td>0.332908</td>
      <td>0.102188</td>
    </tr>
    <tr>
      <th>38</th>
      <td>116</td>
      <td>6.947347e-08</td>
      <td>0.102534</td>
      <td>0.325019</td>
      <td>0.102408</td>
      <td>0.329999</td>
      <td>0.102509</td>
    </tr>
    <tr>
      <th>39</th>
      <td>80</td>
      <td>7.592326e-08</td>
      <td>0.102538</td>
      <td>0.326241</td>
      <td>0.102415</td>
      <td>0.328567</td>
      <td>0.102514</td>
    </tr>
    <tr>
      <th>40</th>
      <td>111</td>
      <td>-1.581654e-08</td>
      <td>0.102781</td>
      <td>0.326168</td>
      <td>0.102739</td>
      <td>0.326581</td>
      <td>0.102772</td>
    </tr>
    <tr>
      <th>41</th>
      <td>98</td>
      <td>5.042758e-08</td>
      <td>0.102842</td>
      <td>0.326228</td>
      <td>0.102668</td>
      <td>0.326550</td>
      <td>0.102807</td>
    </tr>
    <tr>
      <th>42</th>
      <td>87</td>
      <td>8.698981e-08</td>
      <td>0.102296</td>
      <td>0.325354</td>
      <td>0.101990</td>
      <td>0.332758</td>
      <td>0.102235</td>
    </tr>
    <tr>
      <th>43</th>
      <td>123</td>
      <td>6.307235e-08</td>
      <td>0.103129</td>
      <td>0.326391</td>
      <td>0.102828</td>
      <td>0.323361</td>
      <td>0.103069</td>
    </tr>
    <tr>
      <th>44</th>
      <td>102</td>
      <td>2.011095e-08</td>
      <td>0.102592</td>
      <td>0.325649</td>
      <td>0.102498</td>
      <td>0.328519</td>
      <td>0.102573</td>
    </tr>
    <tr>
      <th>45</th>
      <td>133</td>
      <td>4.943590e-08</td>
      <td>0.103334</td>
      <td>0.326803</td>
      <td>0.103473</td>
      <td>0.321474</td>
      <td>0.103362</td>
    </tr>
    <tr>
      <th>46</th>
      <td>96</td>
      <td>5.013611e-08</td>
      <td>0.102532</td>
      <td>0.325522</td>
      <td>0.102558</td>
      <td>0.328560</td>
      <td>0.102537</td>
    </tr>
    <tr>
      <th>47</th>
      <td>148</td>
      <td>-5.638632e-08</td>
      <td>0.102877</td>
      <td>0.326021</td>
      <td>0.102527</td>
      <td>0.325705</td>
      <td>0.102807</td>
    </tr>
    <tr>
      <th>48</th>
      <td>95</td>
      <td>-4.618958e-08</td>
      <td>0.102155</td>
      <td>0.325058</td>
      <td>0.102273</td>
      <td>0.332594</td>
      <td>0.102179</td>
    </tr>
    <tr>
      <th>49</th>
      <td>131</td>
      <td>4.888754e-08</td>
      <td>0.102431</td>
      <td>0.325105</td>
      <td>0.102663</td>
      <td>0.329156</td>
      <td>0.102477</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>So as expected, <code class="docutils literal notranslate"><span class="pre">Train_diff</span></code> is now zero - the predicted mean for all our models is exactly the same as the training data mean, because we set it that way.</p>
<p>But do we lose any predictive power from this?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fixed_bias_val_loss</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">Val_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fixed_bias_val_loss</span><span class="p">,</span> <span class="n">avg_val_loss</span><span class="p">,</span> <span class="n">fixed_bias_val_loss</span> <span class="o">-</span> <span class="n">avg_val_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.32818478763103487 0.32853454232215884 -0.0003497546911239713
</pre></div>
</div>
</div>
</div>
<p>Let us also look at some diagnostics:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the best model in the last run</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;best_model.pt&#39;</span><span class="p">))</span>

<span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_val_numerical</span><span class="p">,</span> <span class="n">x_val_categorical</span><span class="p">)</span>

<span class="n">val_data</span><span class="p">[</span><span class="s2">&quot;predicted&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_val_pred</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">*</span> <span class="n">val_data</span><span class="o">.</span><span class="n">expo</span>
<span class="n">val_data</span><span class="p">[</span><span class="s2">&quot;trueclaims&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">val_data</span><span class="o">.</span><span class="n">truefreq</span> <span class="o">*</span> <span class="n">val_data</span><span class="o">.</span><span class="n">expo</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">numerical_feats</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;predicted&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">numerical_feats</span><span class="p">):</span>
    <span class="n">dat_copy</span> <span class="o">=</span> <span class="n">val_data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">dat_copy</span><span class="p">[</span><span class="s2">&quot;decile&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">qcut</span><span class="p">(</span><span class="n">dat_copy</span><span class="p">[</span><span class="n">f</span><span class="p">],</span> <span class="mi">10</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">duplicates</span><span class="o">=</span><span class="s1">&#39;drop&#39;</span><span class="p">)</span>
    <span class="n">X_sum</span> <span class="o">=</span> <span class="n">dat_copy</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;decile&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
        
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_sum</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">trueclaims</span> <span class="o">/</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">expo</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_sum</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">predicted</span> <span class="o">/</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">expo</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>


<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">categorical_feats</span><span class="p">):</span>
    <span class="n">dat_copy</span> <span class="o">=</span> <span class="n">val_data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">X_sum</span> <span class="o">=</span> <span class="n">dat_copy</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="s2">&quot;sum&quot;</span><span class="p">)[[</span><span class="s2">&quot;trueclaims&quot;</span><span class="p">,</span> <span class="s2">&quot;predicted&quot;</span><span class="p">,</span> <span class="s2">&quot;expo&quot;</span><span class="p">]]</span>
    <span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;trueclaims&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;trueclaims&quot;</span><span class="p">]</span><span class="o">/</span><span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;expo&quot;</span><span class="p">]</span>
    <span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;predicted&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;predicted&quot;</span><span class="p">]</span><span class="o">/</span><span class="n">X_sum</span><span class="p">[</span><span class="s2">&quot;expo&quot;</span><span class="p">]</span>
    <span class="n">X_sum</span> <span class="o">=</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s2">&quot;expo&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_sum</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">rot</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Value&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/84210ca7d2759ea44ee4a1d7949524b5dfaaf0ee2200c32af6a2026215ae7da0.png" src="../_images/84210ca7d2759ea44ee4a1d7949524b5dfaaf0ee2200c32af6a2026215ae7da0.png" />
<img alt="../_images/fb9b9373c37238873d79f881f924b8f2b68832261edc038fcbb9f03f819c4ddb.png" src="../_images/fb9b9373c37238873d79f881f924b8f2b68832261edc038fcbb9f03f819c4ddb.png" />
<img alt="../_images/2e745049b89cc4453b9b7d504427064a9eddc7f271a5b085d44796c3d42a0c63.png" src="../_images/2e745049b89cc4453b9b7d504427064a9eddc7f271a5b085d44796c3d42a0c63.png" />
<img alt="../_images/c7224545a889dc70a6195b9c9017287a551b27dc5bac9269347f3043bf4602c8.png" src="../_images/c7224545a889dc70a6195b9c9017287a551b27dc5bac9269347f3043bf4602c8.png" />
<img alt="../_images/ed936f38475fd89740848018b46f2cc81f3ab84d1c52b441c4a2eacc0fb90d96.png" src="../_images/ed936f38475fd89740848018b46f2cc81f3ab84d1c52b441c4a2eacc0fb90d96.png" />
</div>
</div>
</section>
<section id="conclusions">
<h2>Conclusions<a class="headerlink" href="#conclusions" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>ChatGPT is an effective tool for accelerating actuarial data science programming. However, users need to have sufficient understanding to debug the resulting generated code, and identify any subtle logic errors, additional requirements, or best practices.</p></li>
<li><p>Early stopping does lead to models that are biased. The bias itself is not biased, it averages around the mean, but any given model may be biased.</p></li>
</ul>
<p>We propose other ideas to avoid this other than the solution proposed by Wuthrich.</p>
<ul class="simple">
<li><p>Ensemble a group of models, which will average out biases.</p></li>
<li><p>Increase the learning rate on the bias to ensure that remains close to the right levels. Note that our initialisation strategy means the bias already starts at the right level.</p></li>
<li><p>Reset the bias so the mean reconciles after each epoch</p></li>
</ul>
</section>
<section id="appendix-unedited-chatgpt-code-output">
<h2>Appendix: Unedited ChatGPT code output<a class="headerlink" href="#appendix-unedited-chatgpt-code-output" title="Permalink to this heading">#</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Define the neural network class</span>
<span class="k">class</span> <span class="nc">FeedForwardNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_numerical_feats</span><span class="p">,</span> <span class="n">num_categorical_feats</span><span class="p">,</span> <span class="n">embedding_sizes</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FeedForwardNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">emb_size</span> <span class="ow">in</span> <span class="n">embedding_sizes</span>
        <span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_numerical_feats</span> <span class="o">=</span> <span class="n">num_numerical_feats</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_categorical_feats</span> <span class="o">=</span> <span class="n">num_categorical_feats</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_embed_size</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">emb_size</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">emb_size</span> <span class="ow">in</span> <span class="n">embedding_sizes</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_numerical_feats</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_embed_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_numerical</span><span class="p">,</span> <span class="n">x_categorical</span><span class="p">):</span>
        <span class="n">embedded_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">embedding</span><span class="p">(</span><span class="n">x_categorical</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">)]</span>
        <span class="n">embedded_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">embedded_x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">embedded_x</span><span class="p">,</span> <span class="n">x_numerical</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Load the dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;your_dataset.csv&#39;</span><span class="p">)</span>

<span class="c1"># Split the dataset into training and validation sets</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Define the categorical features and numerical features</span>
<span class="n">categorical_feats</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;gas&#39;</span><span class="p">,</span> <span class="s1">&#39;brand&#39;</span><span class="p">,</span> <span class="s1">&#39;area&#39;</span><span class="p">,</span> <span class="s1">&#39;ct&#39;</span><span class="p">]</span>
<span class="n">numerical_feats</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;ac&#39;</span><span class="p">,</span> <span class="s1">&#39;power&#39;</span><span class="p">,</span> <span class="s1">&#39;dens&#39;</span><span class="p">]</span>

<span class="c1"># Encode the categorical features using LabelEncoder</span>
<span class="n">label_encoders</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">categorical_feats</span><span class="p">:</span>
    <span class="n">label_encoders</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
    <span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoders</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">])</span>
    <span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoders</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">feat</span><span class="p">])</span>

<span class="c1"># Define the embedding sizes for categorical features</span>
<span class="n">embedding_sizes</span> <span class="o">=</span> <span class="p">[(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()),</span> <span class="nb">min</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span> <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">categorical_feats</span><span class="p">]</span>

<span class="c1"># Convert the dataset to PyTorch tensors</span>
<span class="n">x_train_numerical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">numerical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x_train_categorical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">categorical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;claims&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">exposure_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">x_val_numerical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">numerical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x_val_categorical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="n">categorical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="s1">&#39;claims&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">exposure_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Define the hyperparameters</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">early_stopping_epochs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Create an instance of the FeedForwardNet model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FeedForwardNet</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">numerical_feats</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">categorical_feats</span><span class="p">),</span> <span class="n">embedding_sizes</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

<span class="c1"># Define the loss function</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>

<span class="c1"># Define the optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># Train the model</span>
<span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
<span class="n">best_epoch</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train_numerical</span><span class="p">,</span> <span class="n">x_train_categorical</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">*</span> <span class="n">exposure_train</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_val_numerical</span><span class="p">,</span> <span class="n">x_val_categorical</span><span class="p">)</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_val_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">*</span> <span class="n">exposure_val</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>
        <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">val_loss</span>
        <span class="n">best_epoch</span> <span class="o">=</span> <span class="n">epoch</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;best_model.pt&#39;</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">-</span> <span class="n">best_epoch</span> <span class="o">&gt;=</span> <span class="n">early_stopping_epochs</span><span class="p">:</span>
        <span class="k">break</span>

<span class="c1"># Load the best model</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;best_model.pt&#39;</span><span class="p">))</span>

<span class="c1"># Make predictions on the test set</span>
<span class="n">x_test_numerical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="n">numerical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x_test_categorical</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="n">categorical_feats</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="s1">&#39;claims&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">exposure_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="s1">&#39;expo&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_test_numerical</span><span class="p">,</span> <span class="n">x_test_categorical</span><span class="p">)</span>

<span class="c1"># Calculate the test loss</span>
<span class="n">test_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_test_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">*</span> <span class="n">exposure_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">*</span> <span class="n">exposure_test</span><span class="p">)</span>
<span class="n">test_rmse</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">test_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Test RMSE: </span><span class="si">{</span><span class="n">test_rmse</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "ActuariesInstitute/cookbook",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="py_shap_values.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Py: Explainable Models with SHAP</p>
      </div>
    </a>
    <a class="right-next"
       href="LifeRecipeBook.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">R: Life Modelling Recipes</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chatgpt-prompt">ChatGPT prompt:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#response">Response:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-on-bias">Experiment on Bias</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-the-bias-issue">Solving the bias issue</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learn-rates">Learn Rates</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#just-change-the-bias">Just change the bias</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusions">Conclusions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-unedited-chatgpt-code-output">Appendix: Unedited ChatGPT code output</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By YDAWG, DAPC, YAC and other contributors.
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
   The Actuaries' Analytical Cookbook is a series of data and analytics recipes to help actuaries quickly get started with a new project.   This site is intended to be a resource to actuaries in both data science and traditional fields.  Opinions expressed in this publication are the opinions of contributors and do not necessarily represent those of either the Institute of Actuaries of Australia (the ‘Institute’), its members, directors, officers, employees, agents, or that of the employers of the contributors. <br>© Institute of Actuaries of Australia and Contributors 2021. All rights reserved.
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>