
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Py: Customer Churn Classification &#8212; Actuaries&#39; Analytical Cookbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Py/R: Multitasking Risk Pricing Using Deep Learning" href="multitasking_risk_pricing.html" />
    <link rel="prev" title="Version control" href="version_control.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/actuaries-logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Actuaries' Analytical Cookbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Python
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="about_py.html">
   About Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="getting_started.html">
   Setting up Your Python Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="python_by_example.html">
   An Introductory Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="learn_more.html">
   Learn More
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Useful_Python_packages.html">
   Useful Python packages for Data Science
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to R
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="about_R.html">
   About R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="getting_started_R.html">
   Setting Up R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="introductory_R.html">
   Introduction to R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="intermediate_R.html">
   Next Steps With R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="top_ten_r_packages.html">
   Useful Packages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_R_DataTable.html">
   R: data.table for actuaries
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Workflow Management
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="version_control.html">
   Version control
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Regression, Classification and Technical Price
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Py: Customer Churn Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multitasking_risk_pricing.html">
   Py/R: Multitasking Risk Pricing Using Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="py_shap_values.html">
   Py: Explainable Models with SHAP
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Life Insurance
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="LifeRecipeBook.html">
   R: Life Modelling Recipes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="life_stats.html">
   R: Life Industry Stats in Tableau and R
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  General Insurance
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_R_GLMs.html">
   R: Reserving with GLMs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_R_Lasso.html">
   R: Reserving with LASSO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_R_mlr3.html">
   R: Machine Learning Triangles
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MLRWP_Py_triangles_example.html">
   Py: Machine Learning Triangles
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unsupervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M06_CS1.html">
   Py: Socio-Economic Index Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M06_CS2.html">
   Py: Clustering Credit Card Fraud
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M06_Ex4.html">
   Py: K-means clustering of COVID dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M06_Ex5.html">
   Py: Hierarchical clustering on COVID dataset
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Natural Language Processing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="R_case_study_word_cloud.html">
   R: Word Cloud Case Study
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="textClassificationEntry.html">
   Py: Text Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M05_Ex10.html">
   Py: Decision Tree Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M05_Ex18.html">
   Py: Neural Net Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M07_CS1.html">
   Py: Classifying review sentiment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M07_CS2.html">
   Py: Customer Sentiment Analysis
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Business Optimization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_2021_S2_Tutorial10_exercise_scipy.html">
   Py: Linear Programming
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Image Recognition
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="DAA_M05_CS2.html">
   Py: Image Recognition
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Ethics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="automated_decision.html">
   Automated Decision-Making Systems
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Contributing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="contributing.html">
   Contributing
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/DAA_M05_CS1.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/ActuariesInstitute/cookbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        <a class="edit-button" href="https://github.com/ActuariesInstitute/cookbook/edit/main/cookbook/docs/DAA_M05_CS1.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/ActuariesInstitute/cookbook/main?urlpath=tree/cookbook/docs/DAA_M05_CS1.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/ActuariesInstitute/cookbook/blob/main/cookbook/docs/DAA_M05_CS1.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#define-the-problem">
   Define the Problem:
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#purpose">
   Purpose:
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References:
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#packages">
   Packages
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#functions">
   Functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data">
   Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#import-data">
     Import data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#explore-data-eda">
     Explore data (EDA)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prepare-data">
     Prepare data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modelling">
   Modelling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-boosting-machine-gbm">
     Gradient Boosting Machine (GBM)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Prepare data
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fit-initial-gbm-gbm-1">
       Fit initial GBM (GBM 1)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#evaluate-gbm-1">
       Evaluate GBM 1
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#estimate-feature-importance">
       Estimate feature importance
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#examine-effect-of-features">
       Examine effect of features
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#improve-the-model-gbm-2">
       Improve the model (GBM 2)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#evaluate-gbm-2">
       Evaluate GBM 2
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#select-final-model-gbm-final">
       Select final model (GBM final)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simple-neural-networks-built-from-first-principles">
     Simple neural networks built from first principles
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       Prepare data
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fit-a-single-layer-neural-network-nn-1">
       Fit a single layer neural network (NN 1)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#evaluate-nn-1">
       Evaluate NN 1
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fit-a-multi-layer-neural-network-nn-2">
       Fit a multi-layer neural network (NN 2)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#evaluate-nn-2">
       Evaluate NN 2
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-networks-using-keras">
     Neural networks using Keras
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fit-a-single-layer-neural-network-with-keras-nn-3">
       Fit a single layer neural network with Keras (NN 3)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#evaluate-nn-3">
       Evaluate NN 3
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fit-a-more-complex-model-with-keras">
       Fit a more complex model with Keras
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#evaluate-nn-4">
       Evaluate NN 4
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation-and-observations">
   Evaluation and observations
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Py: Customer Churn Classification</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#define-the-problem">
   Define the Problem:
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#purpose">
   Purpose:
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References:
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#packages">
   Packages
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#functions">
   Functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data">
   Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#import-data">
     Import data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#explore-data-eda">
     Explore data (EDA)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prepare-data">
     Prepare data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modelling">
   Modelling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-boosting-machine-gbm">
     Gradient Boosting Machine (GBM)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Prepare data
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fit-initial-gbm-gbm-1">
       Fit initial GBM (GBM 1)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#evaluate-gbm-1">
       Evaluate GBM 1
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#estimate-feature-importance">
       Estimate feature importance
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#examine-effect-of-features">
       Examine effect of features
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#improve-the-model-gbm-2">
       Improve the model (GBM 2)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#evaluate-gbm-2">
       Evaluate GBM 2
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#select-final-model-gbm-final">
       Select final model (GBM final)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simple-neural-networks-built-from-first-principles">
     Simple neural networks built from first principles
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       Prepare data
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fit-a-single-layer-neural-network-nn-1">
       Fit a single layer neural network (NN 1)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#evaluate-nn-1">
       Evaluate NN 1
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fit-a-multi-layer-neural-network-nn-2">
       Fit a multi-layer neural network (NN 2)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#evaluate-nn-2">
       Evaluate NN 2
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-networks-using-keras">
     Neural networks using Keras
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fit-a-single-layer-neural-network-with-keras-nn-3">
       Fit a single layer neural network with Keras (NN 3)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#evaluate-nn-3">
       Evaluate NN 3
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fit-a-more-complex-model-with-keras">
       Fit a more complex model with Keras
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#evaluate-nn-4">
       Evaluate NN 4
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation-and-observations">
   Evaluation and observations
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="py-customer-churn-classification">
<h1>Py: Customer Churn Classification<a class="headerlink" href="#py-customer-churn-classification" title="Permalink to this headline">¶</a></h1>
<p><strong>This notebook was originally created by Josh Jaroudy for the Data Analytics Applications subject, as <em>Case Study 1</em> in the DAA M05 Classification and neural networks module.</strong></p>
<p><strong>Data Analytics Applications is a Fellowship Applications (Module 3) subject with the Actuaries Institute that aims to teach students how to apply a range of data analytics skills, such as neural networks, natural language processing, unsupervised learning and optimisation techniques, together with their professional judgement, to solve a variety of complex and challenging business problems. The business problems used as examples in this subject are drawn from a wide range of industries.</strong></p>
<p><strong>Find out more about the course <a class="reference external" href="https://www.actuaries.asn.au/education-program/fellowship/subjects-and-syllabus/data-analytics-applications-subject">here</a>.</strong></p>
<div class="section" id="define-the-problem">
<h2>Define the Problem:<a class="headerlink" href="#define-the-problem" title="Permalink to this headline">¶</a></h2>
<p>Customer churn, also known as customer attrition, customer turnover or customer defection, is the loss of clients or customers. For many businesses, a high level of customer churn can negatively impact their profits, particularly because it is often quite costly for a business to acquire new customers.</p>
<p>For this reason, many businesses like to understand which customers are likely to churn in a given period. Armed with this information, businesses can employ different strategies to try to retain their customers.
This case study investigates the use of neural networks and gradient boosting machines for predicting which customers are likely to churn.</p>
<p>When trying to predict customer churn, it may seem like a relatively straightforward task to obtain some past customer data and use this to determine whether future customers will churn. However, the task of deciding exactly what the output of such a prediction model should be is quite complex, and heavily dependent on how the model will be used by the business.</p>
</div>
<div class="section" id="purpose">
<h2>Purpose:<a class="headerlink" href="#purpose" title="Permalink to this headline">¶</a></h2>
<p>This notebook investigates the use of neural networks and gradient boosting machines for predicting which customers are likely to churn. This code is used in Case Study 1 in Module 5.</p>
</div>
<div class="section" id="references">
<h2>References:<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>The dataset used in this notebook was sourced from a Kaggle competition  that aimed to predict customer churn behaviour for a telecommunications provider: <a class="reference external" href="https://www.kaggle.com/blastchar/telco-customer-churn">https://www.kaggle.com/blastchar/telco-customer-churn</a>.</p>
<p>This dataset contains 7,043 rows (one for each customer) and 21 features, including information about each customer’s:</p>
<ul class="simple">
<li><p>services with the company, such as phone, internet, online security, online backup, device protection, tech support, and streaming of TV and movies;</p></li>
<li><p>account information, such as how long they have been a customer, contract, payment method, paperless billing, monthly charges and total charges; and</p></li>
<li><p>demographic information, such as gender, age range, and whether they have a partner and dependents.</p></li>
</ul>
<p>The response variable in the dataset is labelled ‘Churn’. It represents whether each customer left the service provider in the month preceding the data extract date.</p>
</div>
<div class="section" id="packages">
<h2>Packages<a class="headerlink" href="#packages" title="Permalink to this headline">¶</a></h2>
<p>This section imports the packages that will be required for this exercise/case study.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> <span class="c1"># Pandas is used for data management.</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> <span class="c1"># Numpy is used for mathematical operations.</span>

<span class="c1"># Matplotlib and Seaborn are used for plotting.</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.image</span> <span class="k">as</span> <span class="nn">mpimg</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">itertools</span> <span class="c1"># Used in the confusion matrix function</span>

<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span> <span class="c1"># Keras, from the Tensorflow package is used for</span>
                             <span class="c1"># building the neural networks.</span>

<span class="c1"># The various functions below from the Scikit-learn package help with</span>
<span class="c1"># modelling and diagnostics.</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span><span class="p">,</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">log_loss</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span> <span class="c1"># For building the GBM</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">auc</span><span class="p">,</span> <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">plot_partial_dependence</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">BatchNormalization</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.wrappers.scikit_learn</span> <span class="kn">import</span> <span class="n">KerasRegressor</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="functions">
<h2>Functions<a class="headerlink" href="#functions" title="Permalink to this headline">¶</a></h2>
<p>The section below defines some general functions that are used in this notebook.
Other functions that are specific to each type of model are defined in the section for that model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a function to split the data into train, validation and test sets.</span>
<span class="c1"># This uses the `train_test_split` function from the sklearn package to do the </span>
<span class="c1"># actual data splitting.</span>
<span class="k">def</span> <span class="nf">create_data_splits</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">id_col</span><span class="p">,</span> <span class="n">response_col</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Splits the data into train, validation and test sets (64%, 16%, 20%)</span>
<span class="sd">    All columns on `dataset` other than the `id_col` and `response_col` will be</span>
<span class="sd">    used as features.</span>
<span class="sd">    Params:</span>
<span class="sd">    dataset: input dataset as a pandas data frame</span>
<span class="sd">    id_col: (str) the name of the column containing the unique row identifier</span>
<span class="sd">    response_col: (str) the name of the response column</span>
<span class="sd">    Returns:</span>
<span class="sd">    train_x: the training data (feature) matrix</span>
<span class="sd">    train_y: the training data response vector</span>
<span class="sd">    validation_x: the validation data (feature) matrix</span>
<span class="sd">    validation_y: the validation data response vector</span>
<span class="sd">    test_x: the test data (feature) matrix</span>
<span class="sd">    test_y: the test data response vector</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Split data into train/test (80%, 20%).</span>
    <span class="n">train_full</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">123</span><span class="p">)</span>

    <span class="c1"># Create a validation set from the training data (20%).</span>
    <span class="n">train</span><span class="p">,</span> <span class="n">validation</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">train_full</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">234</span><span class="p">)</span>

    <span class="c1"># Create train and validation data feature matrices and response vectors</span>
    <span class="c1"># For the response vectors, convert Churn Yes/No to 1/0</span>

    <span class="n">feature_cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">id_col</span> <span class="o">+</span> <span class="n">response_col</span><span class="p">]</span>

    <span class="n">train_x</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">feature_cols</span><span class="p">]</span>
    <span class="n">train_y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">response_col</span><span class="p">]</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="s1">&#39;Yes&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">validation_x</span> <span class="o">=</span> <span class="n">validation</span><span class="p">[</span><span class="n">feature_cols</span><span class="p">]</span>
    <span class="n">validation_y</span> <span class="o">=</span> <span class="n">validation</span><span class="p">[</span><span class="n">response_col</span><span class="p">]</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="s1">&#39;Yes&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">test_x</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">feature_cols</span><span class="p">]</span>
    <span class="n">test_y</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">response_col</span><span class="p">]</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="s1">&#39;Yes&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">validation_x</span><span class="p">,</span> <span class="n">validation_y</span><span class="p">,</span> <span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a function to print and plot a confusion matrix.</span>
<span class="k">def</span> <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span>
                          <span class="n">normalise</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Confusion matrix&#39;</span><span class="p">,</span>
                          <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    This function prints and plots a confusion matrix.</span>
<span class="sd">    Normalisation of the matrix can be applied by setting `normalise=True`.</span>
<span class="sd">    Normalsiation ensures that the sum of each row in the confusion matrix is 1.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">tick_marks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">normalise</span><span class="p">:</span>
        <span class="n">cm</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

    <span class="n">thresh</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">/</span> <span class="mf">2.</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span>
                 <span class="n">horizontalalignment</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
                 <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span> <span class="k">if</span> <span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">thresh</span> <span class="k">else</span> <span class="s1">&#39;black&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True response&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted response&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="data">
<h2>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h2>
<p>This section:</p>
<ul class="simple">
<li><p>imports the data that will be used in the modelling;</p></li>
<li><p>explores the data; and</p></li>
<li><p>prepares the data for modelling.</p></li>
</ul>
<div class="section" id="import-data">
<h3>Import data<a class="headerlink" href="#import-data" title="Permalink to this headline">¶</a></h3>
<p>The below code will read it into a pandas data frame.</p>
<p>We read directly from a URL, but pandas can also read from a file.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="s1">&#39;https://actuariesinstitute.github.io/cookbook/_static/daa_datasets/DAA_M05_CS1_data.csv&#39;</span><span class="p">,</span> 
    <span class="n">header</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="explore-data-eda">
<h3>Explore data (EDA)<a class="headerlink" href="#explore-data-eda" title="Permalink to this headline">¶</a></h3>
<p>Prior to commencing modelling, it is always a good idea to look at the data to get an understanding of the:</p>
<ul class="simple">
<li><p>available features;</p></li>
<li><p>the data types of the features (numeric, categorical, dates, etc.);</p></li>
<li><p>the distribution and missingness of the features;</p></li>
<li><p>correlations between features; and</p></li>
<li><p>relationships between features and the response variable.</p></li>
</ul>
<p>The code below looks at some of these components of the Telco dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check the available features, their data types and their missingness.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">info</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 7043 entries, 0 to 7042
Data columns (total 21 columns):
 #   Column            Non-Null Count  Dtype  
---  ------            --------------  -----  
 0   customerID        7043 non-null   object 
 1   gender            7043 non-null   object 
 2   SeniorCitizen     7043 non-null   int64  
 3   Partner           7043 non-null   object 
 4   Dependents        7043 non-null   object 
 5   tenure            7043 non-null   int64  
 6   PhoneService      7043 non-null   object 
 7   MultipleLines     7043 non-null   object 
 8   InternetService   7043 non-null   object 
 9   OnlineSecurity    7043 non-null   object 
 10  OnlineBackup      7043 non-null   object 
 11  DeviceProtection  7043 non-null   object 
 12  TechSupport       7043 non-null   object 
 13  StreamingTV       7043 non-null   object 
 14  StreamingMovies   7043 non-null   object 
 15  Contract          7043 non-null   object 
 16  PaperlessBilling  7043 non-null   object 
 17  PaymentMethod     7043 non-null   object 
 18  MonthlyCharges    7043 non-null   float64
 19  TotalCharges      7043 non-null   object 
 20  Churn             7043 non-null   object 
dtypes: float64(1), int64(2), object(18)
memory usage: 1.1+ MB
None
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check the number of unique values for each feature.</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>customerID          7043
gender                 2
SeniorCitizen          2
Partner                2
Dependents             2
tenure                73
PhoneService           2
MultipleLines          3
InternetService        3
OnlineSecurity         3
OnlineBackup           3
DeviceProtection       3
TechSupport            3
StreamingTV            3
StreamingMovies        3
Contract               3
PaperlessBilling       2
PaymentMethod          4
MonthlyCharges      1585
TotalCharges        6531
Churn                  2
dtype: int64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print out the first 5 observations in the data.</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>customerID</th>
      <th>gender</th>
      <th>SeniorCitizen</th>
      <th>Partner</th>
      <th>Dependents</th>
      <th>tenure</th>
      <th>PhoneService</th>
      <th>MultipleLines</th>
      <th>InternetService</th>
      <th>OnlineSecurity</th>
      <th>...</th>
      <th>DeviceProtection</th>
      <th>TechSupport</th>
      <th>StreamingTV</th>
      <th>StreamingMovies</th>
      <th>Contract</th>
      <th>PaperlessBilling</th>
      <th>PaymentMethod</th>
      <th>MonthlyCharges</th>
      <th>TotalCharges</th>
      <th>Churn</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7590-VHVEG</td>
      <td>Female</td>
      <td>0</td>
      <td>Yes</td>
      <td>No</td>
      <td>1</td>
      <td>No</td>
      <td>No phone service</td>
      <td>DSL</td>
      <td>No</td>
      <td>...</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Month-to-month</td>
      <td>Yes</td>
      <td>Electronic check</td>
      <td>29.85</td>
      <td>29.85</td>
      <td>No</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5575-GNVDE</td>
      <td>Male</td>
      <td>0</td>
      <td>No</td>
      <td>No</td>
      <td>34</td>
      <td>Yes</td>
      <td>No</td>
      <td>DSL</td>
      <td>Yes</td>
      <td>...</td>
      <td>Yes</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>One year</td>
      <td>No</td>
      <td>Mailed check</td>
      <td>56.95</td>
      <td>1889.5</td>
      <td>No</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3668-QPYBK</td>
      <td>Male</td>
      <td>0</td>
      <td>No</td>
      <td>No</td>
      <td>2</td>
      <td>Yes</td>
      <td>No</td>
      <td>DSL</td>
      <td>Yes</td>
      <td>...</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Month-to-month</td>
      <td>Yes</td>
      <td>Mailed check</td>
      <td>53.85</td>
      <td>108.15</td>
      <td>Yes</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7795-CFOCW</td>
      <td>Male</td>
      <td>0</td>
      <td>No</td>
      <td>No</td>
      <td>45</td>
      <td>No</td>
      <td>No phone service</td>
      <td>DSL</td>
      <td>Yes</td>
      <td>...</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>No</td>
      <td>No</td>
      <td>One year</td>
      <td>No</td>
      <td>Bank transfer (automatic)</td>
      <td>42.30</td>
      <td>1840.75</td>
      <td>No</td>
    </tr>
    <tr>
      <th>4</th>
      <td>9237-HQITU</td>
      <td>Female</td>
      <td>0</td>
      <td>No</td>
      <td>No</td>
      <td>2</td>
      <td>Yes</td>
      <td>No</td>
      <td>Fiber optic</td>
      <td>No</td>
      <td>...</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>Month-to-month</td>
      <td>Yes</td>
      <td>Electronic check</td>
      <td>70.70</td>
      <td>151.65</td>
      <td>Yes</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div></div></div>
</div>
</div>
<div class="section" id="prepare-data">
<h3>Prepare data<a class="headerlink" href="#prepare-data" title="Permalink to this headline">¶</a></h3>
<p>Some data preparation is needed before the modelling can begin.</p>
<p>From the summaries in the EDA section above you can see that:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">customerID</span></code> is the unique identifier for each observation;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Churn</span></code> is the response and takes values ‘Yes’ and ‘No’ with a ‘Yes’ rate of 26.5% (= 1,869/(5,174+1,869));</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Tenure</span></code>, <code class="docutils literal notranslate"><span class="pre">MonthlyCharges</span></code> and <code class="docutils literal notranslate"><span class="pre">TotalCharges</span></code> are numeric features;</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">TotalCharges</span></code> is stored as categorical;</p></li>
</ul>
</li>
<li><p>all other features are categorical (though many with only 2 levels); and</p></li>
<li><p>missing values are not a significant concern.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the ID and response columns</span>
<span class="n">id_col</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;customerID&#39;</span><span class="p">]</span>
<span class="n">response_col</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Churn&#39;</span><span class="p">]</span>

<span class="c1"># Get the list of features by type.</span>
<span class="c1"># Categorical features can be identified as those columns with only a few levels.</span>
<span class="c1"># This code selects the list of features with &lt; 6 levels and puts the names </span>
<span class="c1"># into a list, excluding the id_col and response_col.</span>
<span class="n">cat_cols</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">nunique</span><span class="p">()[</span><span class="n">dataset</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">6</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">cat_cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">cat_cols</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">id_col</span> <span class="o">+</span> <span class="n">response_col</span><span class="p">]</span>

<span class="c1"># Numerical features are left after the categorical features have been removed.</span>
<span class="c1"># List comprehension is used below to select the set of feature names not</span>
<span class="c1"># contained in the cat_cols, id_col, or response_col lists.</span>
<span class="n">num_cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">cat_cols</span> <span class="o">+</span> <span class="n">id_col</span> <span class="o">+</span> <span class="n">response_col</span><span class="p">]</span>

<span class="c1"># Convert TotalCharges to numeric and set equal to 0 where blank.</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;TotalCharges&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39; &#39;</span><span class="p">,</span><span class="s1">&#39;TotalCharges&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;TotalCharges&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;TotalCharges&#39;</span><span class="p">])</span>

<span class="c1"># Check the number of levels for each categorical feature and the</span>
<span class="c1"># response variable.</span>
<span class="k">for</span> <span class="n">cat_col</span> <span class="ow">in</span> <span class="n">cat_cols</span> <span class="o">+</span> <span class="n">response_col</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">cat_col</span><span class="p">,</span> <span class="n">dataset</span><span class="p">[</span><span class="n">cat_col</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>gender [&#39;Female&#39; &#39;Male&#39;]
SeniorCitizen [0 1]
Partner [&#39;Yes&#39; &#39;No&#39;]
Dependents [&#39;No&#39; &#39;Yes&#39;]
PhoneService [&#39;No&#39; &#39;Yes&#39;]
MultipleLines [&#39;No phone service&#39; &#39;No&#39; &#39;Yes&#39;]
InternetService [&#39;DSL&#39; &#39;Fiber optic&#39; &#39;No&#39;]
OnlineSecurity [&#39;No&#39; &#39;Yes&#39; &#39;No internet service&#39;]
OnlineBackup [&#39;Yes&#39; &#39;No&#39; &#39;No internet service&#39;]
DeviceProtection [&#39;No&#39; &#39;Yes&#39; &#39;No internet service&#39;]
TechSupport [&#39;No&#39; &#39;Yes&#39; &#39;No internet service&#39;]
StreamingTV [&#39;No&#39; &#39;Yes&#39; &#39;No internet service&#39;]
StreamingMovies [&#39;No&#39; &#39;Yes&#39; &#39;No internet service&#39;]
Contract [&#39;Month-to-month&#39; &#39;One year&#39; &#39;Two year&#39;]
PaperlessBilling [&#39;Yes&#39; &#39;No&#39;]
PaymentMethod [&#39;Electronic check&#39; &#39;Mailed check&#39; &#39;Bank transfer (automatic)&#39;
 &#39;Credit card (automatic)&#39;]
Churn [&#39;No&#39; &#39;Yes&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check the updated feature types</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">dtypes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>customerID           object
gender               object
SeniorCitizen         int64
Partner              object
Dependents           object
tenure                int64
PhoneService         object
MultipleLines        object
InternetService      object
OnlineSecurity       object
OnlineBackup         object
DeviceProtection     object
TechSupport          object
StreamingTV          object
StreamingMovies      object
Contract             object
PaperlessBilling     object
PaymentMethod        object
MonthlyCharges      float64
TotalCharges        float64
Churn                object
dtype: object
</pre></div>
</div>
</div>
</div>
<p>Now that the data has been cleaned up, the marginal relationship between the features and the response can be analysed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the mean churn rate by each of the candidate features.</span>

<span class="c1"># Loop over each feature.</span>
<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="p">[</span>
    <span class="s1">&#39;gender&#39;</span><span class="p">,</span>
    <span class="s1">&#39;SeniorCitizen&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Partner&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Dependents&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tenure&#39;</span><span class="p">,</span>
    <span class="s1">&#39;PhoneService&#39;</span><span class="p">,</span>
    <span class="s1">&#39;MultipleLines&#39;</span><span class="p">,</span>
    <span class="s1">&#39;InternetService&#39;</span><span class="p">,</span>
    <span class="s1">&#39;OnlineSecurity&#39;</span><span class="p">,</span>
    <span class="s1">&#39;OnlineBackup&#39;</span><span class="p">,</span>
    <span class="s1">&#39;DeviceProtection&#39;</span><span class="p">,</span>
    <span class="s1">&#39;TechSupport&#39;</span><span class="p">,</span>
    <span class="s1">&#39;StreamingTV&#39;</span><span class="p">,</span>
    <span class="s1">&#39;StreamingMovies&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Contract&#39;</span><span class="p">,</span>
    <span class="s1">&#39;PaperlessBilling&#39;</span><span class="p">,</span>
    <span class="s1">&#39;PaymentMethod&#39;</span><span class="p">,</span>
<span class="p">]:</span>
    <span class="p">(</span>
        <span class="c1"># create a binary 1/0 response column, where 1 indicates Churn = &#39;Yes&#39;;</span>
        <span class="c1"># group by the values of the current feature;</span>
        <span class="c1"># calculate the mean of the response; and</span>
        <span class="c1"># plot.</span>
        <span class="n">dataset</span><span class="p">[[</span><span class="n">feature</span><span class="p">]]</span>
        <span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">Response</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">Churn</span> <span class="o">==</span> <span class="s1">&#39;Yes&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
        <span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
        <span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;Dodgerblue&#39;</span><span class="p">)</span>
    <span class="p">)</span>


<span class="p">(</span>
    <span class="c1"># The same is done for MonthlyCharges except that the value is rounded</span>
    <span class="c1"># to the nearest $10 using np.round(, -1).</span>
    <span class="n">dataset</span><span class="p">[[</span><span class="s1">&#39;MonthlyCharges&#39;</span><span class="p">]]</span>
    <span class="o">.</span><span class="n">assign</span><span class="p">(</span>
        <span class="n">Feature_MonthlyCharges</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;MonthlyCharges&#39;</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">Response</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">Churn</span> <span class="o">==</span> <span class="s1">&#39;Yes&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;MonthlyCharges&#39;</span><span class="p">])</span>
    <span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;Feature_MonthlyCharges&#39;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;Dodgerblue&#39;</span><span class="p">)</span>
<span class="p">)</span>


<span class="p">(</span>
    <span class="c1"># TotalCharges is rounded to the nearest $1,000 using np.round(, -3).</span>
    <span class="n">dataset</span><span class="p">[[</span><span class="s1">&#39;TotalCharges&#39;</span><span class="p">]]</span>
    <span class="o">.</span><span class="n">assign</span><span class="p">(</span>
        <span class="n">Feature_TotalCharges</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;TotalCharges&#39;</span><span class="p">],</span> <span class="o">-</span><span class="mi">3</span><span class="p">),</span>
        <span class="n">Response</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">Churn</span> <span class="o">==</span> <span class="s1">&#39;Yes&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;TotalCharges&#39;</span><span class="p">])</span>
    <span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;Feature_TotalCharges&#39;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;Dodgerblue&#39;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:xlabel=&#39;Feature_TotalCharges&#39;&gt;
</pre></div>
</div>
<img alt="../_images/DAA_M05_CS1_19_1.png" src="../_images/DAA_M05_CS1_19_1.png" />
<img alt="../_images/DAA_M05_CS1_19_2.png" src="../_images/DAA_M05_CS1_19_2.png" />
<img alt="../_images/DAA_M05_CS1_19_3.png" src="../_images/DAA_M05_CS1_19_3.png" />
<img alt="../_images/DAA_M05_CS1_19_4.png" src="../_images/DAA_M05_CS1_19_4.png" />
<img alt="../_images/DAA_M05_CS1_19_5.png" src="../_images/DAA_M05_CS1_19_5.png" />
<img alt="../_images/DAA_M05_CS1_19_6.png" src="../_images/DAA_M05_CS1_19_6.png" />
<img alt="../_images/DAA_M05_CS1_19_7.png" src="../_images/DAA_M05_CS1_19_7.png" />
<img alt="../_images/DAA_M05_CS1_19_8.png" src="../_images/DAA_M05_CS1_19_8.png" />
<img alt="../_images/DAA_M05_CS1_19_9.png" src="../_images/DAA_M05_CS1_19_9.png" />
<img alt="../_images/DAA_M05_CS1_19_10.png" src="../_images/DAA_M05_CS1_19_10.png" />
<img alt="../_images/DAA_M05_CS1_19_11.png" src="../_images/DAA_M05_CS1_19_11.png" />
<img alt="../_images/DAA_M05_CS1_19_12.png" src="../_images/DAA_M05_CS1_19_12.png" />
<img alt="../_images/DAA_M05_CS1_19_13.png" src="../_images/DAA_M05_CS1_19_13.png" />
<img alt="../_images/DAA_M05_CS1_19_14.png" src="../_images/DAA_M05_CS1_19_14.png" />
<img alt="../_images/DAA_M05_CS1_19_15.png" src="../_images/DAA_M05_CS1_19_15.png" />
<img alt="../_images/DAA_M05_CS1_19_16.png" src="../_images/DAA_M05_CS1_19_16.png" />
<img alt="../_images/DAA_M05_CS1_19_17.png" src="../_images/DAA_M05_CS1_19_17.png" />
<img alt="../_images/DAA_M05_CS1_19_18.png" src="../_images/DAA_M05_CS1_19_18.png" />
<img alt="../_images/DAA_M05_CS1_19_19.png" src="../_images/DAA_M05_CS1_19_19.png" />
</div>
</div>
</div>
</div>
<div class="section" id="modelling">
<h2>Modelling<a class="headerlink" href="#modelling" title="Permalink to this headline">¶</a></h2>
<p>This section:</p>
<ul class="simple">
<li><p>fits some models; and</p></li>
<li><p>evaluates the fitted models.</p></li>
</ul>
<div class="section" id="gradient-boosting-machine-gbm">
<h3>Gradient Boosting Machine (GBM)<a class="headerlink" href="#gradient-boosting-machine-gbm" title="Permalink to this headline">¶</a></h3>
<p>The first model to be fitted is a Gradient Boosting Machine (GBM).
GBM applies boosting (see Section 5.3.3 of Module 5) in the context of decision trees.</p>
<p>The GBM will be used as a benchmark to compare to a neural network fitted later on.</p>
<p>To fit the GBM, the <code class="docutils literal notranslate"><span class="pre">GradientBoostingClassifier()</span></code> from the sklearn package is used.</p>
<div class="section" id="id1">
<h4>Prepare data<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>To prepare the data for the GBM model, the code below:</p>
<ul class="simple">
<li><p>one hot encodes the categorical features; and</p></li>
<li><p>splits the data into train, validation, and test sets.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># One-hot encode categorical features including an indicator for NAs.</span>
<span class="n">dataset_gbm</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">cat_cols</span><span class="p">,</span> <span class="n">dummy_na</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Split the data into train, valiation, and test sets.</span>
<span class="n">train_gbm_x</span><span class="p">,</span> <span class="n">train_gbm_y</span><span class="p">,</span> \
<span class="n">validation_gbm_x</span><span class="p">,</span> <span class="n">validation_gbm_y</span><span class="p">,</span> \
<span class="n">test_gbm_x</span><span class="p">,</span> <span class="n">test_gbm_y</span> \
<span class="o">=</span> <span class="n">create_data_splits</span><span class="p">(</span><span class="n">dataset_gbm</span><span class="p">,</span> <span class="n">id_col</span><span class="p">,</span> <span class="n">response_col</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="fit-initial-gbm-gbm-1">
<h4>Fit initial GBM (GBM 1)<a class="headerlink" href="#fit-initial-gbm-gbm-1" title="Permalink to this headline">¶</a></h4>
<p>To create and train the model, 20% of the training data will be used as an (internal) validation set for early stopping, to prevent overfitting. The model will stop training if no improvement on this validation data has been observed for 50 consecutive iterations.
This is specified with <code class="docutils literal notranslate"><span class="pre">validation_fraction</span> <span class="pre">=</span> <span class="pre">0.2</span></code> and <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span> <span class="pre">=</span> <span class="pre">50</span></code>.</p>
<p>Other hyperparameters used in the training:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_estimators</span> <span class="pre">=</span> <span class="pre">1000</span></code>: specifies a maximum of 1,000 trees;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">learning_rate</span> <span class="pre">=</span> <span class="pre">0.1</span></code>: sets the learning rate to 10%;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">random_state</span> <span class="pre">=</span> <span class="pre">1234</span></code>: initialises the random seed for the model, for reproducibility; and</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">verbose</span> <span class="pre">=</span> <span class="pre">1</span></code>: requests additional information be printed during training.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify the GBM model.</span>
<span class="n">gbm_model</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                                       <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
                                       <span class="n">validation_fraction</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
                                       <span class="n">n_iter_no_change</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
                                       <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                                       <span class="n">random_state</span> <span class="o">=</span> <span class="mi">1234</span><span class="p">)</span>
<span class="c1"># Train the model.</span>
<span class="c1"># &#39;train_gbm_y.values.ravel()&#39; converts the series of response values into</span>
<span class="c1"># a 1D array which is the format expected by the .fit() method</span>
<span class="n">gbm_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_gbm_x</span><span class="p">,</span> <span class="n">train_gbm_y</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      Iter       Train Loss   Remaining Time 
         1           1.1021            6.82s
         2           1.0617            7.96s
         3           1.0286            7.71s
         4           1.0003            7.72s
         5           0.9774            7.58s
         6           0.9570            7.43s
         7           0.9398            7.27s
         8           0.9251            7.16s
         9           0.9115            7.20s
        10           0.8997            7.16s
        20           0.8311            6.78s
        30           0.7995            6.65s
        40           0.7822            6.48s
        50           0.7657            6.36s
        60           0.7534            6.24s
        70           0.7434            6.18s
        80           0.7341            6.06s
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GradientBoostingClassifier(n_estimators=1000, n_iter_no_change=50,
                           random_state=1234, validation_fraction=0.2,
                           verbose=1)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="evaluate-gbm-1">
<h4>Evaluate GBM 1<a class="headerlink" href="#evaluate-gbm-1" title="Permalink to this headline">¶</a></h4>
<p>The code below looks at some basic goodness of fit measures.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Score the validation dataset.</span>

<span class="c1"># Obtain the predicted churn probabilities (Y_hat).</span>
<span class="c1"># The code below returns these in an array of form ([Prob(0), Prob(1)]).</span>
<span class="c1"># Keep the Prob(1) values only, i.e. the predicted churn probability (Y_hat).</span>
<span class="n">train_y_preds</span> <span class="o">=</span> <span class="n">gbm_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">train_gbm_x</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span> 
<span class="n">validation_y_preds</span> <span class="o">=</span> <span class="n">gbm_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">validation_gbm_x</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span> 

<span class="c1"># Obtain the predicted churn outcomes, G(X).</span>
<span class="c1"># Returns the predicted class as 0 for &#39;no churn&#39; or 1 for &#39;churn&#39;.</span>
<span class="n">train_y_class</span> <span class="o">=</span> <span class="n">gbm_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">train_gbm_x</span><span class="p">)</span>
<span class="n">validation_y_class</span> <span class="o">=</span> <span class="n">gbm_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">validation_gbm_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the AUC on train and validation data.</span>
<span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">train_gbm_y</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">train_y_preds</span><span class="p">),</span>
 <span class="s1">&#39;validation&#39;</span><span class="p">:</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">validation_gbm_y</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">validation_y_preds</span><span class="p">)}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;train&#39;: 0.8774950558946257, &#39;validation&#39;: 0.8589570358298064}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the confusion matrix at a 50% threshold using the training data.</span>
<span class="n">conf_mat_gbm1_train</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">train_gbm_y</span><span class="p">,</span> <span class="n">train_y_class</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">conf_mat_gbm1_train</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/DAA_M05_CS1_30_0.png" src="../_images/DAA_M05_CS1_30_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the confusion matrix at a 50% threshold using the validation data.</span>
<span class="n">conf_mat_gbm1_validation</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">validation_gbm_y</span><span class="p">,</span> <span class="n">validation_y_class</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">conf_mat_gbm1_validation</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/DAA_M05_CS1_31_0.png" src="../_images/DAA_M05_CS1_31_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the F1 score.</span>
<span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">train_gbm_y</span><span class="p">,</span> <span class="n">train_y_class</span><span class="p">),</span>
 <span class="s1">&#39;validation&#39;</span><span class="p">:</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">validation_gbm_y</span><span class="p">,</span> <span class="n">validation_y_class</span><span class="p">)}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;train&#39;: 0.6495086569957885, &#39;validation&#39;: 0.5618860510805501}
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="estimate-feature-importance">
<h4>Estimate feature importance<a class="headerlink" href="#estimate-feature-importance" title="Permalink to this headline">¶</a></h4>
<p>Feature importance provides a measure of how much the model predictions rely on a particular feature. The higher the importance of a feature, the more it contributes to the model’s performance.</p>
<p>The code below plot the importance of each feature in the GBM.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a dictionary with name-importance pairs.</span>
<span class="n">gbm_feat_imps</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="k">for</span> <span class="n">feature</span><span class="p">,</span> <span class="n">importance</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">train_gbm_x</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">gbm_model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">):</span>
    <span class="n">gbm_feat_imps</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="n">importance</span>

<span class="c1"># Convert to a dataframe and order by importance.</span>
<span class="c1"># Note: the feature names become the index for the dataframe, and the importance </span>
<span class="c1"># is the first column (index 0).</span>
<span class="n">gbm_fi</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">gbm_feat_imps</span><span class="p">,</span> <span class="n">orient</span> <span class="o">=</span> <span class="s1">&#39;index&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;Importance&#39;</span><span class="p">})</span>
<span class="n">gbm_fi</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="s1">&#39;Importance&#39;</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="c1"># Plot the feature importances.</span>
<span class="n">gbm_fi</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span> <span class="o">=</span> <span class="s1">&#39;barh&#39;</span><span class="p">,</span> \
            <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">16</span><span class="p">),</span> \
            <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Feature importance&#39;</span><span class="p">,</span> \
            <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;Dodgerblue&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:title={&#39;center&#39;:&#39;Feature importance&#39;}&gt;
</pre></div>
</div>
<img alt="../_images/DAA_M05_CS1_34_1.png" src="../_images/DAA_M05_CS1_34_1.png" />
</div>
</div>
</div>
<div class="section" id="examine-effect-of-features">
<h4>Examine effect of features<a class="headerlink" href="#examine-effect-of-features" title="Permalink to this headline">¶</a></h4>
<p>It is important to understand the shape of the effects learned by the model, in order to:</p>
<ul class="simple">
<li><p>understand what the model is doing; and</p></li>
<li><p>assess whether what the model has learned is reasonable given the business context.</p></li>
</ul>
<p>A partial dependence plot (PDP) shows how each feature affects a model’s predictions. Partial dependence is calculated after a model has been fitted, by examining how the model’s predictions change when the value for one feature (or sometimes two or more features) is changed, with the values of all other features being held constant.</p>
<p>PDPs are used below to visualise the effect shapes for the model’s four most important features.</p>
<p>The y-axis of a PDP represents the marginal impact of the feature on the response variable. For example, if the calculated partial dependence is 0 on some part of the PDP line, then for that value of the feature, there is no impact on the response variable, relative to some central tendency of the response variable, which might be its mean or median value.</p>
<p>You are not required to know how to calculate a PDP for this subject. For this case study, you can use the PDPs below to visualise the effect shapes for the model’s four most important features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Extract the column indices for the four most important features</span>
<span class="c1"># on the training data.</span>
<span class="n">gbm_pdp_idx</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_gbm_x</span><span class="o">.</span><span class="n">columns</span><span class="p">))</span> <span class="k">if</span> \
               <span class="n">train_gbm_x</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="ow">in</span> <span class="n">gbm_fi</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
<span class="n">gbm_pdp_idx</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0, 1, 2, 50]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check that the right columns have been identified.</span>
<span class="n">train_gbm_x</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">gbm_pdp_idx</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Index([&#39;tenure&#39;, &#39;MonthlyCharges&#39;, &#39;TotalCharges&#39;, &#39;Contract_Month-to-month&#39;], dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Produce partial dependence plots.</span>
<span class="c1"># Loop over each feature rather than provide a list as this makes it</span>
<span class="c1"># easier to plot the data. </span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">gbm_pdp_idx</span><span class="p">:</span>
    <span class="n">plot_partial_dependence</span><span class="p">(</span><span class="n">gbm_model</span><span class="p">,</span> <span class="n">train_gbm_x</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span><span class="p">],</span>
                          <span class="n">line_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;Dodgerblue&#39;</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/DAA_M05_CS1_38_0.png" src="../_images/DAA_M05_CS1_38_0.png" />
<img alt="../_images/DAA_M05_CS1_38_1.png" src="../_images/DAA_M05_CS1_38_1.png" />
<img alt="../_images/DAA_M05_CS1_38_2.png" src="../_images/DAA_M05_CS1_38_2.png" />
<img alt="../_images/DAA_M05_CS1_38_3.png" src="../_images/DAA_M05_CS1_38_3.png" />
</div>
</div>
<p>These plots show that the churn rate:</p>
<ul class="simple">
<li><p>decreases with tenure;</p></li>
<li><p>increases with monthly charges;</p></li>
<li><p>has an unclear relationship with total charges (but this will be affected by the correlation between monthly and total charges); and</p></li>
<li><p>increases for those on a monthly contract.</p></li>
</ul>
<p>Note that a PDP for a binary feature like ‘Contract_Month-to-month’ is a bit misleading, as the feature can only take values of 0 or 1, so there are only two points on this PDP that make sense. What is important to take away from this PDP is that people on a month-to-month contract are more likely to be predicted to churn than those on a one or two month contract.</p>
<p>This finding can be used to sense check the model’s predictions. In this case, we have already seen from the Explore Data section above that, across the entire dataset, people on a month-to-month contract have a 43% churn rate, compared to 11% for those on a one year contract and 3% for those on a two year contract, so the direction of the PDP outcomes for the feature ‘Contract_Month-to-month’ makes sense.</p>
</div>
<div class="section" id="improve-the-model-gbm-2">
<h4>Improve the model (GBM 2)<a class="headerlink" href="#improve-the-model-gbm-2" title="Permalink to this headline">¶</a></h4>
<p>Check the documentation for <code class="docutils literal notranslate"><span class="pre">GradientBoostingClassifier()</span></code> to see the hyperparameters available and try a few combinations to improve the performance.</p>
<p>The code below shows some experimentation with hyperparameter values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add additional regularisation by capping the depth of trees at 2 and</span>
<span class="c1"># decreasing the learning rate for the model.</span>
<span class="c1"># Increase the number of trees (n_estimators) to counter some of the effect</span>
<span class="c1"># of the lower learning rate.</span>
<span class="n">gbm_model_v2</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
                                          <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
                                          <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                                          <span class="n">validation_fraction</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
                                          <span class="n">n_iter_no_change</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
                                          <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                                          <span class="n">random_state</span> <span class="o">=</span> <span class="mi">1234</span><span class="p">)</span>
<span class="n">gbm_model_v2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_gbm_x</span><span class="p">,</span> <span class="n">train_gbm_y</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      Iter       Train Loss   Remaining Time 
         1           1.1497           11.15s
         2           1.1455           11.07s
         3           1.1414           10.87s
         4           1.1374           10.90s
         5           1.1335           10.63s
         6           1.1296           10.49s
         7           1.1258           10.34s
         8           1.1221           10.46s
         9           1.1185           10.44s
        10           1.1150           10.37s
        20           1.0827            9.94s
        30           1.0556            9.61s
        40           1.0325            9.41s
        50           1.0118            9.35s
        60           0.9929            9.33s
        70           0.9757            9.37s
        80           0.9611            9.42s
        90           0.9483            9.35s
       100           0.9369            9.26s
       200           0.8682            8.70s
       300           0.8394            8.11s
       400           0.8240            7.54s
       500           0.8144            6.98s
       600           0.8074            6.47s
       700           0.8014            5.98s
       800           0.7965            5.49s
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=2000,
                           n_iter_no_change=50, random_state=1234,
                           validation_fraction=0.2, verbose=1)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="evaluate-gbm-2">
<h4>Evaluate GBM 2<a class="headerlink" href="#evaluate-gbm-2" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Score the validation dataset.</span>

<span class="c1"># Obtain the predicted churn probabilities, Y_hat.</span>
<span class="n">validation_y_preds_v2</span> <span class="o">=</span> <span class="n">gbm_model_v2</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">validation_gbm_x</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span> 

<span class="c1"># Obtain the predicted churn outcomes, G(X).</span>
<span class="n">validation_y_class_v2</span> <span class="o">=</span> <span class="n">gbm_model_v2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">validation_gbm_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare the AUC on validation data under model 1 (&#39;old&#39;) and model 2 (&#39;new&#39;).</span>
<span class="p">{</span><span class="s1">&#39;new&#39;</span><span class="p">:</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">validation_gbm_y</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">validation_y_preds_v2</span><span class="p">),</span>
 <span class="s1">&#39;old&#39;</span><span class="p">:</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">validation_gbm_y</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">validation_y_preds</span><span class="p">)}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;new&#39;: 0.861474435196195, &#39;old&#39;: 0.8589570358298064}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the confusion matrix at 50% threshold for model 2 - validation data</span>
<span class="n">conf_mat_gbm2_validation</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">validation_gbm_y</span><span class="p">,</span> <span class="n">validation_y_class_v2</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">conf_mat_gbm2_validation</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/DAA_M05_CS1_45_0.png" src="../_images/DAA_M05_CS1_45_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the confusion matrix at 50% threshold for model 1 - validation data</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">conf_mat_gbm1_validation</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/DAA_M05_CS1_46_0.png" src="../_images/DAA_M05_CS1_46_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare the F1 score for the two models.</span>
<span class="p">{</span><span class="s1">&#39;new&#39;</span><span class="p">:</span><span class="n">f1_score</span><span class="p">(</span><span class="n">validation_gbm_y</span><span class="p">,</span> <span class="n">validation_y_class_v2</span><span class="p">),</span>
 <span class="s1">&#39;old&#39;</span><span class="p">:</span><span class="n">f1_score</span><span class="p">(</span><span class="n">validation_gbm_y</span><span class="p">,</span> <span class="n">validation_y_class</span><span class="p">)}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;new&#39;: 0.5697211155378485, &#39;old&#39;: 0.5618860510805501}
</pre></div>
</div>
</div>
</div>
<p>The AUC and F1 scores shown above indicate that the changes made to create GBM 2 have had some small (relatively immaterial) improvements on the GBM’s performance.</p>
</div>
<div class="section" id="select-final-model-gbm-final">
<h4>Select final model (GBM final)<a class="headerlink" href="#select-final-model-gbm-final" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Select the final model and call it `gbm_model_final`.</span>
<span class="n">gbm_model_final</span> <span class="o">=</span> <span class="n">gbm_model_v2</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Obtain the predicted churn probabilities, Y_hat, for the validation data.</span>
<span class="n">validation_gbm_preds_final</span> <span class="o">=</span> <span class="n">gbm_model_final</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">validation_gbm_x</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span> 

<span class="c1"># Obtain the predicted churn outcomes, G(X) for the validation data.</span>
<span class="n">validation_gbm_class_final</span> <span class="o">=</span> <span class="n">gbm_model_final</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">validation_gbm_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="simple-neural-networks-built-from-first-principles">
<h3>Simple neural networks built from first principles<a class="headerlink" href="#simple-neural-networks-built-from-first-principles" title="Permalink to this headline">¶</a></h3>
<p>In this section a neural network is built from first principles.
While you will not generally need to build a network from first principles, by reviewing the code below, along with Sections 5.5.2, 5.5.3, and 5.5.4 of Module 5, you should obtain a good understanding of what is going on ‘under the hood’ of a neural network.</p>
<p>To simplify the calculations below:</p>
<ul class="simple">
<li><p>the simple neural networks will be limited to at most 1 hidden layer (the first neural network has no hidden layers);</p></li>
<li><p>a sigmoid activation function is used;</p></li>
<li><p>mean-squared error (MSE) is used as the loss function; and</p></li>
<li><p>the loss function is optimised using backpropagation.</p></li>
</ul>
<div class="section" id="id2">
<h4>Prepare data<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<p>Categorical features must be encoded as numeric.</p>
<p>The categorical feature with <span class="math notranslate nohighlight">\(k\)</span> levels is encoded as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k - 1\)</span> binary features are created;</p></li>
<li><p>the <span class="math notranslate nohighlight">\(j^{th}\)</span> feature takes the value <span class="math notranslate nohighlight">\(1\)</span> if the categorical feature takes the <span class="math notranslate nohighlight">\(j^{th}\)</span> level;</p></li>
<li><p>otherwise the <span class="math notranslate nohighlight">\(j^{th}\)</span> feature takes the value 0.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use the get_dummies() function from the pandas package to produce the encoding.</span>
<span class="c1"># The drop_first = True argument tells pandas to drop the binary indicator for </span>
<span class="c1"># the first level, so the function returns k-1 rather than k features.</span>
<span class="n">dataset_nn</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">cat_cols</span><span class="p">,</span> <span class="n">drop_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split the data into train, validation, and test datasets.</span>
<span class="n">train_nn_x</span><span class="p">,</span> <span class="n">train_nn_y</span><span class="p">,</span> \
<span class="n">validation_nn_x</span><span class="p">,</span> <span class="n">validation_nn_y</span><span class="p">,</span> \
<span class="n">test_nn_x</span><span class="p">,</span> <span class="n">test_nn_y</span> \
<span class="o">=</span> <span class="n">create_data_splits</span><span class="p">(</span><span class="n">dataset_nn</span><span class="p">,</span> <span class="n">id_col</span><span class="p">,</span> <span class="n">response_col</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>When fitting neural networks it is common to scale the features to a 0-1 range. You can also scale to have standard deviation 1. In this case, only the range is scaled for simplicity.</p>
<p>The response vector also needs to be converted to a numpy array for training the neural network.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Scale features to lie in [0, 1].</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_nn_x</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_nn_x</span><span class="p">)</span>

<span class="c1"># Convert the response vector to a Numpy array.</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">train_nn_y</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1"># Prepare the validation and test datasets also.</span>
<span class="n">response_validation</span> <span class="o">=</span> <span class="n">validation_nn_y</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">input_validation</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">validation_nn_x</span><span class="p">)</span>
<span class="n">response_test</span> <span class="o">=</span> <span class="n">test_nn_y</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">input_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_nn_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="fit-a-single-layer-neural-network-nn-1">
<h4>Fit a single layer neural network (NN 1)<a class="headerlink" href="#fit-a-single-layer-neural-network-nn-1" title="Permalink to this headline">¶</a></h4>
<p>In its simplest form, a neural network can be reduced to a basic regression model (see Exercise 5.16 in Module 5). In the example below, a logistic regression model is constructed within the framework of a single cell neural network.</p>
<p>In this network, the single neuron performs the following operations on the <span class="math notranslate nohighlight">\(i^{th}\)</span> training observation (i.e. the <span class="math notranslate nohighlight">\(i^{th}\)</span> row of data):</p>
<ul class="simple">
<li><p>multiplies the input vector, <span class="math notranslate nohighlight">\(X_{i.}\)</span>, by the weights for the neuron, <span class="math notranslate nohighlight">\(a^{T}\)</span> and adds a bias term <span class="math notranslate nohighlight">\(a_0\)</span>;
$<span class="math notranslate nohighlight">\(f(X_{i.}) = a_0+a^{T}X_{i.}\)</span>$</p></li>
<li><p>the output of this linear function is then transformed using a non-linear activation function (in this case the sigmoid function);
$<span class="math notranslate nohighlight">\(\hat{y}_{i,1} = sigmoid(f(X_{i.})) = sigmoid(a_0+a^{T}X_{i.})\)</span>$</p></li>
</ul>
<p>As outlined in Section 5.5.2 of Module 5, there are a range of activation functions that can be used, and different problems require different functions.</p>
<p>The output of this first and final neuron is then fed into a loss function. Loss functions are discussed in Section 5.2 of Module 5. For simplicity, the mean-squared error is used here, so that the formula for the loss function is:</p>
<div class="math notranslate nohighlight">
\[J = \frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{y}_{i,1})^{2}\]</div>
<p>Backpropagation via gradient descent is then used to minimise this loss function. As described in Section 5.5.4, gradient descent computes the gradients of the loss function with respect to the parameters <span class="math notranslate nohighlight">\(a^{T}\)</span> and <span class="math notranslate nohighlight">\(a_{0}\)</span>. It uses these gradients to iteratively update the model’s parameters,  taking small steps towards minimising the loss function.</p>
<p>A single step of gradient descent involves the following computations:</p>
<ol class="simple">
<li><p>Compute the gradients of J with respect to the weights <span class="math notranslate nohighlight">\(a_j,  j=0,...,p\)</span>, denoted by <span class="math notranslate nohighlight">\(\partial a_j\)</span>.</p></li>
<li><p>Update the parameters <span class="math notranslate nohighlight">\(a_j, j=0,...,p\)</span> as follows:
$<span class="math notranslate nohighlight">\(a_j=a_j-\alpha \partial a_j\)</span>$</p></li>
<li><p>Using the updated parameters, perform another iteration of forward propagation over the entire set of training data to compute the new loss and gradients.</p></li>
<li><p>Continue iterating for a set number of updates over the entire training dataset, known as epochs.</p></li>
</ol>
<p>The diagram below shows the way in which each gradient descent step moves closer to a global minimum for the loss of the model. Note that in the diagram, <span class="math notranslate nohighlight">\(w\)</span> refers to <span class="math notranslate nohighlight">\(a_j\)</span> and ‘cost’ refers to ‘loss’.</p>
<p><img alt="grad_descent.png" src="../_images/grad_descent.png" /></p>
<figcaption>Image source: https://medium.com/analytics-vidhya/artificial-neural-networks-part-3-loss-and-cost-functions-and-gradient-descent-76e650bc5162</figcaption>
<p>The parameter <span class="math notranslate nohighlight">\(\alpha\)</span> above is the learning rate as described in Section 5.4.2.  In practice, this parameter is very important and will require some experimentation so that the learning rate:</p>
<ul class="simple">
<li><p>is not too small such that the algorithm will take too long to converge to an optimal set of weights; and</p></li>
<li><p>is not too large such that convergence may not occur at all as the algorithm continually overshoots the minimum point on the loss function.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the sigmoid function (to be used as the activation function).</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">  Sigmoid activation function</span>
<span class="sd">  Params:</span>
<span class="sd">    x: a float or integer value</span>
<span class="sd">  Return:</span>
<span class="sd">    The value of the sigmoid function evaluated at x (float)</span>
<span class="sd">  &#39;&#39;&#39;</span>
  <span class="k">return</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mf">1.0</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>The function below defines the first derivative of the sigmoid function which will be used in backpropagation when fitting the neural network. The first derivative of the sigmoid function is given by:</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dx} \left(\frac{1}{1 + \exp(-x)}\right) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = sigmoid(x) \times (1 - sigmoid(x))
\]</div>
<p>The cells below define some functions that will be used to train the model and make predictions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the first derivative of the sigmoid function.</span>
<span class="k">def</span> <span class="nf">dsigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Derivative of the sigmoid function evaluated at x</span>
<span class="sd">    Params:</span>
<span class="sd">    x: value the function will be evaluated at</span>
<span class="sd">    Return:</span>
<span class="sd">    The value of the function evaluated at x (float)</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the mean-squared error loss function.</span>
<span class="k">def</span> <span class="nf">mse_loss</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39; </span>
<span class="sd">    Mean-squared error loss function</span>
<span class="sd">    Params:</span>
<span class="sd">    response: the vector of responses, Y</span>
<span class="sd">    pred: the vector of predicted values, Y_hat</span>
<span class="sd">    Return:</span>
<span class="sd">    The MSE value (float)</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">return</span> <span class="p">((</span><span class="n">response</span> <span class="o">-</span> <span class="n">pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">response</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a function to provide random starting values for the weights and bias term.</span>
<span class="k">def</span> <span class="nf">init_params</span><span class="p">(</span><span class="n">n_p</span><span class="p">,</span> <span class="n">n_h</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">range</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">start</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Randomly initializes weights and initialises biases to 0</span>
<span class="sd">    Params:</span>
<span class="sd">    n_p: number of features (size of input array)</span>
<span class="sd">    n_h: number of neurons in layer, defaults to 1</span>
<span class="sd">    range: range of random initalisation, defaults to 0.1</span>
<span class="sd">    start: lowest value of random initialisation, defaults to -0.05</span>
<span class="sd">    Returns:</span>
<span class="sd">    a0: bias vector</span>
<span class="sd">    aT: random weights vector</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">a0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_h</span><span class="p">))</span>
    <span class="n">aT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_p</span><span class="p">,</span> <span class="n">n_h</span><span class="p">)</span> <span class="o">*</span> <span class="nb">range</span> <span class="o">+</span> <span class="n">start</span>
    <span class="k">return</span> <span class="n">a0</span><span class="p">,</span> <span class="n">aT</span>


<span class="c1"># Set the random seed for reproducibility.</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1235</span><span class="p">)</span>

<span class="c1"># Initialise the network weights.</span>
<span class="c1"># Note that the bias is initialised to zero and weights to a random value</span>
<span class="c1"># uniformly in the range [-0.05, 0.05].</span>
<span class="c1"># This range is set to match the default for keras (which will be tested later).</span>
<span class="n">bias</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">init_params</span><span class="p">(</span><span class="n">n_p</span> <span class="o">=</span> <span class="n">train_nn_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                            <span class="n">n_h</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                            <span class="nb">range</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
                            <span class="n">start</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a function to perform the forward pass over the data. This will be used</span>
<span class="c1"># for training the model. Once the weights have been selected, it will also</span>
<span class="c1"># serve as the prediction function.</span>
<span class="c1"># The function is fairly simple. The input is a vector with dimension equal to</span>
<span class="c1"># the number of features in the dataset (20). There is a single output neuron</span>
<span class="c1"># and no hidden layers.</span>
<span class="k">def</span> <span class="nf">fwd_pass1</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">keep_intermediate</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Performs the forward pass calculations for a single neuron network</span>
<span class="sd">    Params:</span>
<span class="sd">    input: input data frame</span>
<span class="sd">    bias: bias parameter</span>
<span class="sd">    weights: weights vector</span>
<span class="sd">    keep_intermediate: (logical) keep the intermediate results?</span>
<span class="sd">                       If True, returns the linear score in addition to the </span>
<span class="sd">                       output value y_hat after the activation function has</span>
<span class="sd">                       been applied.</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="c1"># Calculate the value for the neuron on the linear scale,</span>
    <span class="c1"># using the sum-product of the inputs and weights, plus the bias term</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>

    <span class="c1"># Apply the activation function</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>    <span class="c1"># final output</span>

    <span class="k">if</span> <span class="n">keep_intermediate</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">f</span><span class="p">,</span> <span class="n">y_hat</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">y_hat</span>
</pre></div>
</div>
</div>
</div>
<p>The learning rate and number of training iterations are then defined below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialise the hyperparameters.</span>

<span class="c1"># Set the learning rate - must be in (0, 1].</span>
<span class="n">learn_rate</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="c1"># Set the number of training iterations.</span>
<span class="n">n_rounds</span> <span class="o">=</span> <span class="mi">500</span>
</pre></div>
</div>
</div>
</div>
<p>The model can now be trained using the functions and hyperparameters specified above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train the model by implementing the gradient descent algorithm over n_rounds</span>
<span class="c1"># of iterations.</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">):</span>

    <span class="c1"># Perform a forward propagation.</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">y_hat</span> <span class="o">=</span> <span class="n">fwd_pass1</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>


    <span class="c1"># Perform the back-propagation step.</span>
    <span class="c1"># This involves calculating the partial derivative of the loss function</span>
    <span class="c1"># with respect to the weights.</span>

    <span class="c1"># Calculate the partial derivative of the loss function (J)</span>
    <span class="c1"># with respect to the output (y_hat_i)</span>
    <span class="c1"># J = (y_i - y_hat_i)^2 -&gt; dJ/dy_hat_i = -2(y_i - y_hat_i) = 2(y_hat_i-y_i)</span>
    <span class="c1"># In the calculation below, the factor of 2 is dropped as this does not</span>
    <span class="c1"># impact the minimum value of the loss function.</span>
    <span class="n">dJ_dyhat</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_hat</span><span class="o">-</span><span class="n">response</span><span class="p">)</span>

    <span class="c1"># Calculate the partial derivate of the output (y_hat_i)</span>
    <span class="c1"># with respect to the linear values of the neuron (f(X_i)).</span>
    <span class="c1"># The output is simply the activation function applied to the linear values</span>
    <span class="c1"># so the derivative is just the derivative of the activation function.</span>
    <span class="n">dyhat_df</span> <span class="o">=</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="c1"># The partial derivative of the linear values of the neuron (f(X_i))</span>
    <span class="c1"># with respect to the weights is just the inputs (X_i)</span>
    <span class="c1"># because linear values (f(X_i))= a1*x1 + a2*x2 + ... </span>

    <span class="c1"># Calculate the gradient of the loss function, excluding the input values</span>
    <span class="c1"># because these are constants.</span>
    <span class="c1"># This is a useful intermediate calculation step to capture.</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">dJ_dyhat</span><span class="o">*</span><span class="n">dyhat_df</span>

    <span class="c1"># Update the weights.</span>
    <span class="n">weights_old</span> <span class="o">=</span> <span class="n">weights</span>
    <span class="n">weights</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span> <span class="o">/</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Update the bias.</span>
    <span class="n">bias</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Print the loss calculated after every 25th iteration.</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">mod</span><span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;iter </span><span class="si">{</span><span class="n">_</span><span class="si">}</span><span class="s1"> MSE: </span><span class="si">{</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>iter 0 MSE: 0.2674449173283448
iter 25 MSE: 0.18042793537210977
iter 50 MSE: 0.1695903881202274
iter 75 MSE: 0.16375575000190337
iter 100 MSE: 0.15956370573799245
iter 125 MSE: 0.1563918167751347
iter 150 MSE: 0.15393250776603987
iter 175 MSE: 0.15198658083135239
iter 200 MSE: 0.15041856028584913
iter 225 MSE: 0.14913420464122898
iter 250 MSE: 0.14806660930424229
iter 275 MSE: 0.1471673599402955
iter 300 MSE: 0.14640079912380527
iter 325 MSE: 0.14574023742870543
iter 350 MSE: 0.1451654004269753
iter 375 MSE: 0.14466067664172322
iter 400 MSE: 0.14421389450896388
iter 425 MSE: 0.14381545501845847
iter 450 MSE: 0.14345770750271378
iter 475 MSE: 0.14313449424136288
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="evaluate-nn-1">
<h4>Evaluate NN 1<a class="headerlink" href="#evaluate-nn-1" title="Permalink to this headline">¶</a></h4>
<p>The cells below assess the module using the AUC measure and by plotting the confusion matrix.
They also compare the single layer neural network to the final GBM model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Now create predictions with the weights from the training step above,</span>
<span class="c1"># using the fwd_pass1 function. </span>
<span class="n">pred_nn1_train</span> <span class="o">=</span> <span class="n">fwd_pass1</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">pred_nn1_validation</span> <span class="o">=</span> <span class="n">fwd_pass1</span><span class="p">(</span><span class="n">input_validation</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the training and validation AUC.</span>
<span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">pred_nn1_train</span><span class="p">),</span>
 <span class="s1">&#39;validation&#39;</span><span class="p">:</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">pred_nn1_validation</span><span class="p">)}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;train&#39;: 0.8299297457961795, &#39;validation&#39;: 0.8414724395699427}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the confusion matrix, with predictions converted to </span>
<span class="c1"># binary classes using a 50% threshold.</span>
<span class="n">pred_nn1_validation_class</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">pred_nn1_validation</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">conf_mat_nn1_validation</span> <span class="o">=</span> <span class="n">confusion_matrix_new</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span>
    <span class="n">response_validation</span><span class="p">,</span> <span class="n">pred_nn1_validation_class</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">conf_mat_nn1_validation</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/DAA_M05_CS1_74_0.png" src="../_images/DAA_M05_CS1_74_0.png" />
</div>
</div>
<p>The output from the single layer neural network can now be compared to the output from the final GBM.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;GBM final&#39;</span><span class="p">:</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">validation_gbm_preds_final</span><span class="p">),</span>
 <span class="s1">&#39;NN 1&#39;</span><span class="p">:</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">pred_nn1_validation</span><span class="p">)}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;GBM final&#39;: 0.861474435196195, &#39;NN 1&#39;: 0.8414724395699427}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">conf_mat_gbm_validation</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">validation_gbm_y</span><span class="p">,</span> <span class="n">validation_gbm_class_final</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">conf_mat_gbm_validation</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/DAA_M05_CS1_77_0.png" src="../_images/DAA_M05_CS1_77_0.png" />
</div>
</div>
<p>The AUC is slighly higher (better) under the final GBM than under the single layer neural network. However, the very simple neural network isn’t far behind the more complicated GBM and has an AUC above 0.84, so this still seems to be a reasonable model for predicting churn.</p>
</div>
<div class="section" id="fit-a-multi-layer-neural-network-nn-2">
<h4>Fit a multi-layer neural network (NN 2)<a class="headerlink" href="#fit-a-multi-layer-neural-network-nn-2" title="Permalink to this headline">¶</a></h4>
<p>The single layer neural network (NN 1) can now be extended to a more complex model in an attempt to improve the predictive capability of the neural network.</p>
<p>This second neural network (NN 2) will have one hidden layer with four neurons. Again, the sigmoid activation function will be used in the hidden and output layers and mean-squared error will be used as the loss function.</p>
<p>For the time-being, this will still be built and trained from first principles. Again, while it will rarely be necessary for you to build a neural network from first principles, you should review the code below to get a better sense of what is going on within a neural network with a hidden layer.</p>
<p>To train this network, the following two steps will again be performed:</p>
<ol class="simple">
<li><p>a forward propagation step to pass the data through the network from start to finish; and</p></li>
<li><p>a backpropagation step to pass the error back through the network, from the end output (where the error is first observed) to the start of the network (i.e. the input layer).</p></li>
</ol>
<p>The neurons in the network’s hidden layer perform identical operations to those performed by the single neuron in the first neural network. For example, the first neuron in the hidden layer does the following:
$<span class="math notranslate nohighlight">\(f(X_{i.}) = a_{01} + a_{1}^{T}X_{i.} \)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(Z_{1,1} = \sigma (f(X_{i.})) = \sigma (a_{01} + a_{1}^{T}X_{i.})\)</span>$</p>
<p>The sigmoid, dsigmoid, mse_loss and init_params functions that were defined for the purpose of fitting NN 1 can also be used for NN 2.</p>
<p>The following function defines how the forward propagation step should proceed for NN2 that has one hidden layer with four neurons.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fwd_pass2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">l1_bias</span><span class="p">,</span> <span class="n">l1_weights</span><span class="p">,</span> <span class="n">l2_bias</span><span class="p">,</span> <span class="n">l2_weights</span><span class="p">,</span> <span class="n">keep_intermediate</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Performs the forward propagation calculations for a network with a single hidden layer.</span>
<span class="sd">    Params:</span>
<span class="sd">    input: input data</span>
<span class="sd">    l1_bias: bias for layer 1 (the hidden layer)</span>
<span class="sd">    l1_weights: weights for layer 1</span>
<span class="sd">    l2_bias: bias for layer 2 (the output layer)</span>
<span class="sd">    l2_weights: weights for layer 2</span>
<span class="sd">    keep_intermediate: (logical) keep the intermediate results? </span>
<span class="sd">                       If True, returns the linear scores and activations for</span>
<span class="sd">                       the hidden and output layers.</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="c1"># Calculate the neurons in layer 1 (the hidden layer).</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">l1_weights</span><span class="p">)</span> <span class="o">+</span> <span class="n">l1_bias</span> <span class="c1"># linear score</span>
    <span class="n">z1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">f1</span><span class="p">)</span>                            <span class="c1"># activation</span>

    <span class="c1"># Output layer</span>
    <span class="n">f2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">l2_weights</span><span class="p">)</span> <span class="o">+</span> <span class="n">l2_bias</span>       <span class="c1"># linear score </span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">f2</span><span class="p">)</span>                         <span class="c1"># activation: final output</span>

    <span class="k">if</span> <span class="n">keep_intermediate</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">f1</span><span class="p">,</span> <span class="n">z1</span><span class="p">,</span> <span class="n">f2</span><span class="p">,</span> <span class="n">y_hat</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">y_hat</span>
</pre></div>
</div>
</div>
</div>
<p>The learning rate and number of training iterations are then defined below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the learning rate - must be in (0, 1].</span>
<span class="n">learn_rate</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="c1"># Set the number of training iterations.</span>
<span class="n">n_rounds</span> <span class="o">=</span> <span class="mi">500</span>

<span class="c1"># Specify the desired number of neurons in the hidden layer.</span>
<span class="n">hidden_neurons</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># Set the random seed for reproducibility.</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1235</span><span class="p">)</span>

<span class="c1"># Initialise the weights for the hidden layer.</span>
<span class="n">l1_bias</span><span class="p">,</span> <span class="n">l1_weights</span> <span class="o">=</span> <span class="n">init_params</span><span class="p">(</span><span class="n">n_p</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_h</span> <span class="o">=</span> <span class="n">hidden_neurons</span><span class="p">,</span> <span class="nb">range</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">start</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">)</span>

<span class="c1"># Initialise the weights for the output layer.</span>
<span class="n">l2_bias</span><span class="p">,</span> <span class="n">l2_weights</span> <span class="o">=</span> <span class="n">init_params</span><span class="p">(</span><span class="n">n_p</span> <span class="o">=</span> <span class="n">l1_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_h</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">range</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">start</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The model can now be trained using the functions and hyperparameters specified above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">):</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    The model is trained by first performing a forward pass to get the predictions,</span>
<span class="sd">    followed by a backward pass to &#39;propagate&#39; the loss back through each of the</span>
<span class="sd">    neurons.</span>
<span class="sd">    This is repeated each iteration until the network has converged or </span>
<span class="sd">    the maximum number of rounds has been reached.</span>

<span class="sd">    The following notation is used in this function: </span>
<span class="sd">    ln_weights: the weights matrix (one column per neuron) for layer n</span>
<span class="sd">    ln_bias: vector of bias values (one per neuron) for layer n</span>
<span class="sd">    fn: vector of linear scores (one per neuron) for layer n</span>
<span class="sd">    zn: vector of activations (one per neuron) for layer n</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="c1"># Perform the forward pass.</span>
    <span class="n">f1</span><span class="p">,</span> <span class="n">z1</span><span class="p">,</span> <span class="n">f2</span><span class="p">,</span> <span class="n">y_hat</span> <span class="o">=</span> <span class="n">fwd_pass2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">l1_bias</span><span class="p">,</span> <span class="n">l1_weights</span><span class="p">,</span> <span class="n">l2_bias</span><span class="p">,</span> <span class="n">l2_weights</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Perform the backpropagation step.</span>
    <span class="c1"># Perform the intermediate calculations for the loss function gradients.</span>
    <span class="n">delta2</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">response</span><span class="p">)</span> <span class="o">*</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="n">f2</span><span class="p">)</span>        
    <span class="n">delta1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta2</span><span class="p">,</span> <span class="n">l2_weights</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="n">f1</span><span class="p">)</span>

    <span class="n">dloss_dweight2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z1</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">delta2</span><span class="p">)</span><span class="o">/</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Gradient with respect to the layer 2 weights.</span>
    <span class="n">dloss_dweight1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">delta1</span><span class="p">)</span><span class="o">/</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Gradient with respect to the layer 1 weights.</span>

    <span class="c1"># Update the weights.</span>
    <span class="n">l2_weights</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">dloss_dweight2</span>
    <span class="n">l1_weights</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">dloss_dweight1</span>

    <span class="c1"># Update the bias terms.</span>
    <span class="n">l2_bias</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">delta2</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">l1_bias</span> <span class="o">-=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">delta1</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Print the loss after every 25th iteration.</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">mod</span><span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;iter&#39;</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>iter 0 : 0.2511001577139275
iter 25 : 0.21041550733934317
iter 50 : 0.19913895871849857
iter 75 : 0.19548216749243946
iter 100 : 0.1940608216259812
iter 125 : 0.1933855313335157
iter 150 : 0.19298164332895798
iter 175 : 0.19268104914715317
iter 200 : 0.19241961955198858
iter 225 : 0.19217148993015826
iter 250 : 0.19192567962778895
iter 275 : 0.19167721705008536
iter 300 : 0.19142361898305546
iter 325 : 0.19116345236013363
iter 350 : 0.19089573547094899
iter 375 : 0.19061968576294885
iter 400 : 0.19033461306845684
iter 425 : 0.1900398745690468
iter 450 : 0.1897348562620397
iter 475 : 0.18941896598690924
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="evaluate-nn-2">
<h4>Evaluate NN 2<a class="headerlink" href="#evaluate-nn-2" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pred_nn2_train</span> <span class="o">=</span> <span class="n">fwd_pass2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">l1_bias</span><span class="p">,</span> <span class="n">l1_weights</span><span class="p">,</span> <span class="n">l2_bias</span><span class="p">,</span> <span class="n">l2_weights</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">pred_nn2_validation</span> <span class="o">=</span> <span class="n">fwd_pass2</span><span class="p">(</span><span class="n">input_validation</span><span class="p">,</span> <span class="n">l1_bias</span><span class="p">,</span> <span class="n">l1_weights</span><span class="p">,</span> <span class="n">l2_bias</span><span class="p">,</span> <span class="n">l2_weights</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">pred_nn2_train</span><span class="p">),</span>
 <span class="s1">&#39;validation&#39;</span><span class="p">:</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">pred_nn2_validation</span><span class="p">)}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;train&#39;: 0.8136249709132997, &#39;validation&#39;: 0.8250833589715872}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare NN 2 to GBM final and NN 1</span>
<span class="p">{</span><span class="s1">&#39;1. NN 2&#39;</span><span class="p">:</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">pred_nn2_validation</span><span class="p">),</span>
 <span class="s1">&#39;2. NN 1&#39;</span><span class="p">:</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">pred_nn1_validation</span><span class="p">),</span>
 <span class="s1">&#39;3. GBM final&#39;</span><span class="p">:</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">validation_gbm_preds_final</span><span class="p">)}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;1. NN 2&#39;: 0.8250833589715872,
 &#39;2. NN 1&#39;: 0.8414724395699427,
 &#39;3. GBM final&#39;: 0.861474435196195}
</pre></div>
</div>
</div>
</div>
<p>The slightly more complex neural network with one hidden layer (NN 2) performed slightly worse on the validation data than the very simple one neuron neural network (NN 1). Both performaed slightly worse than the GBM but still had AUCs over 83%.</p>
</div>
</div>
<div class="section" id="neural-networks-using-keras">
<h3>Neural networks using Keras<a class="headerlink" href="#neural-networks-using-keras" title="Permalink to this headline">¶</a></h3>
<p>This section demonstrates how to fit a neural network using Python’s Keras package. Keras, which runs on top of the TensorFlow library, does all of the calculations shown above for the simple neural networks, taking a lot of the hard work out of building a neural network.</p>
<p>The following steps are used to build the neural networks using Keras:</p>
<ul class="simple">
<li><p>use <code class="docutils literal notranslate"><span class="pre">Sequential()</span></code> to specify a feedforward neural network;</p></li>
<li><p>use the <code class="docutils literal notranslate"><span class="pre">.add()</span></code> method to add layers to the network, combined with <code class="docutils literal notranslate"><span class="pre">Dense()</span></code> to specify a dense layer (where all the neurons are fully connected to the preceding layer).</p></li>
</ul>
<div class="section" id="fit-a-single-layer-neural-network-with-keras-nn-3">
<h4>Fit a single layer neural network with Keras (NN 3)<a class="headerlink" href="#fit-a-single-layer-neural-network-with-keras-nn-3" title="Permalink to this headline">¶</a></h4>
<p>The following options are taken to align the first Keras model with NN 2:</p>
<ul class="simple">
<li><p>SGD optimiser: this optimises using stochastic gradient descent with momentum. By setting <code class="docutils literal notranslate"><span class="pre">momentum</span> <span class="pre">=</span> <span class="pre">0.0</span></code>, <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> to the input data size, and <code class="docutils literal notranslate"><span class="pre">steps_per_epoch</span> <span class="pre">=</span> <span class="pre">1</span></code> the basic backpropagation algorithm is recovered.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bias_initializer</span> <span class="pre">=</span> <span class="pre">'zeros'</span></code> and <code class="docutils literal notranslate"><span class="pre">kernel_initializer</span> <span class="pre">=</span> <span class="pre">'random_uniform'</span></code>: this sets the initial bias values to 0 and the weights to random uniform (defaulted to a range of [-0.05, 0.05] as used in the simple neural networks above.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the seed for the random number generator, for reproducibility of the results.</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1235</span><span class="p">)</span>

<span class="c1"># Build a model with 1 (dense) hidden layer, 4 neurons and</span>
<span class="c1"># a sigmoid activation function.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="n">kernel_initializer</span> <span class="o">=</span> <span class="s1">&#39;random_uniform&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="n">kernel_initializer</span> <span class="o">=</span> <span class="s1">&#39;random_uniform&#39;</span><span class="p">))</span>

<span class="c1"># Specify the optimiser to use.</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="c1"># Compile the model using the mean-squared error loss function.</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;mse&#39;</span><span class="p">,</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mse&#39;</span><span class="p">],</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">opt</span>    
<span class="p">)</span>

<span class="c1"># Train the model.</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> 
          <span class="n">response</span><span class="p">,</span> 
          <span class="n">epochs</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> 
          <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
          <span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> 
          <span class="n">verbose</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/500
1/1 - 0s - loss: 0.2531 - mse: 0.2531
Epoch 2/500
1/1 - 0s - loss: 0.2474 - mse: 0.2474
Epoch 3/500
1/1 - 0s - loss: 0.2422 - mse: 0.2422
Epoch 4/500
1/1 - 0s - loss: 0.2375 - mse: 0.2375
Epoch 5/500
1/1 - 0s - loss: 0.2332 - mse: 0.2332
Epoch 6/500
1/1 - 0s - loss: 0.2295 - mse: 0.2295
Epoch 7/500
1/1 - 0s - loss: 0.2261 - mse: 0.2261
Epoch 8/500
1/1 - 0s - loss: 0.2230 - mse: 0.2230
Epoch 9/500
1/1 - 0s - loss: 0.2202 - mse: 0.2202
Epoch 10/500
1/1 - 0s - loss: 0.2178 - mse: 0.2178
Epoch 11/500
1/1 - 0s - loss: 0.2156 - mse: 0.2156
Epoch 12/500
1/1 - 0s - loss: 0.2136 - mse: 0.2136
Epoch 13/500
1/1 - 0s - loss: 0.2118 - mse: 0.2118
Epoch 14/500
1/1 - 0s - loss: 0.2101 - mse: 0.2101
Epoch 15/500
1/1 - 0s - loss: 0.2087 - mse: 0.2087
Epoch 16/500
1/1 - 0s - loss: 0.2073 - mse: 0.2073
Epoch 17/500
1/1 - 0s - loss: 0.2061 - mse: 0.2061
Epoch 18/500
1/1 - 0s - loss: 0.2051 - mse: 0.2051
Epoch 19/500
1/1 - 0s - loss: 0.2041 - mse: 0.2041
Epoch 20/500
1/1 - 0s - loss: 0.2032 - mse: 0.2032
Epoch 21/500
1/1 - 0s - loss: 0.2024 - mse: 0.2024
Epoch 22/500
1/1 - 0s - loss: 0.2016 - mse: 0.2016
Epoch 23/500
1/1 - 0s - loss: 0.2010 - mse: 0.2010
Epoch 24/500
1/1 - 0s - loss: 0.2004 - mse: 0.2004
Epoch 25/500
1/1 - 0s - loss: 0.1998 - mse: 0.1998
Epoch 26/500
1/1 - 0s - loss: 0.1993 - mse: 0.1993
Epoch 27/500
1/1 - 0s - loss: 0.1988 - mse: 0.1988
Epoch 28/500
1/1 - 0s - loss: 0.1984 - mse: 0.1984
Epoch 29/500
1/1 - 0s - loss: 0.1980 - mse: 0.1980
Epoch 30/500
1/1 - 0s - loss: 0.1977 - mse: 0.1977
Epoch 31/500
1/1 - 0s - loss: 0.1973 - mse: 0.1973
Epoch 32/500
1/1 - 0s - loss: 0.1970 - mse: 0.1970
Epoch 33/500
1/1 - 0s - loss: 0.1968 - mse: 0.1968
Epoch 34/500
1/1 - 0s - loss: 0.1965 - mse: 0.1965
Epoch 35/500
1/1 - 0s - loss: 0.1963 - mse: 0.1963
Epoch 36/500
1/1 - 0s - loss: 0.1960 - mse: 0.1960
Epoch 37/500
1/1 - 0s - loss: 0.1958 - mse: 0.1958
Epoch 38/500
1/1 - 0s - loss: 0.1956 - mse: 0.1956
Epoch 39/500
1/1 - 0s - loss: 0.1955 - mse: 0.1955
Epoch 40/500
1/1 - 0s - loss: 0.1953 - mse: 0.1953
Epoch 41/500
1/1 - 0s - loss: 0.1952 - mse: 0.1952
Epoch 42/500
1/1 - 0s - loss: 0.1950 - mse: 0.1950
Epoch 43/500
1/1 - 0s - loss: 0.1949 - mse: 0.1949
Epoch 44/500
1/1 - 0s - loss: 0.1948 - mse: 0.1948
Epoch 45/500
1/1 - 0s - loss: 0.1947 - mse: 0.1947
Epoch 46/500
1/1 - 0s - loss: 0.1946 - mse: 0.1946
Epoch 47/500
1/1 - 0s - loss: 0.1945 - mse: 0.1945
Epoch 48/500
1/1 - 0s - loss: 0.1944 - mse: 0.1944
Epoch 49/500
1/1 - 0s - loss: 0.1943 - mse: 0.1943
Epoch 50/500
1/1 - 0s - loss: 0.1942 - mse: 0.1942
Epoch 51/500
1/1 - 0s - loss: 0.1941 - mse: 0.1941
Epoch 52/500
1/1 - 0s - loss: 0.1940 - mse: 0.1940
Epoch 53/500
1/1 - 0s - loss: 0.1940 - mse: 0.1940
Epoch 54/500
1/1 - 0s - loss: 0.1939 - mse: 0.1939
Epoch 55/500
1/1 - 0s - loss: 0.1938 - mse: 0.1938
Epoch 56/500
1/1 - 0s - loss: 0.1938 - mse: 0.1938
Epoch 57/500
1/1 - 0s - loss: 0.1937 - mse: 0.1937
Epoch 58/500
1/1 - 0s - loss: 0.1937 - mse: 0.1937
Epoch 59/500
1/1 - 0s - loss: 0.1936 - mse: 0.1936
Epoch 60/500
1/1 - 0s - loss: 0.1936 - mse: 0.1936
Epoch 61/500
1/1 - 0s - loss: 0.1935 - mse: 0.1935
Epoch 62/500
1/1 - 0s - loss: 0.1935 - mse: 0.1935
Epoch 63/500
1/1 - 0s - loss: 0.1935 - mse: 0.1935
Epoch 64/500
1/1 - 0s - loss: 0.1934 - mse: 0.1934
Epoch 65/500
1/1 - 0s - loss: 0.1934 - mse: 0.1934
Epoch 66/500
1/1 - 0s - loss: 0.1933 - mse: 0.1933
Epoch 67/500
1/1 - 0s - loss: 0.1933 - mse: 0.1933
Epoch 68/500
1/1 - 0s - loss: 0.1933 - mse: 0.1933
Epoch 69/500
1/1 - 0s - loss: 0.1932 - mse: 0.1932
Epoch 70/500
1/1 - 0s - loss: 0.1932 - mse: 0.1932
Epoch 71/500
1/1 - 0s - loss: 0.1932 - mse: 0.1932
Epoch 72/500
1/1 - 0s - loss: 0.1931 - mse: 0.1931
Epoch 73/500
1/1 - 0s - loss: 0.1931 - mse: 0.1931
Epoch 74/500
1/1 - 0s - loss: 0.1931 - mse: 0.1931
Epoch 75/500
1/1 - 0s - loss: 0.1931 - mse: 0.1931
Epoch 76/500
1/1 - 0s - loss: 0.1930 - mse: 0.1930
Epoch 77/500
1/1 - 0s - loss: 0.1930 - mse: 0.1930
Epoch 78/500
1/1 - 0s - loss: 0.1930 - mse: 0.1930
Epoch 79/500
1/1 - 0s - loss: 0.1930 - mse: 0.1930
Epoch 80/500
1/1 - 0s - loss: 0.1929 - mse: 0.1929
Epoch 81/500
1/1 - 0s - loss: 0.1929 - mse: 0.1929
Epoch 82/500
1/1 - 0s - loss: 0.1929 - mse: 0.1929
Epoch 83/500
1/1 - 0s - loss: 0.1929 - mse: 0.1929
Epoch 84/500
1/1 - 0s - loss: 0.1928 - mse: 0.1928
Epoch 85/500
1/1 - 0s - loss: 0.1928 - mse: 0.1928
Epoch 86/500
1/1 - 0s - loss: 0.1928 - mse: 0.1928
Epoch 87/500
1/1 - 0s - loss: 0.1928 - mse: 0.1928
Epoch 88/500
1/1 - 0s - loss: 0.1927 - mse: 0.1927
Epoch 89/500
1/1 - 0s - loss: 0.1927 - mse: 0.1927
Epoch 90/500
1/1 - 0s - loss: 0.1927 - mse: 0.1927
Epoch 91/500
1/1 - 0s - loss: 0.1927 - mse: 0.1927
Epoch 92/500
1/1 - 0s - loss: 0.1927 - mse: 0.1927
Epoch 93/500
1/1 - 0s - loss: 0.1926 - mse: 0.1926
Epoch 94/500
1/1 - 0s - loss: 0.1926 - mse: 0.1926
Epoch 95/500
1/1 - 0s - loss: 0.1926 - mse: 0.1926
Epoch 96/500
1/1 - 0s - loss: 0.1926 - mse: 0.1926
Epoch 97/500
1/1 - 0s - loss: 0.1926 - mse: 0.1926
Epoch 98/500
1/1 - 0s - loss: 0.1925 - mse: 0.1925
Epoch 99/500
1/1 - 0s - loss: 0.1925 - mse: 0.1925
Epoch 100/500
1/1 - 0s - loss: 0.1925 - mse: 0.1925
Epoch 101/500
1/1 - 0s - loss: 0.1925 - mse: 0.1925
Epoch 102/500
1/1 - 0s - loss: 0.1925 - mse: 0.1925
Epoch 103/500
1/1 - 0s - loss: 0.1924 - mse: 0.1924
Epoch 104/500
1/1 - 0s - loss: 0.1924 - mse: 0.1924
Epoch 105/500
1/1 - 0s - loss: 0.1924 - mse: 0.1924
Epoch 106/500
1/1 - 0s - loss: 0.1924 - mse: 0.1924
Epoch 107/500
1/1 - 0s - loss: 0.1924 - mse: 0.1924
Epoch 108/500
1/1 - 0s - loss: 0.1923 - mse: 0.1923
Epoch 109/500
1/1 - 0s - loss: 0.1923 - mse: 0.1923
Epoch 110/500
1/1 - 0s - loss: 0.1923 - mse: 0.1923
Epoch 111/500
1/1 - 0s - loss: 0.1923 - mse: 0.1923
Epoch 112/500
1/1 - 0s - loss: 0.1923 - mse: 0.1923
Epoch 113/500
1/1 - 0s - loss: 0.1922 - mse: 0.1922
Epoch 114/500
1/1 - 0s - loss: 0.1922 - mse: 0.1922
Epoch 115/500
1/1 - 0s - loss: 0.1922 - mse: 0.1922
Epoch 116/500
1/1 - 0s - loss: 0.1922 - mse: 0.1922
Epoch 117/500
1/1 - 0s - loss: 0.1922 - mse: 0.1922
Epoch 118/500
1/1 - 0s - loss: 0.1922 - mse: 0.1922
Epoch 119/500
1/1 - 0s - loss: 0.1921 - mse: 0.1921
Epoch 120/500
1/1 - 0s - loss: 0.1921 - mse: 0.1921
Epoch 121/500
1/1 - 0s - loss: 0.1921 - mse: 0.1921
Epoch 122/500
1/1 - 0s - loss: 0.1921 - mse: 0.1921
Epoch 123/500
1/1 - 0s - loss: 0.1921 - mse: 0.1921
Epoch 124/500
1/1 - 0s - loss: 0.1920 - mse: 0.1920
Epoch 125/500
1/1 - 0s - loss: 0.1920 - mse: 0.1920
Epoch 126/500
1/1 - 0s - loss: 0.1920 - mse: 0.1920
Epoch 127/500
1/1 - 0s - loss: 0.1920 - mse: 0.1920
Epoch 128/500
1/1 - 0s - loss: 0.1920 - mse: 0.1920
Epoch 129/500
1/1 - 0s - loss: 0.1919 - mse: 0.1919
Epoch 130/500
1/1 - 0s - loss: 0.1919 - mse: 0.1919
Epoch 131/500
1/1 - 0s - loss: 0.1919 - mse: 0.1919
Epoch 132/500
1/1 - 0s - loss: 0.1919 - mse: 0.1919
Epoch 133/500
1/1 - 0s - loss: 0.1919 - mse: 0.1919
Epoch 134/500
1/1 - 0s - loss: 0.1918 - mse: 0.1918
Epoch 135/500
1/1 - 0s - loss: 0.1918 - mse: 0.1918
Epoch 136/500
1/1 - 0s - loss: 0.1918 - mse: 0.1918
Epoch 137/500
1/1 - 0s - loss: 0.1918 - mse: 0.1918
Epoch 138/500
1/1 - 0s - loss: 0.1918 - mse: 0.1918
Epoch 139/500
1/1 - 0s - loss: 0.1917 - mse: 0.1917
Epoch 140/500
1/1 - 0s - loss: 0.1917 - mse: 0.1917
Epoch 141/500
1/1 - 0s - loss: 0.1917 - mse: 0.1917
Epoch 142/500
1/1 - 0s - loss: 0.1917 - mse: 0.1917
Epoch 143/500
1/1 - 0s - loss: 0.1917 - mse: 0.1917
Epoch 144/500
1/1 - 0s - loss: 0.1917 - mse: 0.1917
Epoch 145/500
1/1 - 0s - loss: 0.1916 - mse: 0.1916
Epoch 146/500
1/1 - 0s - loss: 0.1916 - mse: 0.1916
Epoch 147/500
1/1 - 0s - loss: 0.1916 - mse: 0.1916
Epoch 148/500
1/1 - 0s - loss: 0.1916 - mse: 0.1916
Epoch 149/500
1/1 - 0s - loss: 0.1916 - mse: 0.1916
Epoch 150/500
1/1 - 0s - loss: 0.1915 - mse: 0.1915
Epoch 151/500
1/1 - 0s - loss: 0.1915 - mse: 0.1915
Epoch 152/500
1/1 - 0s - loss: 0.1915 - mse: 0.1915
Epoch 153/500
1/1 - 0s - loss: 0.1915 - mse: 0.1915
Epoch 154/500
1/1 - 0s - loss: 0.1915 - mse: 0.1915
Epoch 155/500
1/1 - 0s - loss: 0.1914 - mse: 0.1914
Epoch 156/500
1/1 - 0s - loss: 0.1914 - mse: 0.1914
Epoch 157/500
1/1 - 0s - loss: 0.1914 - mse: 0.1914
Epoch 158/500
1/1 - 0s - loss: 0.1914 - mse: 0.1914
Epoch 159/500
1/1 - 0s - loss: 0.1913 - mse: 0.1913
Epoch 160/500
1/1 - 0s - loss: 0.1913 - mse: 0.1913
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 161/500
1/1 - 0s - loss: 0.1913 - mse: 0.1913
Epoch 162/500
1/1 - 0s - loss: 0.1913 - mse: 0.1913
Epoch 163/500
1/1 - 0s - loss: 0.1913 - mse: 0.1913
Epoch 164/500
1/1 - 0s - loss: 0.1912 - mse: 0.1912
Epoch 165/500
1/1 - 0s - loss: 0.1912 - mse: 0.1912
Epoch 166/500
1/1 - 0s - loss: 0.1912 - mse: 0.1912
Epoch 167/500
1/1 - 0s - loss: 0.1912 - mse: 0.1912
Epoch 168/500
1/1 - 0s - loss: 0.1912 - mse: 0.1912
Epoch 169/500
1/1 - 0s - loss: 0.1911 - mse: 0.1911
Epoch 170/500
1/1 - 0s - loss: 0.1911 - mse: 0.1911
Epoch 171/500
1/1 - 0s - loss: 0.1911 - mse: 0.1911
Epoch 172/500
1/1 - 0s - loss: 0.1911 - mse: 0.1911
Epoch 173/500
1/1 - 0s - loss: 0.1911 - mse: 0.1911
Epoch 174/500
1/1 - 0s - loss: 0.1910 - mse: 0.1910
Epoch 175/500
1/1 - 0s - loss: 0.1910 - mse: 0.1910
Epoch 176/500
1/1 - 0s - loss: 0.1910 - mse: 0.1910
Epoch 177/500
1/1 - 0s - loss: 0.1910 - mse: 0.1910
Epoch 178/500
1/1 - 0s - loss: 0.1910 - mse: 0.1910
Epoch 179/500
1/1 - 0s - loss: 0.1909 - mse: 0.1909
Epoch 180/500
1/1 - 0s - loss: 0.1909 - mse: 0.1909
Epoch 181/500
1/1 - 0s - loss: 0.1909 - mse: 0.1909
Epoch 182/500
1/1 - 0s - loss: 0.1909 - mse: 0.1909
Epoch 183/500
1/1 - 0s - loss: 0.1908 - mse: 0.1908
Epoch 184/500
1/1 - 0s - loss: 0.1908 - mse: 0.1908
Epoch 185/500
1/1 - 0s - loss: 0.1908 - mse: 0.1908
Epoch 186/500
1/1 - 0s - loss: 0.1908 - mse: 0.1908
Epoch 187/500
1/1 - 0s - loss: 0.1908 - mse: 0.1908
Epoch 188/500
1/1 - 0s - loss: 0.1907 - mse: 0.1907
Epoch 189/500
1/1 - 0s - loss: 0.1907 - mse: 0.1907
Epoch 190/500
1/1 - 0s - loss: 0.1907 - mse: 0.1907
Epoch 191/500
1/1 - 0s - loss: 0.1907 - mse: 0.1907
Epoch 192/500
1/1 - 0s - loss: 0.1907 - mse: 0.1907
Epoch 193/500
1/1 - 0s - loss: 0.1906 - mse: 0.1906
Epoch 194/500
1/1 - 0s - loss: 0.1906 - mse: 0.1906
Epoch 195/500
1/1 - 0s - loss: 0.1906 - mse: 0.1906
Epoch 196/500
1/1 - 0s - loss: 0.1906 - mse: 0.1906
Epoch 197/500
1/1 - 0s - loss: 0.1905 - mse: 0.1905
Epoch 198/500
1/1 - 0s - loss: 0.1905 - mse: 0.1905
Epoch 199/500
1/1 - 0s - loss: 0.1905 - mse: 0.1905
Epoch 200/500
1/1 - 0s - loss: 0.1905 - mse: 0.1905
Epoch 201/500
1/1 - 0s - loss: 0.1905 - mse: 0.1905
Epoch 202/500
1/1 - 0s - loss: 0.1904 - mse: 0.1904
Epoch 203/500
1/1 - 0s - loss: 0.1904 - mse: 0.1904
Epoch 204/500
1/1 - 0s - loss: 0.1904 - mse: 0.1904
Epoch 205/500
1/1 - 0s - loss: 0.1904 - mse: 0.1904
Epoch 206/500
1/1 - 0s - loss: 0.1903 - mse: 0.1903
Epoch 207/500
1/1 - 0s - loss: 0.1903 - mse: 0.1903
Epoch 208/500
1/1 - 0s - loss: 0.1903 - mse: 0.1903
Epoch 209/500
1/1 - 0s - loss: 0.1903 - mse: 0.1903
Epoch 210/500
1/1 - 0s - loss: 0.1902 - mse: 0.1902
Epoch 211/500
1/1 - 0s - loss: 0.1902 - mse: 0.1902
Epoch 212/500
1/1 - 0s - loss: 0.1902 - mse: 0.1902
Epoch 213/500
1/1 - 0s - loss: 0.1902 - mse: 0.1902
Epoch 214/500
1/1 - 0s - loss: 0.1902 - mse: 0.1902
Epoch 215/500
1/1 - 0s - loss: 0.1901 - mse: 0.1901
Epoch 216/500
1/1 - 0s - loss: 0.1901 - mse: 0.1901
Epoch 217/500
1/1 - 0s - loss: 0.1901 - mse: 0.1901
Epoch 218/500
1/1 - 0s - loss: 0.1901 - mse: 0.1901
Epoch 219/500
1/1 - 0s - loss: 0.1900 - mse: 0.1900
Epoch 220/500
1/1 - 0s - loss: 0.1900 - mse: 0.1900
Epoch 221/500
1/1 - 0s - loss: 0.1900 - mse: 0.1900
Epoch 222/500
1/1 - 0s - loss: 0.1900 - mse: 0.1900
Epoch 223/500
1/1 - 0s - loss: 0.1899 - mse: 0.1899
Epoch 224/500
1/1 - 0s - loss: 0.1899 - mse: 0.1899
Epoch 225/500
1/1 - 0s - loss: 0.1899 - mse: 0.1899
Epoch 226/500
1/1 - 0s - loss: 0.1899 - mse: 0.1899
Epoch 227/500
1/1 - 0s - loss: 0.1898 - mse: 0.1898
Epoch 228/500
1/1 - 0s - loss: 0.1898 - mse: 0.1898
Epoch 229/500
1/1 - 0s - loss: 0.1898 - mse: 0.1898
Epoch 230/500
1/1 - 0s - loss: 0.1898 - mse: 0.1898
Epoch 231/500
1/1 - 0s - loss: 0.1897 - mse: 0.1897
Epoch 232/500
1/1 - 0s - loss: 0.1897 - mse: 0.1897
Epoch 233/500
1/1 - 0s - loss: 0.1897 - mse: 0.1897
Epoch 234/500
1/1 - 0s - loss: 0.1897 - mse: 0.1897
Epoch 235/500
1/1 - 0s - loss: 0.1896 - mse: 0.1896
Epoch 236/500
1/1 - 0s - loss: 0.1896 - mse: 0.1896
Epoch 237/500
1/1 - 0s - loss: 0.1896 - mse: 0.1896
Epoch 238/500
1/1 - 0s - loss: 0.1896 - mse: 0.1896
Epoch 239/500
1/1 - 0s - loss: 0.1895 - mse: 0.1895
Epoch 240/500
1/1 - 0s - loss: 0.1895 - mse: 0.1895
Epoch 241/500
1/1 - 0s - loss: 0.1895 - mse: 0.1895
Epoch 242/500
1/1 - 0s - loss: 0.1895 - mse: 0.1895
Epoch 243/500
1/1 - 0s - loss: 0.1894 - mse: 0.1894
Epoch 244/500
1/1 - 0s - loss: 0.1894 - mse: 0.1894
Epoch 245/500
1/1 - 0s - loss: 0.1894 - mse: 0.1894
Epoch 246/500
1/1 - 0s - loss: 0.1894 - mse: 0.1894
Epoch 247/500
1/1 - 0s - loss: 0.1893 - mse: 0.1893
Epoch 248/500
1/1 - 0s - loss: 0.1893 - mse: 0.1893
Epoch 249/500
1/1 - 0s - loss: 0.1893 - mse: 0.1893
Epoch 250/500
1/1 - 0s - loss: 0.1893 - mse: 0.1893
Epoch 251/500
1/1 - 0s - loss: 0.1892 - mse: 0.1892
Epoch 252/500
1/1 - 0s - loss: 0.1892 - mse: 0.1892
Epoch 253/500
1/1 - 0s - loss: 0.1892 - mse: 0.1892
Epoch 254/500
1/1 - 0s - loss: 0.1892 - mse: 0.1892
Epoch 255/500
1/1 - 0s - loss: 0.1891 - mse: 0.1891
Epoch 256/500
1/1 - 0s - loss: 0.1891 - mse: 0.1891
Epoch 257/500
1/1 - 0s - loss: 0.1891 - mse: 0.1891
Epoch 258/500
1/1 - 0s - loss: 0.1891 - mse: 0.1891
Epoch 259/500
1/1 - 0s - loss: 0.1890 - mse: 0.1890
Epoch 260/500
1/1 - 0s - loss: 0.1890 - mse: 0.1890
Epoch 261/500
1/1 - 0s - loss: 0.1890 - mse: 0.1890
Epoch 262/500
1/1 - 0s - loss: 0.1889 - mse: 0.1889
Epoch 263/500
1/1 - 0s - loss: 0.1889 - mse: 0.1889
Epoch 264/500
1/1 - 0s - loss: 0.1889 - mse: 0.1889
Epoch 265/500
1/1 - 0s - loss: 0.1889 - mse: 0.1889
Epoch 266/500
1/1 - 0s - loss: 0.1888 - mse: 0.1888
Epoch 267/500
1/1 - 0s - loss: 0.1888 - mse: 0.1888
Epoch 268/500
1/1 - 0s - loss: 0.1888 - mse: 0.1888
Epoch 269/500
1/1 - 0s - loss: 0.1888 - mse: 0.1888
Epoch 270/500
1/1 - 0s - loss: 0.1887 - mse: 0.1887
Epoch 271/500
1/1 - 0s - loss: 0.1887 - mse: 0.1887
Epoch 272/500
1/1 - 0s - loss: 0.1887 - mse: 0.1887
Epoch 273/500
1/1 - 0s - loss: 0.1886 - mse: 0.1886
Epoch 274/500
1/1 - 0s - loss: 0.1886 - mse: 0.1886
Epoch 275/500
1/1 - 0s - loss: 0.1886 - mse: 0.1886
Epoch 276/500
1/1 - 0s - loss: 0.1886 - mse: 0.1886
Epoch 277/500
1/1 - 0s - loss: 0.1885 - mse: 0.1885
Epoch 278/500
1/1 - 0s - loss: 0.1885 - mse: 0.1885
Epoch 279/500
1/1 - 0s - loss: 0.1885 - mse: 0.1885
Epoch 280/500
1/1 - 0s - loss: 0.1885 - mse: 0.1885
Epoch 281/500
1/1 - 0s - loss: 0.1884 - mse: 0.1884
Epoch 282/500
1/1 - 0s - loss: 0.1884 - mse: 0.1884
Epoch 283/500
1/1 - 0s - loss: 0.1884 - mse: 0.1884
Epoch 284/500
1/1 - 0s - loss: 0.1883 - mse: 0.1883
Epoch 285/500
1/1 - 0s - loss: 0.1883 - mse: 0.1883
Epoch 286/500
1/1 - 0s - loss: 0.1883 - mse: 0.1883
Epoch 287/500
1/1 - 0s - loss: 0.1883 - mse: 0.1883
Epoch 288/500
1/1 - 0s - loss: 0.1882 - mse: 0.1882
Epoch 289/500
1/1 - 0s - loss: 0.1882 - mse: 0.1882
Epoch 290/500
1/1 - 0s - loss: 0.1882 - mse: 0.1882
Epoch 291/500
1/1 - 0s - loss: 0.1881 - mse: 0.1881
Epoch 292/500
1/1 - 0s - loss: 0.1881 - mse: 0.1881
Epoch 293/500
1/1 - 0s - loss: 0.1881 - mse: 0.1881
Epoch 294/500
1/1 - 0s - loss: 0.1880 - mse: 0.1880
Epoch 295/500
1/1 - 0s - loss: 0.1880 - mse: 0.1880
Epoch 296/500
1/1 - 0s - loss: 0.1880 - mse: 0.1880
Epoch 297/500
1/1 - 0s - loss: 0.1880 - mse: 0.1880
Epoch 298/500
1/1 - 0s - loss: 0.1879 - mse: 0.1879
Epoch 299/500
1/1 - 0s - loss: 0.1879 - mse: 0.1879
Epoch 300/500
1/1 - 0s - loss: 0.1879 - mse: 0.1879
Epoch 301/500
1/1 - 0s - loss: 0.1878 - mse: 0.1878
Epoch 302/500
1/1 - 0s - loss: 0.1878 - mse: 0.1878
Epoch 303/500
1/1 - 0s - loss: 0.1878 - mse: 0.1878
Epoch 304/500
1/1 - 0s - loss: 0.1877 - mse: 0.1877
Epoch 305/500
1/1 - 0s - loss: 0.1877 - mse: 0.1877
Epoch 306/500
1/1 - 0s - loss: 0.1877 - mse: 0.1877
Epoch 307/500
1/1 - 0s - loss: 0.1877 - mse: 0.1877
Epoch 308/500
1/1 - 0s - loss: 0.1876 - mse: 0.1876
Epoch 309/500
1/1 - 0s - loss: 0.1876 - mse: 0.1876
Epoch 310/500
1/1 - 0s - loss: 0.1876 - mse: 0.1876
Epoch 311/500
1/1 - 0s - loss: 0.1875 - mse: 0.1875
Epoch 312/500
1/1 - 0s - loss: 0.1875 - mse: 0.1875
Epoch 313/500
1/1 - 0s - loss: 0.1875 - mse: 0.1875
Epoch 314/500
1/1 - 0s - loss: 0.1874 - mse: 0.1874
Epoch 315/500
1/1 - 0s - loss: 0.1874 - mse: 0.1874
Epoch 316/500
1/1 - 0s - loss: 0.1874 - mse: 0.1874
Epoch 317/500
1/1 - 0s - loss: 0.1873 - mse: 0.1873
Epoch 318/500
1/1 - 0s - loss: 0.1873 - mse: 0.1873
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 319/500
1/1 - 0s - loss: 0.1873 - mse: 0.1873
Epoch 320/500
1/1 - 0s - loss: 0.1873 - mse: 0.1873
Epoch 321/500
1/1 - 0s - loss: 0.1872 - mse: 0.1872
Epoch 322/500
1/1 - 0s - loss: 0.1872 - mse: 0.1872
Epoch 323/500
1/1 - 0s - loss: 0.1872 - mse: 0.1872
Epoch 324/500
1/1 - 0s - loss: 0.1871 - mse: 0.1871
Epoch 325/500
1/1 - 0s - loss: 0.1871 - mse: 0.1871
Epoch 326/500
1/1 - 0s - loss: 0.1871 - mse: 0.1871
Epoch 327/500
1/1 - 0s - loss: 0.1870 - mse: 0.1870
Epoch 328/500
1/1 - 0s - loss: 0.1870 - mse: 0.1870
Epoch 329/500
1/1 - 0s - loss: 0.1870 - mse: 0.1870
Epoch 330/500
1/1 - 0s - loss: 0.1869 - mse: 0.1869
Epoch 331/500
1/1 - 0s - loss: 0.1869 - mse: 0.1869
Epoch 332/500
1/1 - 0s - loss: 0.1869 - mse: 0.1869
Epoch 333/500
1/1 - 0s - loss: 0.1868 - mse: 0.1868
Epoch 334/500
1/1 - 0s - loss: 0.1868 - mse: 0.1868
Epoch 335/500
1/1 - 0s - loss: 0.1868 - mse: 0.1868
Epoch 336/500
1/1 - 0s - loss: 0.1867 - mse: 0.1867
Epoch 337/500
1/1 - 0s - loss: 0.1867 - mse: 0.1867
Epoch 338/500
1/1 - 0s - loss: 0.1867 - mse: 0.1867
Epoch 339/500
1/1 - 0s - loss: 0.1866 - mse: 0.1866
Epoch 340/500
1/1 - 0s - loss: 0.1866 - mse: 0.1866
Epoch 341/500
1/1 - 0s - loss: 0.1866 - mse: 0.1866
Epoch 342/500
1/1 - 0s - loss: 0.1865 - mse: 0.1865
Epoch 343/500
1/1 - 0s - loss: 0.1865 - mse: 0.1865
Epoch 344/500
1/1 - 0s - loss: 0.1865 - mse: 0.1865
Epoch 345/500
1/1 - 0s - loss: 0.1864 - mse: 0.1864
Epoch 346/500
1/1 - 0s - loss: 0.1864 - mse: 0.1864
Epoch 347/500
1/1 - 0s - loss: 0.1864 - mse: 0.1864
Epoch 348/500
1/1 - 0s - loss: 0.1863 - mse: 0.1863
Epoch 349/500
1/1 - 0s - loss: 0.1863 - mse: 0.1863
Epoch 350/500
1/1 - 0s - loss: 0.1863 - mse: 0.1863
Epoch 351/500
1/1 - 0s - loss: 0.1862 - mse: 0.1862
Epoch 352/500
1/1 - 0s - loss: 0.1862 - mse: 0.1862
Epoch 353/500
1/1 - 0s - loss: 0.1862 - mse: 0.1862
Epoch 354/500
1/1 - 0s - loss: 0.1861 - mse: 0.1861
Epoch 355/500
1/1 - 0s - loss: 0.1861 - mse: 0.1861
Epoch 356/500
1/1 - 0s - loss: 0.1861 - mse: 0.1861
Epoch 357/500
1/1 - 0s - loss: 0.1860 - mse: 0.1860
Epoch 358/500
1/1 - 0s - loss: 0.1860 - mse: 0.1860
Epoch 359/500
1/1 - 0s - loss: 0.1859 - mse: 0.1859
Epoch 360/500
1/1 - 0s - loss: 0.1859 - mse: 0.1859
Epoch 361/500
1/1 - 0s - loss: 0.1859 - mse: 0.1859
Epoch 362/500
1/1 - 0s - loss: 0.1858 - mse: 0.1858
Epoch 363/500
1/1 - 0s - loss: 0.1858 - mse: 0.1858
Epoch 364/500
1/1 - 0s - loss: 0.1858 - mse: 0.1858
Epoch 365/500
1/1 - 0s - loss: 0.1857 - mse: 0.1857
Epoch 366/500
1/1 - 0s - loss: 0.1857 - mse: 0.1857
Epoch 367/500
1/1 - 0s - loss: 0.1857 - mse: 0.1857
Epoch 368/500
1/1 - 0s - loss: 0.1856 - mse: 0.1856
Epoch 369/500
1/1 - 0s - loss: 0.1856 - mse: 0.1856
Epoch 370/500
1/1 - 0s - loss: 0.1856 - mse: 0.1856
Epoch 371/500
1/1 - 0s - loss: 0.1855 - mse: 0.1855
Epoch 372/500
1/1 - 0s - loss: 0.1855 - mse: 0.1855
Epoch 373/500
1/1 - 0s - loss: 0.1854 - mse: 0.1854
Epoch 374/500
1/1 - 0s - loss: 0.1854 - mse: 0.1854
Epoch 375/500
1/1 - 0s - loss: 0.1854 - mse: 0.1854
Epoch 376/500
1/1 - 0s - loss: 0.1853 - mse: 0.1853
Epoch 377/500
1/1 - 0s - loss: 0.1853 - mse: 0.1853
Epoch 378/500
1/1 - 0s - loss: 0.1853 - mse: 0.1853
Epoch 379/500
1/1 - 0s - loss: 0.1852 - mse: 0.1852
Epoch 380/500
1/1 - 0s - loss: 0.1852 - mse: 0.1852
Epoch 381/500
1/1 - 0s - loss: 0.1851 - mse: 0.1851
Epoch 382/500
1/1 - 0s - loss: 0.1851 - mse: 0.1851
Epoch 383/500
1/1 - 0s - loss: 0.1851 - mse: 0.1851
Epoch 384/500
1/1 - 0s - loss: 0.1850 - mse: 0.1850
Epoch 385/500
1/1 - 0s - loss: 0.1850 - mse: 0.1850
Epoch 386/500
1/1 - 0s - loss: 0.1850 - mse: 0.1850
Epoch 387/500
1/1 - 0s - loss: 0.1849 - mse: 0.1849
Epoch 388/500
1/1 - 0s - loss: 0.1849 - mse: 0.1849
Epoch 389/500
1/1 - 0s - loss: 0.1848 - mse: 0.1848
Epoch 390/500
1/1 - 0s - loss: 0.1848 - mse: 0.1848
Epoch 391/500
1/1 - 0s - loss: 0.1848 - mse: 0.1848
Epoch 392/500
1/1 - 0s - loss: 0.1847 - mse: 0.1847
Epoch 393/500
1/1 - 0s - loss: 0.1847 - mse: 0.1847
Epoch 394/500
1/1 - 0s - loss: 0.1846 - mse: 0.1846
Epoch 395/500
1/1 - 0s - loss: 0.1846 - mse: 0.1846
Epoch 396/500
1/1 - 0s - loss: 0.1846 - mse: 0.1846
Epoch 397/500
1/1 - 0s - loss: 0.1845 - mse: 0.1845
Epoch 398/500
1/1 - 0s - loss: 0.1845 - mse: 0.1845
Epoch 399/500
1/1 - 0s - loss: 0.1845 - mse: 0.1845
Epoch 400/500
1/1 - 0s - loss: 0.1844 - mse: 0.1844
Epoch 401/500
1/1 - 0s - loss: 0.1844 - mse: 0.1844
Epoch 402/500
1/1 - 0s - loss: 0.1843 - mse: 0.1843
Epoch 403/500
1/1 - 0s - loss: 0.1843 - mse: 0.1843
Epoch 404/500
1/1 - 0s - loss: 0.1843 - mse: 0.1843
Epoch 405/500
1/1 - 0s - loss: 0.1842 - mse: 0.1842
Epoch 406/500
1/1 - 0s - loss: 0.1842 - mse: 0.1842
Epoch 407/500
1/1 - 0s - loss: 0.1841 - mse: 0.1841
Epoch 408/500
1/1 - 0s - loss: 0.1841 - mse: 0.1841
Epoch 409/500
1/1 - 0s - loss: 0.1841 - mse: 0.1841
Epoch 410/500
1/1 - 0s - loss: 0.1840 - mse: 0.1840
Epoch 411/500
1/1 - 0s - loss: 0.1840 - mse: 0.1840
Epoch 412/500
1/1 - 0s - loss: 0.1839 - mse: 0.1839
Epoch 413/500
1/1 - 0s - loss: 0.1839 - mse: 0.1839
Epoch 414/500
1/1 - 0s - loss: 0.1839 - mse: 0.1839
Epoch 415/500
1/1 - 0s - loss: 0.1838 - mse: 0.1838
Epoch 416/500
1/1 - 0s - loss: 0.1838 - mse: 0.1838
Epoch 417/500
1/1 - 0s - loss: 0.1837 - mse: 0.1837
Epoch 418/500
1/1 - 0s - loss: 0.1837 - mse: 0.1837
Epoch 419/500
1/1 - 0s - loss: 0.1836 - mse: 0.1836
Epoch 420/500
1/1 - 0s - loss: 0.1836 - mse: 0.1836
Epoch 421/500
1/1 - 0s - loss: 0.1836 - mse: 0.1836
Epoch 422/500
1/1 - 0s - loss: 0.1835 - mse: 0.1835
Epoch 423/500
1/1 - 0s - loss: 0.1835 - mse: 0.1835
Epoch 424/500
1/1 - 0s - loss: 0.1834 - mse: 0.1834
Epoch 425/500
1/1 - 0s - loss: 0.1834 - mse: 0.1834
Epoch 426/500
1/1 - 0s - loss: 0.1834 - mse: 0.1834
Epoch 427/500
1/1 - 0s - loss: 0.1833 - mse: 0.1833
Epoch 428/500
1/1 - 0s - loss: 0.1833 - mse: 0.1833
Epoch 429/500
1/1 - 0s - loss: 0.1832 - mse: 0.1832
Epoch 430/500
1/1 - 0s - loss: 0.1832 - mse: 0.1832
Epoch 431/500
1/1 - 0s - loss: 0.1831 - mse: 0.1831
Epoch 432/500
1/1 - 0s - loss: 0.1831 - mse: 0.1831
Epoch 433/500
1/1 - 0s - loss: 0.1831 - mse: 0.1831
Epoch 434/500
1/1 - 0s - loss: 0.1830 - mse: 0.1830
Epoch 435/500
1/1 - 0s - loss: 0.1830 - mse: 0.1830
Epoch 436/500
1/1 - 0s - loss: 0.1829 - mse: 0.1829
Epoch 437/500
1/1 - 0s - loss: 0.1829 - mse: 0.1829
Epoch 438/500
1/1 - 0s - loss: 0.1828 - mse: 0.1828
Epoch 439/500
1/1 - 0s - loss: 0.1828 - mse: 0.1828
Epoch 440/500
1/1 - 0s - loss: 0.1828 - mse: 0.1828
Epoch 441/500
1/1 - 0s - loss: 0.1827 - mse: 0.1827
Epoch 442/500
1/1 - 0s - loss: 0.1827 - mse: 0.1827
Epoch 443/500
1/1 - 0s - loss: 0.1826 - mse: 0.1826
Epoch 444/500
1/1 - 0s - loss: 0.1826 - mse: 0.1826
Epoch 445/500
1/1 - 0s - loss: 0.1825 - mse: 0.1825
Epoch 446/500
1/1 - 0s - loss: 0.1825 - mse: 0.1825
Epoch 447/500
1/1 - 0s - loss: 0.1825 - mse: 0.1825
Epoch 448/500
1/1 - 0s - loss: 0.1824 - mse: 0.1824
Epoch 449/500
1/1 - 0s - loss: 0.1824 - mse: 0.1824
Epoch 450/500
1/1 - 0s - loss: 0.1823 - mse: 0.1823
Epoch 451/500
1/1 - 0s - loss: 0.1823 - mse: 0.1823
Epoch 452/500
1/1 - 0s - loss: 0.1822 - mse: 0.1822
Epoch 453/500
1/1 - 0s - loss: 0.1822 - mse: 0.1822
Epoch 454/500
1/1 - 0s - loss: 0.1821 - mse: 0.1821
Epoch 455/500
1/1 - 0s - loss: 0.1821 - mse: 0.1821
Epoch 456/500
1/1 - 0s - loss: 0.1821 - mse: 0.1821
Epoch 457/500
1/1 - 0s - loss: 0.1820 - mse: 0.1820
Epoch 458/500
1/1 - 0s - loss: 0.1820 - mse: 0.1820
Epoch 459/500
1/1 - 0s - loss: 0.1819 - mse: 0.1819
Epoch 460/500
1/1 - 0s - loss: 0.1819 - mse: 0.1819
Epoch 461/500
1/1 - 0s - loss: 0.1818 - mse: 0.1818
Epoch 462/500
1/1 - 0s - loss: 0.1818 - mse: 0.1818
Epoch 463/500
1/1 - 0s - loss: 0.1817 - mse: 0.1817
Epoch 464/500
1/1 - 0s - loss: 0.1817 - mse: 0.1817
Epoch 465/500
1/1 - 0s - loss: 0.1816 - mse: 0.1816
Epoch 466/500
1/1 - 0s - loss: 0.1816 - mse: 0.1816
Epoch 467/500
1/1 - 0s - loss: 0.1816 - mse: 0.1816
Epoch 468/500
1/1 - 0s - loss: 0.1815 - mse: 0.1815
Epoch 469/500
1/1 - 0s - loss: 0.1815 - mse: 0.1815
Epoch 470/500
1/1 - 0s - loss: 0.1814 - mse: 0.1814
Epoch 471/500
1/1 - 0s - loss: 0.1814 - mse: 0.1814
Epoch 472/500
1/1 - 0s - loss: 0.1813 - mse: 0.1813
Epoch 473/500
1/1 - 0s - loss: 0.1813 - mse: 0.1813
Epoch 474/500
1/1 - 0s - loss: 0.1812 - mse: 0.1812
Epoch 475/500
1/1 - 0s - loss: 0.1812 - mse: 0.1812
Epoch 476/500
1/1 - 0s - loss: 0.1811 - mse: 0.1811
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 477/500
1/1 - 0s - loss: 0.1811 - mse: 0.1811
Epoch 478/500
1/1 - 0s - loss: 0.1810 - mse: 0.1810
Epoch 479/500
1/1 - 0s - loss: 0.1810 - mse: 0.1810
Epoch 480/500
1/1 - 0s - loss: 0.1809 - mse: 0.1809
Epoch 481/500
1/1 - 0s - loss: 0.1809 - mse: 0.1809
Epoch 482/500
1/1 - 0s - loss: 0.1808 - mse: 0.1808
Epoch 483/500
1/1 - 0s - loss: 0.1808 - mse: 0.1808
Epoch 484/500
1/1 - 0s - loss: 0.1808 - mse: 0.1808
Epoch 485/500
1/1 - 0s - loss: 0.1807 - mse: 0.1807
Epoch 486/500
1/1 - 0s - loss: 0.1807 - mse: 0.1807
Epoch 487/500
1/1 - 0s - loss: 0.1806 - mse: 0.1806
Epoch 488/500
1/1 - 0s - loss: 0.1806 - mse: 0.1806
Epoch 489/500
1/1 - 0s - loss: 0.1805 - mse: 0.1805
Epoch 490/500
1/1 - 0s - loss: 0.1805 - mse: 0.1805
Epoch 491/500
1/1 - 0s - loss: 0.1804 - mse: 0.1804
Epoch 492/500
1/1 - 0s - loss: 0.1804 - mse: 0.1804
Epoch 493/500
1/1 - 0s - loss: 0.1803 - mse: 0.1803
Epoch 494/500
1/1 - 0s - loss: 0.1803 - mse: 0.1803
Epoch 495/500
1/1 - 0s - loss: 0.1802 - mse: 0.1802
Epoch 496/500
1/1 - 0s - loss: 0.1802 - mse: 0.1802
Epoch 497/500
1/1 - 0s - loss: 0.1801 - mse: 0.1801
Epoch 498/500
1/1 - 0s - loss: 0.1801 - mse: 0.1801
Epoch 499/500
1/1 - 0s - loss: 0.1800 - mse: 0.1800
Epoch 500/500
1/1 - 0s - loss: 0.1800 - mse: 0.1800
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tensorflow.python.keras.callbacks.History at 0x1584ccfa0&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Obtain the predictions on the training and validation data.</span>
<span class="n">keras_train_preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">keras_validation_preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_validation</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="evaluate-nn-3">
<h4>Evaluate NN 3<a class="headerlink" href="#evaluate-nn-3" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the AUC on the training and validation data.</span>
<span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">keras_train_preds</span><span class="p">),</span> <span class="s1">&#39;validation&#39;</span><span class="p">:</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">keras_validation_preds</span><span class="p">)}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;train&#39;: 0.8112781367883518, &#39;validation&#39;: 0.8222811671087532}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare the AUCs under all models built to date.</span>
<span class="p">{</span><span class="s1">&#39;1. NN 3&#39;</span><span class="p">:</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">keras_validation_preds</span><span class="p">),</span>
 <span class="s1">&#39;2. NN 2&#39;</span><span class="p">:</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">pred_nn2_validation</span><span class="p">),</span>
 <span class="s1">&#39;3. NN 1&#39;</span><span class="p">:</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">pred_nn1_validation</span><span class="p">),</span>
 <span class="s1">&#39;4. GBM final&#39;</span><span class="p">:</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">validation_gbm_preds_final</span><span class="p">)}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;1. NN 3&#39;: 0.8222811671087532,
 &#39;2. NN 2&#39;: 0.8250833589715872,
 &#39;3. NN 1&#39;: 0.8414724395699427,
 &#39;4. GBM final&#39;: 0.861474435196195}
</pre></div>
</div>
</div>
</div>
<p>As expected, the AUC for NN 3 (Neural network with 4 hidden neurons using Keras) is very similar to that for NN 2 (neural network with 4 hidden neurons built from first principles). All three neural networks underperform the GBM final model.</p>
</div>
<div class="section" id="fit-a-more-complex-model-with-keras">
<h4>Fit a more complex model with Keras<a class="headerlink" href="#fit-a-more-complex-model-with-keras" title="Permalink to this headline">¶</a></h4>
<p>Now, the power of Keras will be used to easily extend the simple single hidden layer model into a more complex neural network.</p>
<p>There are many features available in Keras. The code below makes the following relatively simple adjustments to NN 3:</p>
<ul class="simple">
<li><p>add a second hidden (dense) layer;</p></li>
<li><p>increase the number of neurons in each hidden layer to eight;</p></li>
<li><p>use a ReLU activation function for the hidden layers;</p></li>
<li><p>apply regularisation to the weights to avoid the model overfitting to the training data;</p></li>
<li><p>use a binary cross-entropy loss (logistic loss for a binary classifier); and</p></li>
<li><p>use the more advanced Adam optimizer, in place of standard backpropagation.</p></li>
</ul>
<p>Details of these options and more can be found on the keras webpage: <a class="reference external" href="https://keras.io/">https://keras.io/</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1235</span><span class="p">)</span>

<span class="c1"># Construct the adjusted neural network model.</span>
<span class="n">model2</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">))</span>
<span class="n">model2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">))</span>
<span class="n">model2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
<span class="c1"># Compile the model using the Adam optimiser and the logistic loss function</span>
<span class="c1"># for a binary classifier (binary_cross_entropy).</span>
<span class="n">model2</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/100
5/5 [==============================] - 0s 2ms/step - loss: 1.0556 - binary_crossentropy: 0.8496
Epoch 2/100
5/5 [==============================] - 0s 2ms/step - loss: 1.0217 - binary_crossentropy: 0.8192
Epoch 3/100
5/5 [==============================] - 0s 2ms/step - loss: 0.9922 - binary_crossentropy: 0.7932
Epoch 4/100
5/5 [==============================] - 0s 2ms/step - loss: 0.9665 - binary_crossentropy: 0.7710
Epoch 5/100
5/5 [==============================] - 0s 2ms/step - loss: 0.9437 - binary_crossentropy: 0.7518
Epoch 6/100
5/5 [==============================] - 0s 1ms/step - loss: 0.9236 - binary_crossentropy: 0.7353
Epoch 7/100
5/5 [==============================] - 0s 1ms/step - loss: 0.9055 - binary_crossentropy: 0.7208
Epoch 8/100
5/5 [==============================] - 0s 1ms/step - loss: 0.8894 - binary_crossentropy: 0.7083
Epoch 9/100
5/5 [==============================] - 0s 1ms/step - loss: 0.8748 - binary_crossentropy: 0.6973
Epoch 10/100
5/5 [==============================] - 0s 1ms/step - loss: 0.8612 - binary_crossentropy: 0.6870
Epoch 11/100
5/5 [==============================] - 0s 1ms/step - loss: 0.8485 - binary_crossentropy: 0.6777
Epoch 12/100
5/5 [==============================] - 0s 1ms/step - loss: 0.8360 - binary_crossentropy: 0.6686
Epoch 13/100
5/5 [==============================] - 0s 1ms/step - loss: 0.8238 - binary_crossentropy: 0.6596
Epoch 14/100
5/5 [==============================] - 0s 1ms/step - loss: 0.8118 - binary_crossentropy: 0.6507
Epoch 15/100
5/5 [==============================] - 0s 1ms/step - loss: 0.7996 - binary_crossentropy: 0.6414
Epoch 16/100
5/5 [==============================] - 0s 1ms/step - loss: 0.7865 - binary_crossentropy: 0.6312
Epoch 17/100
5/5 [==============================] - 0s 1ms/step - loss: 0.7721 - binary_crossentropy: 0.6194
Epoch 18/100
5/5 [==============================] - 0s 1ms/step - loss: 0.7565 - binary_crossentropy: 0.6064
Epoch 19/100
5/5 [==============================] - 0s 1ms/step - loss: 0.7403 - binary_crossentropy: 0.5925
Epoch 20/100
5/5 [==============================] - 0s 1ms/step - loss: 0.7251 - binary_crossentropy: 0.5796
Epoch 21/100
5/5 [==============================] - 0s 1ms/step - loss: 0.7104 - binary_crossentropy: 0.5670
Epoch 22/100
5/5 [==============================] - 0s 1ms/step - loss: 0.6975 - binary_crossentropy: 0.5563
Epoch 23/100
5/5 [==============================] - 0s 1ms/step - loss: 0.6867 - binary_crossentropy: 0.5477
Epoch 24/100
5/5 [==============================] - 0s 2ms/step - loss: 0.6764 - binary_crossentropy: 0.5396
Epoch 25/100
5/5 [==============================] - 0s 1ms/step - loss: 0.6673 - binary_crossentropy: 0.5329
Epoch 26/100
5/5 [==============================] - 0s 1ms/step - loss: 0.6588 - binary_crossentropy: 0.5267
Epoch 27/100
5/5 [==============================] - 0s 1ms/step - loss: 0.6510 - binary_crossentropy: 0.5211
Epoch 28/100
5/5 [==============================] - 0s 2ms/step - loss: 0.6437 - binary_crossentropy: 0.5161
Epoch 29/100
5/5 [==============================] - 0s 1ms/step - loss: 0.6371 - binary_crossentropy: 0.5117
Epoch 30/100
5/5 [==============================] - 0s 1ms/step - loss: 0.6308 - binary_crossentropy: 0.5075
Epoch 31/100
5/5 [==============================] - 0s 1ms/step - loss: 0.6248 - binary_crossentropy: 0.5037
Epoch 32/100
5/5 [==============================] - 0s 1ms/step - loss: 0.6192 - binary_crossentropy: 0.5002
Epoch 33/100
5/5 [==============================] - 0s 1ms/step - loss: 0.6137 - binary_crossentropy: 0.4968
Epoch 34/100
5/5 [==============================] - 0s 1ms/step - loss: 0.6087 - binary_crossentropy: 0.4937
Epoch 35/100
5/5 [==============================] - 0s 2ms/step - loss: 0.6038 - binary_crossentropy: 0.4909
Epoch 36/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5991 - binary_crossentropy: 0.4881
Epoch 37/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5947 - binary_crossentropy: 0.4856
Epoch 38/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5905 - binary_crossentropy: 0.4833
Epoch 39/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5864 - binary_crossentropy: 0.4811
Epoch 40/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5826 - binary_crossentropy: 0.4791
Epoch 41/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5789 - binary_crossentropy: 0.4771
Epoch 42/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5753 - binary_crossentropy: 0.4753
Epoch 43/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5719 - binary_crossentropy: 0.4736
Epoch 44/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5686 - binary_crossentropy: 0.4720
Epoch 45/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5654 - binary_crossentropy: 0.4705
Epoch 46/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5623 - binary_crossentropy: 0.4691
Epoch 47/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5594 - binary_crossentropy: 0.4677
Epoch 48/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5565 - binary_crossentropy: 0.4663
Epoch 49/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5537 - binary_crossentropy: 0.4651
Epoch 50/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5510 - binary_crossentropy: 0.4639
Epoch 51/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5483 - binary_crossentropy: 0.4627
Epoch 52/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5457 - binary_crossentropy: 0.4616
Epoch 53/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5432 - binary_crossentropy: 0.4605
Epoch 54/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5408 - binary_crossentropy: 0.4596
Epoch 55/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5385 - binary_crossentropy: 0.4586
Epoch 56/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5362 - binary_crossentropy: 0.4577
Epoch 57/100
5/5 [==============================] - 0s 3ms/step - loss: 0.5340 - binary_crossentropy: 0.4568
Epoch 58/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5318 - binary_crossentropy: 0.4559
Epoch 59/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5297 - binary_crossentropy: 0.4550
Epoch 60/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5276 - binary_crossentropy: 0.4542
Epoch 61/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5256 - binary_crossentropy: 0.4534
Epoch 62/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5237 - binary_crossentropy: 0.4526
Epoch 63/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5218 - binary_crossentropy: 0.4519
Epoch 64/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5199 - binary_crossentropy: 0.4512
Epoch 65/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5182 - binary_crossentropy: 0.4507
Epoch 66/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5164 - binary_crossentropy: 0.4500
Epoch 67/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5147 - binary_crossentropy: 0.4494
Epoch 68/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5131 - binary_crossentropy: 0.4488
Epoch 69/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5115 - binary_crossentropy: 0.4482
Epoch 70/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5099 - binary_crossentropy: 0.4476
Epoch 71/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5084 - binary_crossentropy: 0.4471
Epoch 72/100
5/5 [==============================] - 0s 2ms/step - loss: 0.5069 - binary_crossentropy: 0.4467
Epoch 73/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5055 - binary_crossentropy: 0.4461
Epoch 74/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5040 - binary_crossentropy: 0.4455
Epoch 75/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5027 - binary_crossentropy: 0.4451
Epoch 76/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5014 - binary_crossentropy: 0.4446
Epoch 77/100
5/5 [==============================] - 0s 1ms/step - loss: 0.5000 - binary_crossentropy: 0.4441
Epoch 78/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4987 - binary_crossentropy: 0.4437
Epoch 79/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4975 - binary_crossentropy: 0.4433
Epoch 80/100
5/5 [==============================] - 0s 2ms/step - loss: 0.4963 - binary_crossentropy: 0.4429
Epoch 81/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4950 - binary_crossentropy: 0.4424
Epoch 82/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4938 - binary_crossentropy: 0.4420
Epoch 83/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4926 - binary_crossentropy: 0.4416
Epoch 84/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4915 - binary_crossentropy: 0.4411
Epoch 85/100
5/5 [==============================] - 0s 2ms/step - loss: 0.4903 - binary_crossentropy: 0.4407
Epoch 86/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4893 - binary_crossentropy: 0.4404
Epoch 87/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4881 - binary_crossentropy: 0.4399
Epoch 88/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4871 - binary_crossentropy: 0.4395
Epoch 89/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4861 - binary_crossentropy: 0.4391
Epoch 90/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4851 - binary_crossentropy: 0.4388
Epoch 91/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4841 - binary_crossentropy: 0.4384
Epoch 92/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4832 - binary_crossentropy: 0.4381
Epoch 93/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4823 - binary_crossentropy: 0.4377
Epoch 94/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4814 - binary_crossentropy: 0.4374
Epoch 95/100
5/5 [==============================] - 0s 2ms/step - loss: 0.4806 - binary_crossentropy: 0.4370
Epoch 96/100
5/5 [==============================] - 0s 2ms/step - loss: 0.4797 - binary_crossentropy: 0.4366
Epoch 97/100
5/5 [==============================] - 0s 2ms/step - loss: 0.4789 - binary_crossentropy: 0.4363
Epoch 98/100
5/5 [==============================] - 0s 1ms/step - loss: 0.4781 - binary_crossentropy: 0.4359
Epoch 99/100
5/5 [==============================] - 0s 2ms/step - loss: 0.4773 - binary_crossentropy: 0.4356
Epoch 100/100
5/5 [==============================] - 0s 2ms/step - loss: 0.4766 - binary_crossentropy: 0.4355
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tensorflow.python.keras.callbacks.History at 0x1594ec880&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Obtain the predictions on the training and validation data.</span>
<span class="n">keras2_train_preds</span> <span class="o">=</span> <span class="n">model2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">keras2_validation_preds</span> <span class="o">=</span> <span class="n">model2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_validation</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="evaluate-nn-4">
<h4>Evaluate NN 4<a class="headerlink" href="#evaluate-nn-4" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the AUC on the training and validation data.</span>
<span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">keras2_train_preds</span><span class="p">),</span> <span class="s1">&#39;validation&#39;</span><span class="p">:</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">keras2_validation_preds</span><span class="p">)}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;train&#39;: 0.8345909687506179, &#39;validation&#39;: 0.8480247457655306}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the relative improvement in AUC between NN 4 and NN 3</span>
<span class="c1"># using the validation data.</span>
<span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">keras2_validation_preds</span><span class="p">)</span> <span class="o">/</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">keras_validation_preds</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.031307513398726
</pre></div>
</div>
</div>
</div>
<p>The modifications made to the first Keras model (NN 3) to produce NN 4 have resulted in a slight improvement in the AUC.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare the results of all five models built in this notebook.</span>
<span class="p">{</span><span class="s1">&#39;1. NN 4 (Keras2)&#39;</span><span class="p">:</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">keras2_validation_preds</span><span class="p">),</span>
 <span class="s1">&#39;2. NN 3 (Keras1)&#39;</span><span class="p">:</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">keras_validation_preds</span><span class="p">),</span>
 <span class="s1">&#39;3. NN 2&#39;</span><span class="p">:</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">pred_nn2_validation</span><span class="p">),</span>
 <span class="s1">&#39;4. NN 1&#39;</span><span class="p">:</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">pred_nn1_validation</span><span class="p">),</span>
 <span class="s1">&#39;5. GBM final&#39;</span><span class="p">:</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">response_validation</span><span class="p">,</span> <span class="n">validation_gbm_preds_final</span><span class="p">)}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;1. NN 4 (Keras2)&#39;: 0.8480247457655306,
 &#39;2. NN 3 (Keras1)&#39;: 0.8222811671087532,
 &#39;3. NN 2&#39;: 0.8250833589715872,
 &#39;4. NN 1&#39;: 0.8414724395699427,
 &#39;5. GBM final&#39;: 0.861474435196195}
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="evaluation-and-observations">
<h2>Evaluation and observations<a class="headerlink" href="#evaluation-and-observations" title="Permalink to this headline">¶</a></h2>
<p>The Keras model provides the best performance (based on AUC) of the neural networks, but still underperforms the GBM.</p>
<p>As an exercise, play around with some of the features available in Keras and try to build a neural network that outperforms the GBM on the validation data.</p>
<p>You should note that the test data (20% of the original dataset) was not used in any of the modelling above. This is because it should be held-out until a final model has been selected. The test data can then be used to estimate the expected error of the final model on unseen data.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "ActuariesInstitute/cookbook",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="version_control.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Version control</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="multitasking_risk_pricing.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Py/R: Multitasking Risk Pricing Using Deep Learning</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By YDAWG, DAPC, YAC and other contributors.<br/>
    
      <div class="extra_footer">
         The Actuaries' Analytical Cookbook is a series of data and analytics recipes to help actuaries quickly get started with a new project.   This site is intended to be a resource to actuaries in both data science and traditional fields.  Opinions expressed in this publication are the opinions of contributors and do not necessarily represent those of either the Institute of Actuaries of Australia (the ‘Institute’), its members, directors, officers, employees, agents, or that of the employers of the contributors. <br>© Institute of Actuaries of Australia and Contributors 2021. All rights reserved.
      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>